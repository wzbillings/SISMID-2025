{
  "hash": "ed2fc3dc7e59b325b935e6b65caa8728",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Module 5: Data Import and Export\"\nformat: \n  revealjs:\n    scrollable: true\n    smaller: true\n    toc: false\n---\n\n\n## Learning Objectives\n\nAfter module 5, you should be able to...\n\n-   Use Base R functions to load data\n-   Install and attach external R Packages to extend R's functionality\n-   Load any type of data into R\n-   Find loaded data in the Environment pane of RStudio\n-   Reading and writing R .Rds and .Rda/.RData files\n\n\n## Import (read) Data\n\n-   Importing or 'Reading in' data are the first step of any real project / data analysis\n-   R can read almost any file format, especially with external, non-Base R, packages\n-   We are going to focus on simple delimited files first. \n    -   comma separated (e.g. '.csv')\n    -   tab delimited (e.g. '.txt')\n\nA delimited file is a sequential file with column delimiters. Each delimited file is a stream of records, which consists of fields that are ordered by column. Each record contains fields for one row. Within each row, individual fields are separated by column **delimiters** (IBM.com definition)\n\n## Mini exercise\n\n1. Download 5 data from the website and save the data to your data subdirectory -- specifically `SISMID_IntroToR_RProject/data`\n\n1. Open the 'serodata.csv' and 'serodata1.txt' and 'serodata2.txt' data files in a text editor application and familiarize yourself with the data (i.e., Notepad for Windows and TextEdit for Mac)\n\n1. Determine the delimiter of the two '.txt' files\n\n1. Open the 'serodata.xlsx' data file in excel and familiarize yourself with the data\n\t\t-\t\tif you use a Mac **do not** open in Numbers, it can corrupt the file\n\t\t-\t\tif you do not have excel, you can upload it to Google Sheets\n\n\n## Mini exercise\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/txt_files.png){width=100%}\n:::\n:::\n\n\n\n## Import delimited data\n\nWithin the Base R 'util' package we can find a handful of useful functions including  `read.csv()` and `read.delim()` to importing data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?read.csv\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nRegistered S3 method overwritten by 'printr':\n  method                from     \n  knit_print.data.frame rmarkdown\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData Input\n\nDescription:\n\n     Reads a file in table format and creates a data frame from it,\n     with cases corresponding to lines and variables to fields in the\n     file.\n\nUsage:\n\n     read.table(file, header = FALSE, sep = \"\", quote = \"\\\"'\",\n                dec = \".\", numerals = c(\"allow.loss\", \"warn.loss\", \"no.loss\"),\n                row.names, col.names, as.is = !stringsAsFactors, tryLogical = TRUE,\n                na.strings = \"NA\", colClasses = NA, nrows = -1,\n                skip = 0, check.names = TRUE, fill = !blank.lines.skip,\n                strip.white = FALSE, blank.lines.skip = TRUE,\n                comment.char = \"#\",\n                allowEscapes = FALSE, flush = FALSE,\n                stringsAsFactors = FALSE,\n                fileEncoding = \"\", encoding = \"unknown\", text, skipNul = FALSE)\n     \n     read.csv(file, header = TRUE, sep = \",\", quote = \"\\\"\",\n              dec = \".\", fill = TRUE, comment.char = \"\", ...)\n     \n     read.csv2(file, header = TRUE, sep = \";\", quote = \"\\\"\",\n               dec = \",\", fill = TRUE, comment.char = \"\", ...)\n     \n     read.delim(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n                dec = \".\", fill = TRUE, comment.char = \"\", ...)\n     \n     read.delim2(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n                 dec = \",\", fill = TRUE, comment.char = \"\", ...)\n     \nArguments:\n\n    file: the name of the file which the data are to be read from.\n          Each row of the table appears as one line of the file.  If it\n          does not contain an _absolute_ path, the file name is\n          _relative_ to the current working directory, 'getwd()'.\n          Tilde-expansion is performed where supported.  This can be a\n          compressed file (see 'file').\n\n          Alternatively, 'file' can be a readable text-mode connection\n          (which will be opened for reading if necessary, and if so\n          'close'd (and hence destroyed) at the end of the function\n          call).  (If 'stdin()' is used, the prompts for lines may be\n          somewhat confusing.  Terminate input with a blank line or an\n          EOF signal, 'Ctrl-D' on Unix and 'Ctrl-Z' on Windows.  Any\n          pushback on 'stdin()' will be cleared before return.)\n\n          'file' can also be a complete URL.  (For the supported URL\n          schemes, see the 'URLs' section of the help for 'url'.)\n\n  header: a logical value indicating whether the file contains the\n          names of the variables as its first line.  If missing, the\n          value is determined from the file format: 'header' is set to\n          'TRUE' if and only if the first row contains one fewer field\n          than the number of columns.\n\n     sep: the field separator character.  Values on each line of the\n          file are separated by this character.  If 'sep = \"\"' (the\n          default for 'read.table') the separator is 'white space',\n          that is one or more spaces, tabs, newlines or carriage\n          returns.\n\n   quote: the set of quoting characters. To disable quoting altogether,\n          use 'quote = \"\"'.  See 'scan' for the behaviour on quotes\n          embedded in quotes.  Quoting is only considered for columns\n          read as character, which is all of them unless 'colClasses'\n          is specified.\n\n     dec: the character used in the file for decimal points.\n\nnumerals: string indicating how to convert numbers whose conversion to\n          double precision would lose accuracy, see 'type.convert'.\n          Can be abbreviated.  (Applies also to complex-number inputs.)\n\nrow.names: a vector of row names.  This can be a vector giving the\n          actual row names, or a single number giving the column of the\n          table which contains the row names, or character string\n          giving the name of the table column containing the row names.\n\n          If there is a header and the first row contains one fewer\n          field than the number of columns, the first column in the\n          input is used for the row names.  Otherwise if 'row.names' is\n          missing, the rows are numbered.\n\n          Using 'row.names = NULL' forces row numbering. Missing or\n          'NULL' 'row.names' generate row names that are considered to\n          be 'automatic' (and not preserved by 'as.matrix').\n\ncol.names: a vector of optional names for the variables.  The default\n          is to use '\"V\"' followed by the column number.\n\n   as.is: controls conversion of character variables (insofar as they\n          are not converted to logical, numeric or complex) to factors,\n          if not otherwise specified by 'colClasses'.  Its value is\n          either a vector of logicals (values are recycled if\n          necessary), or a vector of numeric or character indices which\n          specify which columns should not be converted to factors.\n\n          Note: to suppress all conversions including those of numeric\n          columns, set 'colClasses = \"character\"'.\n\n          Note that 'as.is' is specified per column (not per variable)\n          and so includes the column of row names (if any) and any\n          columns to be skipped.\n\ntryLogical: a 'logical' determining if columns consisting entirely of\n          '\"F\"', '\"T\"', '\"FALSE\"', and '\"TRUE\"' should be converted to\n          'logical'; passed to 'type.convert', true by default.\n\nna.strings: a character vector of strings which are to be interpreted\n          as 'NA' values.  Blank fields are also considered to be\n          missing values in logical, integer, numeric and complex\n          fields.  Note that the test happens _after_ white space is\n          stripped from the input (if enabled), so 'na.strings' values\n          may need their own white space stripped in advance.\n\ncolClasses: character.  A vector of classes to be assumed for the\n          columns.  If unnamed, recycled as necessary.  If named, names\n          are matched with unspecified values being taken to be 'NA'.\n\n          Possible values are 'NA' (the default, when 'type.convert' is\n          used), '\"NULL\"' (when the column is skipped), one of the\n          atomic vector classes (logical, integer, numeric, complex,\n          character, raw), or '\"factor\"', '\"Date\"' or '\"POSIXct\"'.\n          Otherwise there needs to be an 'as' method (from package\n          'methods') for conversion from '\"character\"' to the specified\n          formal class.\n\n          Note that 'colClasses' is specified per column (not per\n          variable) and so includes the column of row names (if any).\n\n   nrows: integer: the maximum number of rows to read in.  Negative and\n          other invalid values are ignored.\n\n    skip: integer: the number of lines of the data file to skip before\n          beginning to read data.\n\ncheck.names: logical.  If 'TRUE' then the names of the variables in the\n          data frame are checked to ensure that they are syntactically\n          valid variable names.  If necessary they are adjusted (by\n          'make.names') so that they are, and also to ensure that there\n          are no duplicates.\n\n    fill: logical. If 'TRUE' then in case the rows have unequal length,\n          blank fields are implicitly added.  See 'Details'.\n\nstrip.white: logical. Used only when 'sep' has been specified, and\n          allows the stripping of leading and trailing white space from\n          unquoted 'character' fields ('numeric' fields are always\n          stripped).  See 'scan' for further details (including the\n          exact meaning of 'white space'), remembering that the columns\n          may include the row names.\n\nblank.lines.skip: logical: if 'TRUE' blank lines in the input are\n          ignored.\n\ncomment.char: character: a character vector of length one containing a\n          single character or an empty string.  Use '\"\"' to turn off\n          the interpretation of comments altogether.\n\nallowEscapes: logical.  Should C-style escapes such as '\\n' be\n          processed or read verbatim (the default)?  Note that if not\n          within quotes these could be interpreted as a delimiter (but\n          not as a comment character).  For more details see 'scan'.\n\n   flush: logical: if 'TRUE', 'scan' will flush to the end of the line\n          after reading the last of the fields requested.  This allows\n          putting comments after the last field.\n\nstringsAsFactors: logical: should character vectors be converted to\n          factors?  Note that this is overridden by 'as.is' and\n          'colClasses', both of which allow finer control.\n\nfileEncoding: character string: if non-empty declares the encoding used\n          on a file when given as a character string (not on an\n          existing connection) so the character data can be re-encoded.\n          See the 'Encoding' section of the help for 'file', the 'R\n          Data Import/Export' manual and 'Note'.\n\nencoding: encoding to be assumed for input strings.  It is used to mark\n          character strings as known to be in Latin-1 or UTF-8 (see\n          'Encoding'): it is not used to re-encode the input, but\n          allows R to handle encoded strings in their native encoding\n          (if one of those two).  See 'Value' and 'Note'.\n\n    text: character string: if 'file' is not supplied and this is, then\n          data are read from the value of 'text' via a text connection.\n          Notice that a literal string can be used to include (small)\n          data sets within R code.\n\n skipNul: logical: should NULs be skipped?\n\n     ...: Further arguments to be passed to 'read.table'.\n\nDetails:\n\n     This function is the principal means of reading tabular data into\n     R.\n\n     Unless 'colClasses' is specified, all columns are read as\n     character columns and then converted using 'type.convert' to\n     logical, integer, numeric, complex or (depending on 'as.is')\n     factor as appropriate.  Quotes are (by default) interpreted in all\n     fields, so a column of values like '\"42\"' will result in an\n     integer column.\n\n     A field or line is 'blank' if it contains nothing (except\n     whitespace if no separator is specified) before a comment\n     character or the end of the field or line.\n\n     If 'row.names' is not specified and the header line has one less\n     entry than the number of columns, the first column is taken to be\n     the row names.  This allows data frames to be read in from the\n     format in which they are printed.  If 'row.names' is specified and\n     does not refer to the first column, that column is discarded from\n     such files.\n\n     The number of data columns is determined by looking at the first\n     five lines of input (or the whole input if it has less than five\n     lines), or from the length of 'col.names' if it is specified and\n     is longer.  This could conceivably be wrong if 'fill' or\n     'blank.lines.skip' are true, so specify 'col.names' if necessary\n     (as in the 'Examples').\n\n     'read.csv' and 'read.csv2' are identical to 'read.table' except\n     for the defaults.  They are intended for reading 'comma separated\n     value' files ('.csv') or ('read.csv2') the variant used in\n     countries that use a comma as decimal point and a semicolon as\n     field separator.  Similarly, 'read.delim' and 'read.delim2' are\n     for reading delimited files, defaulting to the TAB character for\n     the delimiter.  Notice that 'header = TRUE' and 'fill = TRUE' in\n     these variants, and that the comment character is disabled.\n\n     The rest of the line after a comment character is skipped; quotes\n     are not processed in comments.  Complete comment lines are allowed\n     provided 'blank.lines.skip = TRUE'; however, comment lines prior\n     to the header must have the comment character in the first\n     non-blank column.\n\n     Quoted fields with embedded newlines are supported except after a\n     comment character.  Embedded NULs are unsupported: skipping them\n     (with 'skipNul = TRUE') may work.\n\nValue:\n\n     A data frame ('data.frame') containing a representation of the\n     data in the file.\n\n     Empty input is an error unless 'col.names' is specified, when a\n     0-row data frame is returned: similarly giving just a header line\n     if 'header = TRUE' results in a 0-row data frame.  Note that in\n     either case the columns will be logical unless 'colClasses' was\n     supplied.\n\n     Character strings in the result (including factor levels) will\n     have a declared encoding if 'encoding' is '\"latin1\"' or '\"UTF-8\"'.\n\nCSV files:\n\n     See the help on 'write.csv' for the various conventions for '.csv'\n     files.  The commonest form of CSV file with row names needs to be\n     read with 'read.csv(..., row.names = 1)' to use the names in the\n     first column of the file as row names.\n\nMemory usage:\n\n     These functions can use a surprising amount of memory when reading\n     large files.  There is extensive discussion in the 'R Data\n     Import/Export' manual, supplementing the notes here.\n\n     Less memory will be used if 'colClasses' is specified as one of\n     the six atomic vector classes.  This can be particularly so when\n     reading a column that takes many distinct numeric values, as\n     storing each distinct value as a character string can take up to\n     14 times as much memory as storing it as an integer.\n\n     Using 'nrows', even as a mild over-estimate, will help memory\n     usage.\n\n     Using 'comment.char = \"\"' will be appreciably faster than the\n     'read.table' default.\n\n     'read.table' is not the right tool for reading large matrices,\n     especially those with many columns: it is designed to read _data\n     frames_ which may have columns of very different classes.  Use\n     'scan' instead for matrices.\n\nNote:\n\n     The columns referred to in 'as.is' and 'colClasses' include the\n     column of row names (if any).\n\n     There are two approaches for reading input that is not in the\n     local encoding.  If the input is known to be UTF-8 or Latin1, use\n     the 'encoding' argument to declare that.  If the input is in some\n     other encoding, then it may be translated on input.  The\n     'fileEncoding' argument achieves this by setting up a connection\n     to do the re-encoding into the current locale.  Note that on\n     Windows or other systems not running in a UTF-8 locale, this may\n     not be possible.\n\nReferences:\n\n     Chambers, J. M. (1992) _Data for models._ Chapter 3 of\n     _Statistical Models in S_ eds J. M. Chambers and T. J. Hastie,\n     Wadsworth & Brooks/Cole.\n\nSee Also:\n\n     The 'R Data Import/Export' manual.\n\n     'scan', 'type.convert', 'read.fwf' for reading _f_ixed _w_idth\n     _f_ormatted input; 'write.table'; 'data.frame'.\n\n     'count.fields' can be useful to determine problems with reading\n     files which result in reports of incorrect record lengths (see the\n     'Examples' below).\n\n     <https://www.rfc-editor.org/rfc/rfc4180> for the IANA definition\n     of CSV files (which requires comma as separator and CRLF line\n     endings).\n\nExamples:\n\n     ## using count.fields to handle unknown maximum number of fields\n     ## when fill = TRUE\n     test1 <- c(1:5, \"6,7\", \"8,9,10\")\n     tf <- tempfile()\n     writeLines(test1, tf)\n     \n     read.csv(tf, fill = TRUE) # 1 column\n     ncol <- max(count.fields(tf, sep = \",\"))\n     read.csv(tf, fill = TRUE, header = FALSE,\n              col.names = paste0(\"V\", seq_len(ncol)))\n     unlink(tf)\n     \n     ## \"Inline\" data set, using text=\n     ## Notice that leading and trailing empty lines are auto-trimmed\n     \n     read.table(header = TRUE, text = \"\n     a b\n     1 2\n     3 4\n     \")\n```\n\n\n:::\n:::\n\n\n## Import .csv files\n\nFunction signature reminder\n```\nread.csv(file, header = TRUE, sep = \",\", quote = \"\\\"\",\n         dec = \".\", fill = TRUE, comment.char = \"\", ...)\n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Examples\ndf <- read.csv(file = \"data/serodata.csv\") #relative path\n```\n:::\n\n\nNote #1, I assigned the data frame to an object called `df`.  I could have called the data anything, but in order to use the data (i.e., as an object we can find in the Environment), I need to assign it as an object. \n\nNote #2, If the data is imported correct, you can expect to see the `df` object ready to be used.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/df_in_env.png){width=100%}\n:::\n:::\n\n\n## Import .txt files\n\n`read.csv()` is a special case of `read.delim()` -- a general function to read a delimited file into a data frame  \n\nReminder function signature\n```\nread.delim(file, header = TRUE, sep = \"\\t\", quote = \"\\\"\",\n           dec = \".\", fill = TRUE, comment.char = \"\", ...)\n```\n\n\t\t- `file` is the path to your file, in quotes \n\t\t- `delim` is what separates the fields within a record. The default for csv is comma\n\nWe can import the '.txt' files given that we know that 'serodata1.txt' uses a tab delimiter and 'serodata2.txt' uses a semicolon delimiter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Examples\ndf <- read.delim(file = \"data/serodata.txt\", sep = \"\\t\")\ndf <- read.delim(file = \"data/serodata.txt\", sep = \";\")\n```\n:::\n\n\nThe dataset is now successfully read into your R workspace, **many times actually.** Notice, that each time we imported the data we assigned the data to the `df` object, meaning we replaced it each time we reassigned the `df` object.  \n\n\n## What if we have a .xlsx file - what do we do?\n\n1. Ask Google / ChatGPT\n2. Find and vet function and package you want\n3. Install package\n4. Attach package\n5. Use function\n\n\n## 1. Internet Search\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/ChatGPT.png){width=100%}\n:::\n\n::: {.cell-output-display}\n![](images/GoogleSearch.png){width=100%}\n:::\n\n::: {.cell-output-display}\n![](images/StackOverflow.png){width=100%}\n:::\n:::\n\n\n## 2. Find and vet function and package you want\n\nI am getting consistent message to use the the `read_excel()` function found in the `readxl` package.  This package was developed by Hadley Wickham, who we know is reputable. Also, you can check that data was read in correctly, b/c this is a straightforward task. \n\n## 3. Install Package\n\nTo use the bundle or \"package\" of code (and or possibly data) from a package, you need to install and also attach the package.\n\nTo install a package you can \n\n1. go to Tools ---\\> Install Packages in the RStudio header\n\nOR\n\n2. use the following code:\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"package_name\")\n```\n:::\n\n\n\nTherefore,\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"readxl\")\n```\n:::\n\n\n## 4. Attach Package\n\nReminder - To attach (i.e., be able to use the package) you can use the following code:\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(package_name)\n```\n:::\n\n\nTherefore, \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(readxl)\n```\n:::\n\n\n## 5. Use Function\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?read_excel\n```\n:::\n\nRead xls and xlsx files\n\nDescription:\n\n     Read xls and xlsx files\n\n     'read_excel()' calls 'excel_format()' to determine if 'path' is\n     xls or xlsx, based on the file extension and the file itself, in\n     that order. Use 'read_xls()' and 'read_xlsx()' directly if you\n     know better and want to prevent such guessing.\n\nUsage:\n\n     read_excel(\n       path,\n       sheet = NULL,\n       range = NULL,\n       col_names = TRUE,\n       col_types = NULL,\n       na = \"\",\n       trim_ws = TRUE,\n       skip = 0,\n       n_max = Inf,\n       guess_max = min(1000, n_max),\n       progress = readxl_progress(),\n       .name_repair = \"unique\"\n     )\n     \n     read_xls(\n       path,\n       sheet = NULL,\n       range = NULL,\n       col_names = TRUE,\n       col_types = NULL,\n       na = \"\",\n       trim_ws = TRUE,\n       skip = 0,\n       n_max = Inf,\n       guess_max = min(1000, n_max),\n       progress = readxl_progress(),\n       .name_repair = \"unique\"\n     )\n     \n     read_xlsx(\n       path,\n       sheet = NULL,\n       range = NULL,\n       col_names = TRUE,\n       col_types = NULL,\n       na = \"\",\n       trim_ws = TRUE,\n       skip = 0,\n       n_max = Inf,\n       guess_max = min(1000, n_max),\n       progress = readxl_progress(),\n       .name_repair = \"unique\"\n     )\n     \nArguments:\n\n    path: Path to the xls/xlsx file.\n\n   sheet: Sheet to read. Either a string (the name of a sheet), or an\n          integer (the position of the sheet). Ignored if the sheet is\n          specified via 'range'. If neither argument specifies the\n          sheet, defaults to the first sheet.\n\n   range: A cell range to read from, as described in\n          cell-specification. Includes typical Excel ranges like\n          \"B3:D87\", possibly including the sheet name like\n          \"Budget!B2:G14\", and more. Interpreted strictly, even if the\n          range forces the inclusion of leading or trailing empty rows\n          or columns. Takes precedence over 'skip', 'n_max' and\n          'sheet'.\n\ncol_names: 'TRUE' to use the first row as column names, 'FALSE' to get\n          default names, or a character vector giving a name for each\n          column. If user provides 'col_types' as a vector, 'col_names'\n          can have one entry per column, i.e. have the same length as\n          'col_types', or one entry per unskipped column.\n\ncol_types: Either 'NULL' to guess all from the spreadsheet or a\n          character vector containing one entry per column from these\n          options: \"skip\", \"guess\", \"logical\", \"numeric\", \"date\",\n          \"text\" or \"list\". If exactly one 'col_type' is specified, it\n          will be recycled. The content of a cell in a skipped column\n          is never read and that column will not appear in the data\n          frame output. A list cell loads a column as a list of length\n          1 vectors, which are typed using the type guessing logic from\n          'col_types = NULL', but on a cell-by-cell basis.\n\n      na: Character vector of strings to interpret as missing values.\n          By default, readxl treats blank cells as missing data.\n\n trim_ws: Should leading and trailing whitespace be trimmed?\n\n    skip: Minimum number of rows to skip before reading anything, be it\n          column names or data. Leading empty rows are automatically\n          skipped, so this is a lower bound. Ignored if 'range' is\n          given.\n\n   n_max: Maximum number of data rows to read. Trailing empty rows are\n          automatically skipped, so this is an upper bound on the\n          number of rows in the returned tibble. Ignored if 'range' is\n          given.\n\nguess_max: Maximum number of data rows to use for guessing column\n          types.\n\nprogress: Display a progress spinner? By default, the spinner appears\n          only in an interactive session, outside the context of\n          knitting a document, and when the call is likely to run for\n          several seconds or more. See 'readxl_progress()' for more\n          details.\n\n.name_repair: Handling of column names. Passed along to\n          'tibble::as_tibble()'. readxl's default is `.name_repair =\n          \"unique\", which ensures column names are not empty and are\n          unique.\n\nValue:\n\n     A tibble\n\nSee Also:\n\n     cell-specification for more details on targetting cells with the\n     'range' argument\n\nExamples:\n\n     datasets <- readxl_example(\"datasets.xlsx\")\n     read_excel(datasets)\n     \n     # Specify sheet either by position or by name\n     read_excel(datasets, 2)\n     read_excel(datasets, \"mtcars\")\n     \n     # Skip rows and use default column names\n     read_excel(datasets, skip = 148, col_names = FALSE)\n     \n     # Recycle a single column type\n     read_excel(datasets, col_types = \"text\")\n     \n     # Specify some col_types and guess others\n     read_excel(datasets, col_types = c(\"text\", \"guess\", \"numeric\", \"guess\", \"guess\"))\n     \n     # Accomodate a column with disparate types via col_type = \"list\"\n     df <- read_excel(readxl_example(\"clippy.xlsx\"), col_types = c(\"text\", \"list\"))\n     df\n     df$value\n     sapply(df$value, class)\n     \n     # Limit the number of data rows read\n     read_excel(datasets, n_max = 3)\n     \n     # Read from an Excel range using A1 or R1C1 notation\n     read_excel(datasets, range = \"C1:E7\")\n     read_excel(datasets, range = \"R1C2:R2C5\")\n     \n     # Specify the sheet as part of the range\n     read_excel(datasets, range = \"mtcars!B1:D5\")\n     \n     # Read only specific rows or columns\n     read_excel(datasets, range = cell_rows(102:151), col_names = FALSE)\n     read_excel(datasets, range = cell_cols(\"B:D\"))\n     \n     # Get a preview of column names\n     names(read_excel(readxl_example(\"datasets.xlsx\"), n_max = 0))\n     \n     # exploit full .name_repair flexibility from tibble\n     \n     # \"universal\" names are unique and syntactic\n     read_excel(\n       readxl_example(\"deaths.xlsx\"),\n       range = \"arts!A5:F15\",\n       .name_repair = \"universal\"\n     )\n     \n     # specify name repair as a built-in function\n     read_excel(readxl_example(\"clippy.xlsx\"), .name_repair = toupper)\n     \n     # specify name repair as a custom function\n     my_custom_name_repair <- function(nms) tolower(gsub(\"[.]\", \"_\", nms))\n     read_excel(\n       readxl_example(\"datasets.xlsx\"),\n       .name_repair = my_custom_name_repair\n     )\n     \n     # specify name repair as an anonymous function\n     read_excel(\n       readxl_example(\"datasets.xlsx\"),\n       sheet = \"chickwts\",\n       .name_repair = ~ substr(.x, start = 1, stop = 3)\n     )\n\n\n## 5. Use Function\n\nReminder of function signature\n```\nread_excel(\n  path,\n  sheet = NULL,\n  range = NULL,\n  col_names = TRUE,\n  col_types = NULL,\n  na = \"\",\n  trim_ws = TRUE,\n  skip = 0,\n  n_max = Inf,\n  guess_max = min(1000, n_max),\n  progress = readxl_progress(),\n  .name_repair = \"unique\"\n)\n```\n\nLet's practice\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read_excel(path = \"data/serodata.xlsx\", sheet = \"Data\")\n```\n:::\n\n\n\n## What would happen if we made these mistakes (*)\n\n1. What do you think would happen if I had imported  the data without assigning it to an object \n\n::: {.cell}\n\n```{.r .cell-code}\nread_excel(path = \"data/serodata.xlsx\", sheet = \"Data\")\n```\n:::\n\n\n2. What do you think would happen if I forgot to specify the `sheet` argument?\n\n::: {.cell}\n\n```{.r .cell-code}\ndd <- read_excel(path = \"data/serodata.xlsx\")\n```\n:::\n\n\n\n## Installing and attaching packages - Common confusion\n\n</br>\n\nYou only need to install a package once (unless you update R or want to update the package), but you will need to attach a package each time you want to use it. \n\n</br>\n\nThe exception to this rule are the \"base\" set of packages (i.e., **Base R**) that are installed automatically when you install R and that automatically attached whenever you open R or RStudio.\n\n\n## Common Error\n\nBe prepared to see this error\n\n\n::: {.cell}\n\n```{.r .cell-code}\nError: could not find function \"some_function_name\"\n```\n:::\n\n\nThis usually means that either \n\n- you called the function by the wrong name \n- you have not installed a package that contains the function\n- you have installed a package but you forgot to attach it (i.e., `require(package_name)`) -- **most likely**\n\n\n## Export (write) Data \n\n-   Exporting or 'Writing out' data allows you to save modified files for future use or sharing\n-   R can write almost any file format, especially with external, non-Base R, packages\n-   We are going to focus again on writing delimited files\n\n\n## Export delimited data\n\nWithin the Base R 'util' package we can find a handful of useful functions including  `write.csv()` and `write.table()` to exporting data.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nData Output\n\nDescription:\n\n     'write.table' prints its required argument 'x' (after converting\n     it to a data frame if it is not one nor a matrix) to a file or\n     connection.\n\nUsage:\n\n     write.table(x, file = \"\", append = FALSE, quote = TRUE, sep = \" \",\n                 eol = \"\\n\", na = \"NA\", dec = \".\", row.names = TRUE,\n                 col.names = TRUE, qmethod = c(\"escape\", \"double\"),\n                 fileEncoding = \"\")\n     \n     write.csv(...)\n     write.csv2(...)\n     \nArguments:\n\n       x: the object to be written, preferably a matrix or data frame.\n          If not, it is attempted to coerce 'x' to a data frame.\n\n    file: either a character string naming a file or a connection open\n          for writing.  '\"\"' indicates output to the console.\n\n  append: logical. Only relevant if 'file' is a character string.  If\n          'TRUE', the output is appended to the file.  If 'FALSE', any\n          existing file of the name is destroyed.\n\n   quote: a logical value ('TRUE' or 'FALSE') or a numeric vector.  If\n          'TRUE', any character or factor columns will be surrounded by\n          double quotes.  If a numeric vector, its elements are taken\n          as the indices of columns to quote.  In both cases, row and\n          column names are quoted if they are written.  If 'FALSE',\n          nothing is quoted.\n\n     sep: the field separator string.  Values within each row of 'x'\n          are separated by this string.\n\n     eol: the character(s) to print at the end of each line (row).  For\n          example, 'eol = \"\\r\\n\"' will produce Windows' line endings on\n          a Unix-alike OS, and 'eol = \"\\r\"' will produce files as\n          expected by Excel:mac 2004.\n\n      na: the string to use for missing values in the data.\n\n     dec: the string to use for decimal points in numeric or complex\n          columns: must be a single character.\n\nrow.names: either a logical value indicating whether the row names of\n          'x' are to be written along with 'x', or a character vector\n          of row names to be written.\n\ncol.names: either a logical value indicating whether the column names\n          of 'x' are to be written along with 'x', or a character\n          vector of column names to be written.  See the section on\n          'CSV files' for the meaning of 'col.names = NA'.\n\n qmethod: a character string specifying how to deal with embedded\n          double quote characters when quoting strings.  Must be one of\n          '\"escape\"' (default for 'write.table'), in which case the\n          quote character is escaped in C style by a backslash, or\n          '\"double\"' (default for 'write.csv' and 'write.csv2'), in\n          which case it is doubled.  You can specify just the initial\n          letter.\n\nfileEncoding: character string: if non-empty declares the encoding to\n          be used on a file (not a connection) so the character data\n          can be re-encoded as they are written.  See 'file'.\n\n     ...: arguments to 'write.table': 'append', 'col.names', 'sep',\n          'dec' and 'qmethod' cannot be altered.\n\nDetails:\n\n     If the table has no columns the rownames will be written only if\n     'row.names = TRUE', and _vice versa_.\n\n     Real and complex numbers are written to the maximal possible\n     precision.\n\n     If a data frame has matrix-like columns these will be converted to\n     multiple columns in the result (_via_ 'as.matrix') and so a\n     character 'col.names' or a numeric 'quote' should refer to the\n     columns in the result, not the input.  Such matrix-like columns\n     are unquoted by default.\n\n     Any columns in a data frame which are lists or have a class (e.g.,\n     dates) will be converted by the appropriate 'as.character' method:\n     such columns are unquoted by default.  On the other hand, any\n     class information for a matrix is discarded and non-atomic (e.g.,\n     list) matrices are coerced to character.\n\n     Only columns which have been converted to character will be quoted\n     if specified by 'quote'.\n\n     The 'dec' argument only applies to columns that are not subject to\n     conversion to character because they have a class or are part of a\n     matrix-like column (or matrix), in particular to columns protected\n     by 'I()'.  Use 'options(\"OutDec\")' to control such conversions.\n\n     In almost all cases the conversion of numeric quantities is\n     governed by the option '\"scipen\"' (see 'options'), but with the\n     internal equivalent of 'digits = 15'.  For finer control, use\n     'format' to make a character matrix/data frame, and call\n     'write.table' on that.\n\n     These functions check for a user interrupt every 1000 lines of\n     output.\n\n     If 'file' is a non-open connection, an attempt is made to open it\n     and then close it after use.\n\n     To write a Unix-style file on Windows, use a binary connection\n     e.g. 'file = file(\"filename\", \"wb\")'.\n\nCSV files:\n\n     By default there is no column name for a column of row names.  If\n     'col.names = NA' and 'row.names = TRUE' a blank column name is\n     added, which is the convention used for CSV files to be read by\n     spreadsheets.  Note that such CSV files can be read in R by\n\n       read.csv(file = \"<filename>\", row.names = 1)\n     \n     'write.csv' and 'write.csv2' provide convenience wrappers for\n     writing CSV files.  They set 'sep' and 'dec' (see below), 'qmethod\n     = \"double\"', and 'col.names' to 'NA' if 'row.names = TRUE' (the\n     default) and to 'TRUE' otherwise.\n\n     'write.csv' uses '\".\"' for the decimal point and a comma for the\n     separator.\n\n     'write.csv2' uses a comma for the decimal point and a semicolon\n     for the separator, the Excel convention for CSV files in some\n     Western European locales.\n\n     These wrappers are deliberately inflexible: they are designed to\n     ensure that the correct conventions are used to write a valid\n     file.  Attempts to change 'append', 'col.names', 'sep', 'dec' or\n     'qmethod' are ignored, with a warning.\n\n     CSV files do not record an encoding, and this causes problems if\n     they are not ASCII for many other applications.  Windows Excel\n     2007/10 will open files (e.g., by the file association mechanism)\n     correctly if they are ASCII or UTF-16 (use 'fileEncoding =\n     \"UTF-16LE\"') or perhaps in the current Windows codepage (e.g.,\n     '\"CP1252\"'), but the 'Text Import Wizard' (from the 'Data' tab)\n     allows far more choice of encodings.  Excel:mac 2004/8 can\n     _import_ only 'Macintosh' (which seems to mean Mac Roman),\n     'Windows' (perhaps Latin-1) and 'PC-8' files.  OpenOffice 3.x asks\n     for the character set when opening the file.\n\n     There is an IETF RFC4180\n     (<https://www.rfc-editor.org/rfc/rfc4180>) for CSV files, which\n     mandates comma as the separator and CRLF line endings.\n     'write.csv' writes compliant files on Windows: use 'eol = \"\\r\\n\"'\n     on other platforms.\n\nNote:\n\n     'write.table' can be slow for data frames with large numbers\n     (hundreds or more) of columns: this is inevitable as each column\n     could be of a different class and so must be handled separately.\n     If they are all of the same class, consider using a matrix\n     instead.\n\nSee Also:\n\n     The 'R Data Import/Export' manual.\n\n     'read.table', 'write'.\n\n     'write.matrix' in package 'MASS'.\n\nExamples:\n\n     x <- data.frame(a = I(\"a \\\" quote\"), b = pi)\n     tf <- tempfile(fileext = \".csv\")\n     \n     ## To write a CSV file for input to Excel one might use\n     write.table(x, file = tf, sep = \",\", col.names = NA,\n                 qmethod = \"double\")\n     file.show(tf)\n     ## and to read this file back into R one needs\n     read.table(tf, header = TRUE, sep = \",\", row.names = 1)\n     ## NB: you do need to specify a separator if qmethod = \"double\".\n     \n     ### Alternatively\n     write.csv(x, file = tf)\n     read.csv(tf, row.names = 1)\n     ## or without row names\n     write.csv(x, file = tf, row.names = FALSE)\n     read.csv(tf)\n     \n     ## Not run:\n     \n     ## To write a file in Mac Roman for simple use in Mac Excel 2004/8\n     write.csv(x, file = \"foo.csv\", fileEncoding = \"macroman\")\n     ## or for Windows Excel 2007/10\n     write.csv(x, file = \"foo.csv\", fileEncoding = \"UTF-16LE\")\n     ## End(Not run)\n```\n\n\n:::\n:::\n\n\n## Export delimited data\n\nLet's practice exporting the data as three files with three different delimiters (comma, tab, semicolon)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.csv(df, file=\"data/serodata_new.csv\", row.names = FALSE) #comma delimited\nwrite.table(df, file=\"data/serodata1_new.txt\", sep=\"\\t\", row.names = FALSE) #tab delimited\nwrite.table(df, file=\"data/serodata2_new.txt\", sep=\";\", row.names = FALSE) #semicolon delimited\n```\n:::\n\n\nNote, I wrote the data to new file names.  Even though we didn't change the data at all in this module, it is good practice to keep raw data raw, and not to write over it.\n\n## R .rds and .rda/RData files\n\nThere are two file extensions worth discussing.\n\nR has two native data formatsâ€”'Rdata' (sometimes shortened to 'Rda') and 'Rds'. These formats are used when R objects are saved for later use. 'Rdata' is used to save multiple R objects, while 'Rds' is used to save a single R object. 'Rds' is fast to write/read and is very small.\n\n## .rds binary file\n\nSaving datasets in `.rds` format can save time if you have to read it back in later.\n\n`write_rds()` and `read_rds()` from `readr` package can be used to write/read a single R object to/from file.\n\n```\nrequire(readr)\nwrite_rds(object1, file = \"filename.rds\")\nobject1 <- read_rds(file = \"filename.rds\")\n```\n\n\n## .rda/RData files \n\nThe Base R functions `save()` and `load()` can be used to save and load multiple R objects. \n\n`save()` writes an external representation of R objects to the specified file, and can by loaded back into the environment using `load()`. A nice feature about using `save` and `load` is that the R object(s) is directly imported into the environment and you don't have to specify the name. The files can be saved as `.RData` or `.Rda` files.\n\nFunction signature\n```\nsave(object1, object2, file = \"filename.RData\")\nload(\"filename.RData\")\n```\n\nNote, that you separate the objects you want to save with commas.\n\n\n\n## Summary\n\n- Importing or 'Reading in' data are the first step of any real project /  data analysis\n- The Base R 'util' package has useful functions including  `read.csv()` and `read.delim()` to importing/reading data or `write.csv()` and `write.table()` for exporting/writing data\n- When importing data (exception is object from .RData), you must assign it to an object, otherwise it cannot be used\n- If data are imported correctly, they can be found in the Environment pane of RStudio\n- You only need to install a package once (unless you update R or the package), but you will need to attach a package each time you want to use it. \n- To complete a task you don't know how to do (e.g., reading in an excel data file) use the following steps: 1. Asl Google / ChatGPT, 2. Find and vet function and package you want, 3. Install package, 4. Attach package, 5. Use function\n\n\n## Acknowledgements\n\nThese are the materials we looked through, modified, or extracted to complete this module's lecture.\n\n-   [\"Introduction to R for Public Health Researchers\" Johns Hopkins University](https://jhudatascience.org/intro_to_r/)\n\n",
    "supporting": [
      "Module05-DataImportExport_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}