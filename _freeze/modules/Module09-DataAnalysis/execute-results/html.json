{
  "hash": "452ede3a29ec71357c68ba430e1ff200",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Module 9: Data Analysis\"\nformat: \n  revealjs:\n    scrollable: true\n    smaller: true\n    toc: false\n---\n\n\n\n## Learning Objectives\n\nAfter module 9, you should be able to...\n\n-\tDescriptively assess association between two variables\n-\tCompute basic statistics \n-\tFit a generalized linear model\n\n## Import data for this module\n\nLet's read in our data (again) and take a quick look.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read.csv(file = \"data/serodata.csv\") #relative path\nhead(x=df, n=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  observation_id IgG_concentration age gender     slum\n1           5772         0.3176895   2 Female Non slum\n2           8095         3.4368231   4 Female Non slum\n3           9784         0.3000000   4   Male Non slum\n```\n\n\n:::\n:::\n\n\n\n## Prep data\n\nCreate `age_group` three level factor variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf$age_group <- ifelse(df$age <= 5, \"young\", \n                       ifelse(df$age<=10 & df$age>5, \"middle\", \"old\"))\ndf$age_group <- factor(df$age_group, levels=c(\"young\", \"middle\", \"old\"))\n```\n:::\n\n\n\nCreate `seropos` binary variable representing seropositivity if antibody concentrations are >10 IU/mL.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf$seropos <- ifelse(df$IgG_concentration<10, 0, 1)\n```\n:::\n\n\n\n## Grouped analyses\n\n* Most of this module will discuss statistical analyses. But first we'll\ndiscuss doing univariate analyses we've already used on multiple groups.\n* We can use the `aggregate()` function to do many analyses across groups.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?aggregate\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(printr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRegistered S3 method overwritten by 'printr':\n  method                from     \n  knit_print.data.frame rmarkdown\n```\n\n\n:::\n\n```{.r .cell-code}\n?aggregate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCompute Summary Statistics of Data Subsets\n\nDescription:\n\n     Splits the data into subsets, computes summary statistics for\n     each, and returns the result in a convenient form.\n\nUsage:\n\n     aggregate(x, ...)\n     \n     ## Default S3 method:\n     aggregate(x, ...)\n     \n     ## S3 method for class 'data.frame'\n     aggregate(x, by, FUN, ..., simplify = TRUE, drop = TRUE)\n     \n     ## S3 method for class 'formula'\n     aggregate(x, data, FUN, ...,\n               subset, na.action = na.omit)\n     \n     ## S3 method for class 'ts'\n     aggregate(x, nfrequency = 1, FUN = sum, ndeltat = 1,\n               ts.eps = getOption(\"ts.eps\"), ...)\n     \nArguments:\n\n       x: an R object.  For the 'formula' method a 'formula', such as\n          'y ~ x' or 'cbind(y1, y2) ~ x1 + x2', where the 'y' variables\n          are numeric data to be split into groups according to the\n          grouping 'x' variables (usually factors).\n\n      by: a list of grouping elements, each as long as the variables in\n          the data frame 'x', or a formula.  The elements are coerced\n          to factors before use.\n\n     FUN: a function to compute the summary statistics which can be\n          applied to all data subsets.\n\nsimplify: a logical indicating whether results should be simplified to\n          a vector or matrix if possible.\n\n    drop: a logical indicating whether to drop unused combinations of\n          grouping values.  The non-default case 'drop=FALSE' has been\n          amended for R 3.5.0 to drop unused combinations.\n\n    data: a data frame (or list) from which the variables in the\n          formula should be taken.\n\n  subset: an optional vector specifying a subset of observations to be\n          used.\n\nna.action: a function which indicates what should happen when the data\n          contain 'NA' values. The default is to ignore missing values\n          in the given variables.\n\nnfrequency: new number of observations per unit of time; must be a\n          divisor of the frequency of 'x'.\n\n ndeltat: new fraction of the sampling period between successive\n          observations; must be a divisor of the sampling interval of\n          'x'.\n\n  ts.eps: tolerance used to decide if 'nfrequency' is a sub-multiple of\n          the original frequency.\n\n     ...: further arguments passed to or used by methods.\n\nDetails:\n\n     'aggregate' is a generic function with methods for data frames and\n     time series.\n\n     The default method, 'aggregate.default', uses the time series\n     method if 'x' is a time series, and otherwise coerces 'x' to a\n     data frame and calls the data frame method.\n\n     'aggregate.data.frame' is the data frame method.  If 'x' is not a\n     data frame, it is coerced to one, which must have a non-zero\n     number of rows.  Then, each of the variables (columns) in 'x' is\n     split into subsets of cases (rows) of identical combinations of\n     the components of 'by', and 'FUN' is applied to each such subset\n     with further arguments in '...' passed to it.  The result is\n     reformatted into a data frame containing the variables in 'by' and\n     'x'.  The ones arising from 'by' contain the unique combinations\n     of grouping values used for determining the subsets, and the ones\n     arising from 'x' the corresponding summaries for the subset of the\n     respective variables in 'x'.  If 'simplify' is true, summaries are\n     simplified to vectors or matrices if they have a common length of\n     one or greater than one, respectively; otherwise, lists of summary\n     results according to subsets are obtained.  Rows with missing\n     values in any of the 'by' variables will be omitted from the\n     result.  (Note that versions of R prior to 2.11.0 required 'FUN'\n     to be a scalar function.)\n\n     The formula method provides a standard formula interface to\n     'aggregate.data.frame'.  The latter invokes the formula method if\n     'by' is a formula, in which case 'aggregate(x, by, FUN)' is the\n     same as 'aggregate(by, x, FUN)' for a data frame 'x'.\n\n     'aggregate.ts' is the time series method, and requires 'FUN' to be\n     a scalar function.  If 'x' is not a time series, it is coerced to\n     one.  Then, the variables in 'x' are split into appropriate blocks\n     of length 'frequency(x) / nfrequency', and 'FUN' is applied to\n     each such block, with further (named) arguments in '...' passed to\n     it.  The result returned is a time series with frequency\n     'nfrequency' holding the aggregated values.  Note that this make\n     most sense for a quarterly or yearly result when the original\n     series covers a whole number of quarters or years: in particular\n     aggregating a monthly series to quarters starting in February does\n     not give a conventional quarterly series.\n\n     'FUN' is passed to 'match.fun', and hence it can be a function or\n     a symbol or character string naming a function.\n\nValue:\n\n     For the time series method, a time series of class '\"ts\"' or class\n     'c(\"mts\", \"ts\")'.\n\n     For the data frame method, a data frame with columns corresponding\n     to the grouping variables in 'by' followed by aggregated columns\n     from 'x'.  If the 'by' has names, the non-empty times are used to\n     label the columns in the results, with unnamed grouping variables\n     being named 'Group.i' for 'by[[i]]'.\n\nWarning:\n\n     The first argument of the '\"formula\"' method was named 'formula'\n     rather than 'x' prior to R 4.2.0.  Portable uses should not name\n     that argument.\n\nAuthor(s):\n\n     Kurt Hornik, with contributions by Arni Magnusson.\n\nReferences:\n\n     Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) _The New S\n     Language_.  Wadsworth & Brooks/Cole.\n\nSee Also:\n\n     'apply', 'lapply', 'tapply'.\n\nExamples:\n\n     ## Compute the averages for the variables in 'state.x77', grouped\n     ## according to the region (Northeast, South, North Central, West) that\n     ## each state belongs to.\n     aggregate(state.x77, list(Region = state.region), mean)\n     \n     ## Compute the averages according to region and the occurrence of more\n     ## than 130 days of frost.\n     aggregate(state.x77,\n               list(Region = state.region,\n                    Cold = state.x77[,\"Frost\"] > 130),\n               mean)\n     ## (Note that no state in 'South' is THAT cold.)\n     \n     \n     ## example with character variables and NAs\n     testDF <- data.frame(v1 = c(1,3,5,7,8,3,5,NA,4,5,7,9),\n                          v2 = c(11,33,55,77,88,33,55,NA,44,55,77,99) )\n     by1 <- c(\"red\", \"blue\", 1, 2, NA, \"big\", 1, 2, \"red\", 1, NA, 12)\n     by2 <- c(\"wet\", \"dry\", 99, 95, NA, \"damp\", 95, 99, \"red\", 99, NA, NA)\n     aggregate(x = testDF, by = list(by1, by2), FUN = \"mean\")\n     \n     # and if you want to treat NAs as a group\n     fby1 <- factor(by1, exclude = \"\")\n     fby2 <- factor(by2, exclude = \"\")\n     aggregate(x = testDF, by = list(fby1, fby2), FUN = \"mean\")\n     \n     \n     ## Formulas, one ~ one, one ~ many, many ~ one, and many ~ many:\n     aggregate(weight ~ feed, data = chickwts, mean)\n     aggregate(breaks ~ wool + tension, data = warpbreaks, mean)\n     aggregate(cbind(Ozone, Temp) ~ Month, data = airquality, mean)\n     aggregate(cbind(ncases, ncontrols) ~ alcgp + tobgp, data = esoph, sum)\n     \n     ## Dot notation:\n     aggregate(. ~ Species, data = iris, mean)\n     aggregate(len ~ ., data = ToothGrowth, mean)\n     \n     ## Often followed by xtabs():\n     ag <- aggregate(len ~ ., data = ToothGrowth, mean)\n     xtabs(len ~ ., data = ag)\n     \n     ## Formula interface via 'by' (for pipe operations)\n     ToothGrowth |> aggregate(len ~ ., FUN = mean)\n     \n     ## Compute the average annual approval ratings for American presidents.\n     aggregate(presidents, nfrequency = 1, FUN = mean)\n     ## Give the summer less weight.\n     aggregate(presidents, nfrequency = 1,\n               FUN = weighted.mean, w = c(1, 1, 0.5, 1))\n```\n\n\n:::\n:::\n\n\n\n## Grouped analyses\n\n* Let's calculate seropositivity rate across age groups using the variables\nwe just created.\n* The easiest way to use `aggregate()` is with the formula option. The syntax\nis `variable_of_intest ~ grouping_variables`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naggregate(\n\t# Formula specifies we are calculating statistics on seropos, separately for\n\t# each level of age_group\n\tseropos ~ age_group,\n\tdata = df, # Data argument\n\tFUN = mean # function for our calculation WITHOUT PARENTHESES\n)\n```\n\n::: {.cell-output-display}\n\n\n|age_group |   seropos|\n|:---------|---------:|\n|young     | 0.1832797|\n|middle    | 0.6000000|\n|old       | 0.7945205|\n:::\n:::\n\n\n\n* We can add as many things as we want on the RHS of the formula.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naggregate(\n\tIgG_concentration ~ age_group + slum,\n\tdata = df,\n\tFUN = sd # standard deviation\n)\n```\n\n::: {.cell-output-display}\n\n\n|age_group |slum     | IgG_concentration|\n|:---------|:--------|-----------------:|\n|young     |Mixed    |         174.89797|\n|middle    |Mixed    |         162.08188|\n|old       |Mixed    |         150.07063|\n|young     |Non slum |         114.68422|\n|middle    |Non slum |         177.62113|\n|old       |Non slum |         141.22330|\n|young     |Slum     |          61.85705|\n|middle    |Slum     |         202.42018|\n|old       |Slum     |          74.75217|\n:::\n:::\n\n\n\n* We can also add multiple variables on the LHS at the same time using\n`cbind()` syntax.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naggregate(\n\tcbind(age, IgG_concentration) ~ gender + slum,\n\tdata = df,\n\tFUN = median\n)\n```\n\n::: {.cell-output-display}\n\n\n|gender |slum     | age| IgG_concentration|\n|:------|:--------|---:|-----------------:|\n|Female |Mixed    | 5.0|         2.0117423|\n|Male   |Mixed    | 6.0|         2.2082192|\n|Female |Non slum | 6.0|         2.5040431|\n|Male   |Non slum | 5.0|         1.1245846|\n|Female |Slum     | 3.0|         5.1482480|\n|Male   |Slum     | 5.5|         0.7753834|\n:::\n:::\n\n\n\n## 2 variable contingency tables\n\nWe use `table()` prior to look at one variable, now we can generate frequency tables for 2 plus variables.  To get cell percentages, the `prop.table()` is useful.  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?prop.table\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(printr)\n?prop.table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpress Table Entries as Fraction of Marginal Table\n\nDescription:\n\n     Returns conditional proportions given 'margins', i.e. entries of\n     'x', divided by the appropriate marginal sums.\n\nUsage:\n\n     proportions(x, margin = NULL)\n     prop.table(x, margin = NULL)\n     \nArguments:\n\n       x: table\n\n  margin: a vector giving the margins to split by.  E.g., for a matrix\n          '1' indicates rows, '2' indicates columns, 'c(1, 2)'\n          indicates rows and columns.  When 'x' has named dimnames, it\n          can be a character vector selecting dimension names.\n\nValue:\n\n     Table like 'x' expressed relative to 'margin'\n\nNote:\n\n     'prop.table' is an earlier name, retained for back-compatibility.\n\nAuthor(s):\n\n     Peter Dalgaard\n\nSee Also:\n\n     'marginSums'. 'apply', 'sweep' are a more general mechanism for\n     sweeping out marginal statistics.\n\nExamples:\n\n     m <- matrix(1:4, 2)\n     m\n     proportions(m, 1)\n     \n     DF <- as.data.frame(UCBAdmissions)\n     tbl <- xtabs(Freq ~ Gender + Admit, DF)\n     \n     proportions(tbl, \"Gender\")\n```\n\n\n:::\n:::\n\n\n\n## 2 variable contingency tables\n\nLet's practice\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreq <- table(df$age_group, df$seropos)\nfreq\n```\n\n::: {.cell-output-display}\n\n\n|/      |   0|   1|\n|:------|---:|---:|\n|young  | 254|  57|\n|middle |  70| 105|\n|old    |  30| 116|\n:::\n:::\n\n\n\nNow, lets move to percentages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.cell.percentages <- prop.table(freq)\nprop.cell.percentages\n```\n\n::: {.cell-output-display}\n\n\n|/      |         0|         1|\n|:------|---------:|---------:|\n|young  | 0.4018987| 0.0901899|\n|middle | 0.1107595| 0.1661392|\n|old    | 0.0474684| 0.1835443|\n:::\n\n```{.r .cell-code}\nprop.column.percentages <- prop.table(freq, margin=2)\nprop.column.percentages\n```\n\n::: {.cell-output-display}\n\n\n|/      |         0|         1|\n|:------|---------:|---------:|\n|young  | 0.7175141| 0.2050360|\n|middle | 0.1977401| 0.3776978|\n|old    | 0.0847458| 0.4172662|\n:::\n:::\n\n\n\n\n## Chi-Square test\n\nThe `chisq.test()` function test of independence of factor variables from `stats` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?chisq.test\n```\n:::\n\nPearson's Chi-squared Test for Count Data\n\nDescription:\n\n     'chisq.test' performs chi-squared contingency table tests and\n     goodness-of-fit tests.\n\nUsage:\n\n     chisq.test(x, y = NULL, correct = TRUE,\n                p = rep(1/length(x), length(x)), rescale.p = FALSE,\n                simulate.p.value = FALSE, B = 2000)\n     \nArguments:\n\n       x: a numeric vector or matrix. 'x' and 'y' can also both be\n          factors.\n\n       y: a numeric vector; ignored if 'x' is a matrix.  If 'x' is a\n          factor, 'y' should be a factor of the same length.\n\n correct: a logical indicating whether to apply continuity correction\n          when computing the test statistic for 2 by 2 tables: one half\n          is subtracted from all |O - E| differences; however, the\n          correction will not be bigger than the differences\n          themselves.  No correction is done if 'simulate.p.value =\n          TRUE'.\n\n       p: a vector of probabilities of the same length as 'x'.  An\n          error is given if any entry of 'p' is negative.\n\nrescale.p: a logical scalar; if TRUE then 'p' is rescaled (if\n          necessary) to sum to 1.  If 'rescale.p' is FALSE, and 'p'\n          does not sum to 1, an error is given.\n\nsimulate.p.value: a logical indicating whether to compute p-values by\n          Monte Carlo simulation.\n\n       B: an integer specifying the number of replicates used in the\n          Monte Carlo test.\n\nDetails:\n\n     If 'x' is a matrix with one row or column, or if 'x' is a vector\n     and 'y' is not given, then a _goodness-of-fit test_ is performed\n     ('x' is treated as a one-dimensional contingency table).  The\n     entries of 'x' must be non-negative integers.  In this case, the\n     hypothesis tested is whether the population probabilities equal\n     those in 'p', or are all equal if 'p' is not given.\n\n     If 'x' is a matrix with at least two rows and columns, it is taken\n     as a two-dimensional contingency table: the entries of 'x' must be\n     non-negative integers.  Otherwise, 'x' and 'y' must be vectors or\n     factors of the same length; cases with missing values are removed,\n     the objects are coerced to factors, and the contingency table is\n     computed from these.  Then Pearson's chi-squared test is performed\n     of the null hypothesis that the joint distribution of the cell\n     counts in a 2-dimensional contingency table is the product of the\n     row and column marginals.\n\n     If 'simulate.p.value' is 'FALSE', the p-value is computed from the\n     asymptotic chi-squared distribution of the test statistic;\n     continuity correction is only used in the 2-by-2 case (if\n     'correct' is 'TRUE', the default).  Otherwise the p-value is\n     computed for a Monte Carlo test (Hope, 1968) with 'B' replicates.\n     The default 'B = 2000' implies a minimum p-value of about 0.0005\n     (1/(B+1)).\n\n     In the contingency table case, simulation is done by random\n     sampling from the set of all contingency tables with given\n     marginals, and works only if the marginals are strictly positive.\n     Continuity correction is never used, and the statistic is quoted\n     without it.  Note that this is not the usual sampling situation\n     assumed for the chi-squared test but rather that for Fisher's\n     exact test.\n\n     In the goodness-of-fit case simulation is done by random sampling\n     from the discrete distribution specified by 'p', each sample being\n     of size 'n = sum(x)'.  This simulation is done in R and may be\n     slow.\n\nValue:\n\n     A list with class '\"htest\"' containing the following components:\n\nstatistic: the value the chi-squared test statistic.\n\nparameter: the degrees of freedom of the approximate chi-squared\n          distribution of the test statistic, 'NA' if the p-value is\n          computed by Monte Carlo simulation.\n\n p.value: the p-value for the test.\n\n  method: a character string indicating the type of test performed, and\n          whether Monte Carlo simulation or continuity correction was\n          used.\n\ndata.name: a character string giving the name(s) of the data.\n\nobserved: the observed counts.\n\nexpected: the expected counts under the null hypothesis.\n\nresiduals: the Pearson residuals, '(observed - expected) /\n          sqrt(expected)'.\n\n  stdres: standardized residuals, '(observed - expected) / sqrt(V)',\n          where 'V' is the residual cell variance (Agresti, 2007,\n          section 2.4.5 for the case where 'x' is a matrix, 'n * p * (1\n          - p)' otherwise).\n\nSource:\n\n     The code for Monte Carlo simulation is a C translation of the\n     Fortran algorithm of Patefield (1981).\n\nReferences:\n\n     Hope, A. C. A. (1968).  A simplified Monte Carlo significance test\n     procedure.  _Journal of the Royal Statistical Society Series B_,\n     *30*, 582-598.  doi:10.1111/j.2517-6161.1968.tb00759.x\n     <https://doi.org/10.1111/j.2517-6161.1968.tb00759.x>.\n\n     Patefield, W. M. (1981).  Algorithm AS 159: An efficient method of\n     generating r x c tables with given row and column totals.\n     _Applied Statistics_, *30*, 91-97.  doi:10.2307/2346669\n     <https://doi.org/10.2307/2346669>.\n\n     Agresti, A. (2007).  _An Introduction to Categorical Data\n     Analysis_, 2nd ed.  New York: John Wiley & Sons.  Page 38.\n\nSee Also:\n\n     For goodness-of-fit testing, notably of continuous distributions,\n     'ks.test'.\n\nExamples:\n\n     ## From Agresti(2007) p.39\n     M <- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))\n     dimnames(M) <- list(gender = c(\"F\", \"M\"),\n                         party = c(\"Democrat\",\"Independent\", \"Republican\"))\n     (Xsq <- chisq.test(M))  # Prints test summary\n     Xsq$observed   # observed counts (same as M)\n     Xsq$expected   # expected counts under the null\n     Xsq$residuals  # Pearson residuals\n     Xsq$stdres     # standardized residuals\n     \n     \n     ## Effect of simulating p-values\n     x <- matrix(c(12, 5, 7, 7), ncol = 2)\n     chisq.test(x)$p.value           # 0.4233\n     chisq.test(x, simulate.p.value = TRUE, B = 10000)$p.value\n                                     # around 0.29!\n     \n     ## Testing for population probabilities\n     ## Case A. Tabulated data\n     x <- c(A = 20, B = 15, C = 25)\n     chisq.test(x)\n     chisq.test(as.table(x))             # the same\n     x <- c(89,37,30,28,2)\n     p <- c(40,20,20,15,5)\n     try(\n     chisq.test(x, p = p)                # gives an error\n     )\n     chisq.test(x, p = p, rescale.p = TRUE)\n                                     # works\n     p <- c(0.40,0.20,0.20,0.19,0.01)\n                                     # Expected count in category 5\n                                     # is 1.86 < 5 ==> chi square approx.\n     chisq.test(x, p = p)            #               maybe doubtful, but is ok!\n     chisq.test(x, p = p, simulate.p.value = TRUE)\n     \n     ## Case B. Raw data\n     x <- trunc(5 * runif(100))\n     chisq.test(table(x))            # NOT 'chisq.test(x)'!\n\n\n\n\n## Chi-Square test\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(freq)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  freq\nX-squared = 175.85, df = 2, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\nWe reject the null hypothesis that the proportion of seropositive individuals in the young, middle, and old age groups are the same.\n\n\n## Correlation\n\nFirst, we compute correlation by providing two vectors.\n\nLike other functions, if there are `NA`s, you get `NA` as the result. But if you specify use only the complete observations, then it will give you correlation using the non-missing data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(df$age, df$IgG_concentration, method=\"pearson\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] NA\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(df$age, df$IgG_concentration, method=\"pearson\", use = \"complete.obs\") #IF have missing data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2604783\n```\n\n\n:::\n:::\n\n\n\nSmall positive correlation between IgG concentration and age.\n\n## Correlation confidence interval\n\nThe function `cor.test()` also gives you the confidence interval of the correlation statistic. Note, it uses complete observations by default. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(df$age, df$IgG_concentration, method=\"pearson\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  df$age and df$IgG_concentration\nt = 6.7717, df = 630, p-value = 2.921e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1862722 0.3317295\nsample estimates:\n      cor \n0.2604783 \n```\n\n\n:::\n:::\n\n\n\n\n## T-test\n\nThe commonly used are:\n\n-   **one-sample t-test** -- used to test mean of a variable in one group (to the null hypothesis mean)\n-   **two-sample t-test** -- used to test difference in means of a variable between two groups (null hypothesis - the group means are the *same*)\n\n## T-test\n\nWe can use the `t.test()` function from the `stats` package.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?t.test\n```\n:::\n\nStudent's t-Test\n\nDescription:\n\n     Performs one and two sample t-tests on vectors of data.\n\nUsage:\n\n     t.test(x, ...)\n     \n     ## Default S3 method:\n     t.test(x, y = NULL,\n            alternative = c(\"two.sided\", \"less\", \"greater\"),\n            mu = 0, paired = FALSE, var.equal = FALSE,\n            conf.level = 0.95, ...)\n     \n     ## S3 method for class 'formula'\n     t.test(formula, data, subset, na.action, ...)\n     \nArguments:\n\n       x: a (non-empty) numeric vector of data values.\n\n       y: an optional (non-empty) numeric vector of data values.\n\nalternative: a character string specifying the alternative hypothesis,\n          must be one of '\"two.sided\"' (default), '\"greater\"' or\n          '\"less\"'.  You can specify just the initial letter.\n\n      mu: a number indicating the true value of the mean (or difference\n          in means if you are performing a two sample test).\n\n  paired: a logical indicating whether you want a paired t-test.\n\nvar.equal: a logical variable indicating whether to treat the two\n          variances as being equal. If 'TRUE' then the pooled variance\n          is used to estimate the variance otherwise the Welch (or\n          Satterthwaite) approximation to the degrees of freedom is\n          used.\n\nconf.level: confidence level of the interval.\n\n formula: a formula of the form 'lhs ~ rhs' where 'lhs' is a numeric\n          variable giving the data values and 'rhs' either '1' for a\n          one-sample or paired test or a factor with two levels giving\n          the corresponding groups. If 'lhs' is of class '\"Pair\"' and\n          'rhs' is '1', a paired test is done.\n\n    data: an optional matrix or data frame (or similar: see\n          'model.frame') containing the variables in the formula\n          'formula'.  By default the variables are taken from\n          'environment(formula)'.\n\n  subset: an optional vector specifying a subset of observations to be\n          used.\n\nna.action: a function which indicates what should happen when the data\n          contain 'NA's.  Defaults to 'getOption(\"na.action\")'.\n\n     ...: further arguments to be passed to or from methods.\n\nDetails:\n\n     'alternative = \"greater\"' is the alternative that 'x' has a larger\n     mean than 'y'. For the one-sample case: that the mean is positive.\n\n     If 'paired' is 'TRUE' then both 'x' and 'y' must be specified and\n     they must be the same length.  Missing values are silently removed\n     (in pairs if 'paired' is 'TRUE').  If 'var.equal' is 'TRUE' then\n     the pooled estimate of the variance is used.  By default, if\n     'var.equal' is 'FALSE' then the variance is estimated separately\n     for both groups and the Welch modification to the degrees of\n     freedom is used.\n\n     If the input data are effectively constant (compared to the larger\n     of the two means) an error is generated.\n\nValue:\n\n     A list with class '\"htest\"' containing the following components:\n\nstatistic: the value of the t-statistic.\n\nparameter: the degrees of freedom for the t-statistic.\n\n p.value: the p-value for the test.\n\nconf.int: a confidence interval for the mean appropriate to the\n          specified alternative hypothesis.\n\nestimate: the estimated mean or difference in means depending on\n          whether it was a one-sample test or a two-sample test.\n\nnull.value: the specified hypothesized value of the mean or mean\n          difference depending on whether it was a one-sample test or a\n          two-sample test.\n\n  stderr: the standard error of the mean (difference), used as\n          denominator in the t-statistic formula.\n\nalternative: a character string describing the alternative hypothesis.\n\n  method: a character string indicating what type of t-test was\n          performed.\n\ndata.name: a character string giving the name(s) of the data.\n\nSee Also:\n\n     'prop.test'\n\nExamples:\n\n     require(graphics)\n     \n     t.test(1:10, y = c(7:20))      # P = .00001855\n     t.test(1:10, y = c(7:20, 200)) # P = .1245    -- NOT significant anymore\n     \n     ## Classical example: Student's sleep data\n     plot(extra ~ group, data = sleep)\n     ## Traditional interface\n     with(sleep, t.test(extra[group == 1], extra[group == 2]))\n     \n     ## Formula interface\n     t.test(extra ~ group, data = sleep)\n     \n     ## Formula interface to one-sample test\n     t.test(extra ~ 1, data = sleep)\n     \n     ## Formula interface to paired test\n     ## The sleep data are actually paired, so could have been in wide format:\n     sleep2 <- reshape(sleep, direction = \"wide\", \n                       idvar = \"ID\", timevar = \"group\")\n     t.test(Pair(extra.1, extra.2) ~ 1, data = sleep2)\n\n\n\n## Running two-sample t-test\n\nThe **base R** - `t.test()` function from the `stats` package. It tests test difference in means of a variable between two groups. By default:\n\n-   tests whether difference in means of a variable is equal to 0 (default `mu=0`)\n-   uses \"two sided\" alternative (`alternative = \"two.sided\"`)\n-   returns result assuming confidence level 0.95 (`conf.level = 0.95`)\n-   assumes data are not paired (`paired = FALSE`)\n-   assumes true variance in the two groups is not equal (`var.equal = FALSE`)\n\n## Running two-sample t-test\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nIgG_young <- df$IgG_concentration[df$age_group==\"young\"]\nIgG_old <- df$IgG_concentration[df$age_group==\"old\"]\n\nt.test(IgG_young, IgG_old)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  IgG_young and IgG_old\nt = -6.1969, df = 259.54, p-value = 2.25e-09\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -111.09281  -57.51515\nsample estimates:\nmean of x mean of y \n 45.05056 129.35454 \n```\n\n\n:::\n:::\n\n\n\nThe mean IgG concenration of young and old is 45.05 and 129.35 IU/mL, respectively. We reject null hypothesis that the difference in the mean IgG concentration of young and old is 0 IU/mL.\n\n## Linear regression fit in R\n\nTo fit regression models in R, we use the function `glm()` (Generalized Linear Model).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?glm\n```\n:::\n\nFitting Generalized Linear Models\n\nDescription:\n\n     'glm' is used to fit generalized linear models, specified by\n     giving a symbolic description of the linear predictor and a\n     description of the error distribution.\n\nUsage:\n\n     glm(formula, family = gaussian, data, weights, subset,\n         na.action, start = NULL, etastart, mustart, offset,\n         control = list(...), model = TRUE, method = \"glm.fit\",\n         x = FALSE, y = TRUE, singular.ok = TRUE, contrasts = NULL, ...)\n     \n     glm.fit(x, y, weights = rep.int(1, nobs),\n             start = NULL, etastart = NULL, mustart = NULL,\n             offset = rep.int(0, nobs), family = gaussian(),\n             control = list(), intercept = TRUE, singular.ok = TRUE)\n     \n     ## S3 method for class 'glm'\n     weights(object, type = c(\"prior\", \"working\"), ...)\n     \nArguments:\n\n formula: an object of class '\"formula\"' (or one that can be coerced to\n          that class): a symbolic description of the model to be\n          fitted.  The details of model specification are given under\n          'Details'.\n\n  family: a description of the error distribution and link function to\n          be used in the model.  For 'glm' this can be a character\n          string naming a family function, a family function or the\n          result of a call to a family function.  For 'glm.fit' only\n          the third option is supported.  (See 'family' for details of\n          family functions.)\n\n    data: an optional data frame, list or environment (or object\n          coercible by 'as.data.frame' to a data frame) containing the\n          variables in the model.  If not found in 'data', the\n          variables are taken from 'environment(formula)', typically\n          the environment from which 'glm' is called.\n\n weights: an optional vector of 'prior weights' to be used in the\n          fitting process.  Should be 'NULL' or a numeric vector.\n\n  subset: an optional vector specifying a subset of observations to be\n          used in the fitting process.\n\nna.action: a function which indicates what should happen when the data\n          contain 'NA's.  The default is set by the 'na.action' setting\n          of 'options', and is 'na.fail' if that is unset.  The\n          'factory-fresh' default is 'na.omit'.  Another possible value\n          is 'NULL', no action.  Value 'na.exclude' can be useful.\n\n   start: starting values for the parameters in the linear predictor.\n\netastart: starting values for the linear predictor.\n\n mustart: starting values for the vector of means.\n\n  offset: this can be used to specify an _a priori_ known component to\n          be included in the linear predictor during fitting.  This\n          should be 'NULL' or a numeric vector of length equal to the\n          number of cases.  One or more 'offset' terms can be included\n          in the formula instead or as well, and if more than one is\n          specified their sum is used.  See 'model.offset'.\n\n control: a list of parameters for controlling the fitting process.\n          For 'glm.fit' this is passed to 'glm.control'.\n\n   model: a logical value indicating whether _model frame_ should be\n          included as a component of the returned value.\n\n  method: the method to be used in fitting the model.  The default\n          method '\"glm.fit\"' uses iteratively reweighted least squares\n          (IWLS): the alternative '\"model.frame\"' returns the model\n          frame and does no fitting.\n\n          User-supplied fitting functions can be supplied either as a\n          function or a character string naming a function, with a\n          function which takes the same arguments as 'glm.fit'.  If\n          specified as a character string it is looked up from within\n          the 'stats' namespace.\n\n    x, y: For 'glm': logical values indicating whether the response\n          vector and model matrix used in the fitting process should be\n          returned as components of the returned value.\n\n          For 'glm.fit': 'x' is a design matrix of dimension 'n * p',\n          and 'y' is a vector of observations of length 'n'.\n\nsingular.ok: logical; if 'FALSE' a singular fit is an error.\n\ncontrasts: an optional list. See the 'contrasts.arg' of\n          'model.matrix.default'.\n\nintercept: logical. Should an intercept be included in the _null_\n          model?\n\n  object: an object inheriting from class '\"glm\"'.\n\n    type: character, partial matching allowed.  Type of weights to\n          extract from the fitted model object.  Can be abbreviated.\n\n     ...: For 'glm': arguments to be used to form the default 'control'\n          argument if it is not supplied directly.\n\n          For 'weights': further arguments passed to or from other\n          methods.\n\nDetails:\n\n     A typical predictor has the form 'response ~ terms' where\n     'response' is the (numeric) response vector and 'terms' is a\n     series of terms which specifies a linear predictor for 'response'.\n     For 'binomial' and 'quasibinomial' families the response can also\n     be specified as a 'factor' (when the first level denotes failure\n     and all others success) or as a two-column matrix with the columns\n     giving the numbers of successes and failures.  A terms\n     specification of the form 'first + second' indicates all the terms\n     in 'first' together with all the terms in 'second' with any\n     duplicates removed.\n\n     A specification of the form 'first:second' indicates the set of\n     terms obtained by taking the interactions of all terms in 'first'\n     with all terms in 'second'.  The specification 'first*second'\n     indicates the _cross_ of 'first' and 'second'.  This is the same\n     as 'first + second + first:second'.\n\n     The terms in the formula will be re-ordered so that main effects\n     come first, followed by the interactions, all second-order, all\n     third-order and so on: to avoid this pass a 'terms' object as the\n     formula.\n\n     Non-'NULL' 'weights' can be used to indicate that different\n     observations have different dispersions (with the values in\n     'weights' being inversely proportional to the dispersions); or\n     equivalently, when the elements of 'weights' are positive integers\n     w_i, that each response y_i is the mean of w_i unit-weight\n     observations.  For a binomial GLM prior weights are used to give\n     the number of trials when the response is the proportion of\n     successes: they would rarely be used for a Poisson GLM.\n\n     'glm.fit' is the workhorse function: it is not normally called\n     directly but can be more efficient where the response vector,\n     design matrix and family have already been calculated.\n\n     If more than one of 'etastart', 'start' and 'mustart' is\n     specified, the first in the list will be used.  It is often\n     advisable to supply starting values for a 'quasi' family, and also\n     for families with unusual links such as 'gaussian(\"log\")'.\n\n     All of 'weights', 'subset', 'offset', 'etastart' and 'mustart' are\n     evaluated in the same way as variables in 'formula', that is first\n     in 'data' and then in the environment of 'formula'.\n\n     For the background to warning messages about 'fitted probabilities\n     numerically 0 or 1 occurred' for binomial GLMs, see Venables &\n     Ripley (2002, pp. 197-8).\n\nValue:\n\n     'glm' returns an object of class inheriting from '\"glm\"' which\n     inherits from the class '\"lm\"'. See later in this section.  If a\n     non-standard 'method' is used, the object will also inherit from\n     the class (if any) returned by that function.\n\n     The function 'summary' (i.e., 'summary.glm') can be used to obtain\n     or print a summary of the results and the function 'anova' (i.e.,\n     'anova.glm') to produce an analysis of variance table.\n\n     The generic accessor functions 'coefficients', 'effects',\n     'fitted.values' and 'residuals' can be used to extract various\n     useful features of the value returned by 'glm'.\n\n     'weights' extracts a vector of weights, one for each case in the\n     fit (after subsetting and 'na.action').\n\n     An object of class '\"glm\"' is a list containing at least the\n     following components:\n\ncoefficients: a named vector of coefficients\n\nresiduals: the _working_ residuals, that is the residuals in the final\n          iteration of the IWLS fit.  Since cases with zero weights are\n          omitted, their working residuals are 'NA'.\n\nfitted.values: the fitted mean values, obtained by transforming the\n          linear predictors by the inverse of the link function.\n\n    rank: the numeric rank of the fitted linear model.\n\n  family: the 'family' object used.\n\nlinear.predictors: the linear fit on link scale.\n\ndeviance: up to a constant, minus twice the maximized log-likelihood.\n          Where sensible, the constant is chosen so that a saturated\n          model has deviance zero.\n\n     aic: A version of Akaike's _An Information Criterion_, minus twice\n          the maximized log-likelihood plus twice the number of\n          parameters, computed via the 'aic' component of the family.\n          For binomial and Poison families the dispersion is fixed at\n          one and the number of parameters is the number of\n          coefficients.  For gaussian, Gamma and inverse gaussian\n          families the dispersion is estimated from the residual\n          deviance, and the number of parameters is the number of\n          coefficients plus one.  For a gaussian family the MLE of the\n          dispersion is used so this is a valid value of AIC, but for\n          Gamma and inverse gaussian families it is not.  For families\n          fitted by quasi-likelihood the value is 'NA'.\n\nnull.deviance: The deviance for the null model, comparable with\n          'deviance'. The null model will include the offset, and an\n          intercept if there is one in the model.  Note that this will\n          be incorrect if the link function depends on the data other\n          than through the fitted mean: specify a zero offset to force\n          a correct calculation.\n\n    iter: the number of iterations of IWLS used.\n\n weights: the _working_ weights, that is the weights in the final\n          iteration of the IWLS fit.\n\nprior.weights: the weights initially supplied, a vector of '1's if none\n          were.\n\ndf.residual: the residual degrees of freedom.\n\n df.null: the residual degrees of freedom for the null model.\n\n       y: if requested (the default) the 'y' vector used. (It is a\n          vector even for a binomial model.)\n\n       x: if requested, the model matrix.\n\n   model: if requested (the default), the model frame.\n\nconverged: logical. Was the IWLS algorithm judged to have converged?\n\nboundary: logical. Is the fitted value on the boundary of the\n          attainable values?\n\n    call: the matched call.\n\n formula: the formula supplied.\n\n   terms: the 'terms' object used.\n\n    data: the 'data argument'.\n\n  offset: the offset vector used.\n\n control: the value of the 'control' argument used.\n\n  method: the name of the fitter function used (when provided as a\n          'character' string to 'glm()') or the fitter 'function' (when\n          provided as that).\n\ncontrasts: (where relevant) the contrasts used.\n\n xlevels: (where relevant) a record of the levels of the factors used\n          in fitting.\n\nna.action: (where relevant) information returned by 'model.frame' on\n          the special handling of 'NA's.\n\n     In addition, non-empty fits will have components 'qr', 'R' and\n     'effects' relating to the final weighted linear fit.\n\n     Objects of class '\"glm\"' are normally of class 'c(\"glm\", \"lm\")',\n     that is inherit from class '\"lm\"', and well-designed methods for\n     class '\"lm\"' will be applied to the weighted linear model at the\n     final iteration of IWLS.  However, care is needed, as extractor\n     functions for class '\"glm\"' such as 'residuals' and 'weights' do\n     *not* just pick out the component of the fit with the same name.\n\n     If a 'binomial' 'glm' model was specified by giving a two-column\n     response, the weights returned by 'prior.weights' are the total\n     numbers of cases (factored by the supplied case weights) and the\n     component 'y' of the result is the proportion of successes.\n\nFitting functions:\n\n     The argument 'method' serves two purposes.  One is to allow the\n     model frame to be recreated with no fitting.  The other is to\n     allow the default fitting function 'glm.fit' to be replaced by a\n     function which takes the same arguments and uses a different\n     fitting algorithm.  If 'glm.fit' is supplied as a character string\n     it is used to search for a function of that name, starting in the\n     'stats' namespace.\n\n     The class of the object return by the fitter (if any) will be\n     prepended to the class returned by 'glm'.\n\nAuthor(s):\n\n     The original R implementation of 'glm' was written by Simon Davies\n     working for Ross Ihaka at the University of Auckland, but has\n     since been extensively re-written by members of the R Core team.\n\n     The design was inspired by the S function of the same name\n     described in Hastie & Pregibon (1992).\n\nReferences:\n\n     Dobson, A. J. (1990) _An Introduction to Generalized Linear\n     Models._ London: Chapman and Hall.\n\n     Hastie, T. J. and Pregibon, D. (1992) _Generalized linear models._\n     Chapter 6 of _Statistical Models in S_ eds J. M. Chambers and T.\n     J. Hastie, Wadsworth & Brooks/Cole.\n\n     McCullagh P. and Nelder, J. A. (1989) _Generalized Linear Models._\n     London: Chapman and Hall.\n\n     Venables, W. N. and Ripley, B. D. (2002) _Modern Applied\n     Statistics with S._ New York: Springer.\n\nSee Also:\n\n     'anova.glm', 'summary.glm', etc. for 'glm' methods, and the\n     generic functions 'anova', 'summary', 'effects', 'fitted.values',\n     and 'residuals'.\n\n     'lm' for non-generalized _linear_ models (which SAS calls GLMs,\n     for 'general' linear models).\n\n     'loglin' and 'loglm' (package 'MASS') for fitting log-linear\n     models (which binomial and Poisson GLMs are) to contingency\n     tables.\n\n     'bigglm' in package 'biglm' for an alternative way to fit GLMs to\n     large datasets (especially those with many cases).\n\n     'esoph', 'infert' and 'predict.glm' have examples of fitting\n     binomial glms.\n\nExamples:\n\n     ## Dobson (1990) Page 93: Randomized Controlled Trial :\n     counts <- c(18,17,15,20,10,20,25,13,12)\n     outcome <- gl(3,1,9)\n     treatment <- gl(3,3)\n     data.frame(treatment, outcome, counts) # showing data\n     glm.D93 <- glm(counts ~ outcome + treatment, family = poisson())\n     anova(glm.D93)\n     summary(glm.D93)\n     ## Computing AIC [in many ways]:\n     (A0 <- AIC(glm.D93))\n     (ll <- logLik(glm.D93))\n     A1 <- -2*c(ll) + 2*attr(ll, \"df\")\n     A2 <- glm.D93$family$aic(counts, mu=fitted(glm.D93), wt=1) +\n             2 * length(coef(glm.D93))\n     stopifnot(exprs = {\n       all.equal(A0, A1)\n       all.equal(A1, A2)\n       all.equal(A1, glm.D93$aic)\n     })\n     \n     \n     ## an example with offsets from Venables & Ripley (2002, p.189)\n     utils::data(anorexia, package = \"MASS\")\n     \n     anorex.1 <- glm(Postwt ~ Prewt + Treat + offset(Prewt),\n                     family = gaussian, data = anorexia)\n     summary(anorex.1)\n     \n     \n     # A Gamma example, from McCullagh & Nelder (1989, pp. 300-2)\n     clotting <- data.frame(\n         u = c(5,10,15,20,30,40,60,80,100),\n         lot1 = c(118,58,42,35,27,25,21,19,18),\n         lot2 = c(69,35,26,21,18,16,13,12,12))\n     summary(glm(lot1 ~ log(u), data = clotting, family = Gamma))\n     summary(glm(lot2 ~ log(u), data = clotting, family = Gamma))\n     ## Aliased (\"S\"ingular) -> 1 NA coefficient\n     (fS <- glm(lot2 ~ log(u) + log(u^2), data = clotting, family = Gamma))\n     tools::assertError(update(fS, singular.ok=FALSE), verbose=interactive())\n     ## -> .. \"singular fit encountered\"\n     \n     ## Not run:\n     \n     ## for an example of the use of a terms object as a formula\n     demo(glm.vr)\n     ## End(Not run)\n\n\n\n## Linear regression fit in R\n\nWe tend to focus on three arguments:\n\n- `formula` -- model formula written using names of columns in our data\n- `data` -- our data frame\n- `family` -- error distribution and link function\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <- glm(IgG_concentration~age+gender+slum, data=df, family=gaussian())\nfit2 <- glm(seropos~age_group+gender+slum, data=df, family = binomial(link = \"logit\"))\n```\n:::\n\n\n\n## `summary.glm()`\n\nThe `summary()` function when applied to a fit object based on a glm is technically the `summary.glm()` function and produces details of the model fit. Note on object oriented code.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/rstudio_script.png){width=200%}\n:::\n:::\n\nSummarizing Generalized Linear Model Fits\n\nDescription:\n\n     These functions are all 'methods' for class 'glm' or 'summary.glm'\n     objects.\n\nUsage:\n\n     ## S3 method for class 'glm'\n     summary(object, dispersion = NULL, correlation = FALSE,\n             symbolic.cor = FALSE, ...)\n     \n     ## S3 method for class 'summary.glm'\n     print(x, digits = max(3, getOption(\"digits\") - 3),\n           symbolic.cor = x$symbolic.cor,\n           signif.stars = getOption(\"show.signif.stars\"),\n           show.residuals = FALSE, ...)\n     \nArguments:\n\n  object: an object of class '\"glm\"', usually, a result of a call to\n          'glm'.\n\n       x: an object of class '\"summary.glm\"', usually, a result of a\n          call to 'summary.glm'.\n\ndispersion: the dispersion parameter for the family used.  Either a\n          single numerical value or 'NULL' (the default), when it is\n          inferred from 'object' (see 'Details').\n\ncorrelation: logical; if 'TRUE', the correlation matrix of the\n          estimated parameters is returned and printed.\n\n  digits: the number of significant digits to use when printing.\n\nsymbolic.cor: logical. If 'TRUE', print the correlations in a symbolic\n          form (see 'symnum') rather than as numbers.\n\nsignif.stars: logical. If 'TRUE', 'significance stars' are printed for\n          each coefficient.\n\nshow.residuals: logical. If 'TRUE' then a summary of the deviance\n          residuals is printed at the head of the output.\n\n     ...: further arguments passed to or from other methods.\n\nDetails:\n\n     'print.summary.glm' tries to be smart about formatting the\n     coefficients, standard errors, etc. and additionally gives\n     'significance stars' if 'signif.stars' is 'TRUE'.  The\n     'coefficients' component of the result gives the estimated\n     coefficients and their estimated standard errors, together with\n     their ratio.  This third column is labelled 't ratio' if the\n     dispersion is estimated, and 'z ratio' if the dispersion is known\n     (or fixed by the family).  A fourth column gives the two-tailed\n     p-value corresponding to the t or z ratio based on a Student t or\n     Normal reference distribution.  (It is possible that the\n     dispersion is not known and there are no residual degrees of\n     freedom from which to estimate it.  In that case the estimate is\n     'NaN'.)\n\n     Aliased coefficients are omitted in the returned object but\n     restored by the 'print' method.\n\n     Correlations are printed to two decimal places (or symbolically):\n     to see the actual correlations print 'summary(object)$correlation'\n     directly.\n\n     The dispersion of a GLM is not used in the fitting process, but it\n     is needed to find standard errors.  If 'dispersion' is not\n     supplied or 'NULL', the dispersion is taken as '1' for the\n     'binomial' and 'Poisson' families, and otherwise estimated by the\n     residual Chisquared statistic (calculated from cases with non-zero\n     weights) divided by the residual degrees of freedom.\n\n     'summary' can be used with Gaussian 'glm' fits to handle the case\n     of a linear regression with known error variance, something not\n     handled by 'summary.lm'.\n\nValue:\n\n     'summary.glm' returns an object of class '\"summary.glm\"', a list\n     with components\n\n    call: the component from 'object'.\n\n  family: the component from 'object'.\n\ndeviance: the component from 'object'.\n\ncontrasts: the component from 'object'.\n\ndf.residual: the component from 'object'.\n\nnull.deviance: the component from 'object'.\n\n df.null: the component from 'object'.\n\ndeviance.resid: the deviance residuals: see 'residuals.glm'.\n\ncoefficients: the matrix of coefficients, standard errors, z-values and\n          p-values.  Aliased coefficients are omitted.\n\n aliased: named logical vector showing if the original coefficients are\n          aliased.\n\ndispersion: either the supplied argument or the inferred/estimated\n          dispersion if the former is 'NULL'.\n\n      df: a 3-vector of the rank of the model and the number of\n          residual degrees of freedom, plus number of coefficients\n          (including aliased ones).\n\ncov.unscaled: the unscaled ('dispersion = 1') estimated covariance\n          matrix of the estimated coefficients.\n\ncov.scaled: ditto, scaled by 'dispersion'.\n\ncorrelation: (only if 'correlation' is true.)  The estimated\n          correlations of the estimated coefficients.\n\nsymbolic.cor: (only if 'correlation' is true.)  The value of the\n          argument 'symbolic.cor'.\n\nSee Also:\n\n     'glm', 'summary'.\n\nExamples:\n\n     ## For examples see example(glm)\n\n\n\n\n## Linear regression fit in R\n\nLets look at the output...\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = IgG_concentration ~ age + gender + slum, family = gaussian(), \n    data = df)\n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    46.132     16.774   2.750  0.00613 ** \nage             9.324      1.388   6.718 4.15e-11 ***\ngenderMale     -9.655     11.543  -0.836  0.40321    \nslumNon slum  -20.353     14.299  -1.423  0.15513    \nslumSlum      -29.705     25.009  -1.188  0.23536    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 20918.39)\n\n    Null deviance: 14141483  on 631  degrees of freedom\nResidual deviance: 13115831  on 627  degrees of freedom\n  (19 observations deleted due to missingness)\nAIC: 8087.9\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = seropos ~ age_group + gender + slum, family = binomial(link = \"logit\"), \n    data = df)\n\nCoefficients:\n                Estimate Std. Error z value Pr(>|z|)    \n(Intercept)      -1.3220     0.2516  -5.254 1.49e-07 ***\nage_groupmiddle   1.9020     0.2133   8.916  < 2e-16 ***\nage_groupold      2.8443     0.2522  11.278  < 2e-16 ***\ngenderMale       -0.1725     0.1895  -0.910    0.363    \nslumNon slum     -0.1099     0.2329  -0.472    0.637    \nslumSlum         -0.1073     0.4118  -0.261    0.794    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 866.98  on 631  degrees of freedom\nResidual deviance: 679.10  on 626  degrees of freedom\n  (19 observations deleted due to missingness)\nAIC: 691.1\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n\n\n\n## Summary\n\n- the `aggregate()` function can be used to conduct analyses across groups (i.e., categorical variables in the data(\n- the `table()` function can generate frequency tables for 2 plus variables, but to get percentage tables, the `prop.table()` is useful\n- the `chisq.test()` function tests independence of factor variables \n- the `cor()` or `cor.test()` functions can be used to calculate correlation between two numeric vectors\n- the `t.test()` functions conducts one and two sample (paired or unpaired) t-tests\n- the function `glm()` fits generalized linear modules to data and returns a fit object that can be read with the `summary()` function\n- changing the `family` argument in the `glm()` function allows you to fit models with different link functions\n\n## Acknowledgements\n\nThese are the materials we looked through, modified, or extracted to complete this module's lecture.\n\n-   [\"Introduction to R for Public Health Researchers\" Johns Hopkins University](https://jhudatascience.org/intro_to_r/)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}