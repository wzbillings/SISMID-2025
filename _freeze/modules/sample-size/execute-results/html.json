{
  "hash": "8d5915d0465a1891c9a06df63088385a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Power and Sample Size\"\nformat: revealjs\neditor: visual\necho: true\n---\n\n\n## Lets import our package for these calculations\n\nThe **`pwrss`** package provides flexible functions for calculating statistical power, required sample size, or detectable effect sizes across a wide range of models, including t-tests, ANOVA, correlation, regression, and generalized linear models. It is designed to handle complex scenarios, such as adjusting for covariates or specifying model-specific parameters like odds ratios or variance explained. This makes it a valuable tool for researchers planning studies to ensure sufficient power and precision in hypothesis testing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pwrss)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'pwrss'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:stats':\n\n    power.t.test\n```\n\n\n:::\n:::\n\n\n## Generic Functions\n\n-   These functions compute statistical power and can plot Type I and II errors if test statistics and degrees of freedom are known.\n\n-   They’re useful because z, t, \\$\\\\chi\\^{2}\\$, and F stats with degrees of freedom are often reported in publications or software outputs.\n\n-   Power can be calculated from test statistics as noncentrality parameters, but post-hoc power estimates should be interpreted cautiously.\n\n## *t* Test\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npower.t.test(ncp = 1.96, df = 99, alpha = 0.05,\n             alternative = \"equivalent\", plot = TRUE)\n```\n\n::: {.cell-output-display}\n![](sample-size_files/figure-revealjs/unnamed-chunk-2-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     power ncp.alt ncp.null.1 ncp.null.2 alpha df   t.crit.1  t.crit.2\n 0.2371389       0      -1.96       1.96  0.05 99 -0.3155295 0.3155295\n```\n\n\n:::\n:::\n\n\n## *z* Test\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npower.z.test(ncp = 1.96, alpha = 0.05, \n             alternative = \"not equal\", plot = TRUE)\n```\n\n::: {.cell-output-display}\n![](sample-size_files/figure-revealjs/unnamed-chunk-3-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     power ncp.alt ncp.null alpha  z.crit.1 z.crit.2\n 0.5000586    1.96        0  0.05 -1.959964 1.959964\n```\n\n\n:::\n:::\n\n\n## $\\chi^2$ Test\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npower.chisq.test(ncp = 15, df = 20,\n                 alpha = 0.05, plot = TRUE)\n```\n\n::: {.cell-output-display}\n![](sample-size_files/figure-revealjs/unnamed-chunk-4-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     power ncp.alt ncp.null alpha df chisq.crit\n 0.6110368      15        0  0.05 20   31.41043\n```\n\n\n:::\n:::\n\n\n## *F* Test\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npower.f.test(ncp = 3, df1 = 2, df2 = 98,\n             alpha = 0.05, plot = TRUE)\n```\n\n::: {.cell-output-display}\n![](sample-size_files/figure-revealjs/unnamed-chunk-5-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     power ncp.alt ncp.null alpha df1 df2   f.crit\n 0.3128778       3        0  0.05   2  98 3.089203\n```\n\n\n:::\n:::\n\n\n## Multiple Parameters\n\nMultiple parameters are allowed but plots should be turned off (`plot = FALSE`).\n\n## *t* Test\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npower.t.test(\n  ncp = c(0.50, 1.00, 1.50, 2.00, 2.50), \n  plot = FALSE,\n  df = 99, \n  alpha = 0.05, \n  alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      power ncp.alt ncp.null alpha df  t.crit.1 t.crit.2\n 0.07852973     0.5        0  0.05 99 -1.984217 1.984217\n 0.16769955     1.0        0  0.05 99 -1.984217 1.984217\n 0.31785490     1.5        0  0.05 99 -1.984217 1.984217\n 0.50826481     2.0        0  0.05 99 -1.984217 1.984217\n 0.69698027     2.5        0  0.05 99 -1.984217 1.984217\n```\n\n\n:::\n:::\n\n\n## *z* Test\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npower.z.test(\n  alpha = c(0.001, 0.010, 0.025, 0.050), \n  plot = FALSE,\n  ncp = 1.96, \n  alternative = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     power ncp.alt ncp.null alpha   z.crit\n 0.1291892    1.96        0 0.001 3.090232\n 0.3570528    1.96        0 0.010 2.326348\n 0.5000144    1.96        0 0.025 1.959964\n 0.6236747    1.96        0 0.050 1.644854\n```\n\n\n:::\n:::\n\n\n## $\\chi^2$ Test\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npower.chisq.test(\n  df = c(80, 90, 100, 120, 150, 200), \n  plot = FALSE, \n  ncp = 2, \n  alpha = 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      power ncp.alt ncp.null alpha  df chisq.crit\n 0.06989507       2        0  0.05  80   101.8795\n 0.06856779       2        0  0.05  90   113.1453\n 0.06746196       2        0  0.05 100   124.3421\n 0.06571411       2        0  0.05 120   146.5674\n 0.06382959       2        0  0.05 150   179.5806\n 0.06175379       2        0  0.05 200   233.9943\n```\n\n\n:::\n:::\n\n\n## Type I and Type II Error Plots\n\nWe use the `plot()` function (S3 method) is a wrapper around the generic functions above. Assign results of any `pwrss` function to an R object and pass it to `plot()` function.\n\n## Comparing two means\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\ndesign1 <- pwrss.t.2means(\n  mu1 = 0.20, \n  margin = -0.05, \n  paired = TRUE,\n  power = 0.80, \n  alpha = 0.05,\n  alternative = \"non-inferior\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Difference between Two means \n (Paired Samples t Test) \n H0: mu1 - mu2 <= margin \n HA: mu1 - mu2 > margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 101 \n ------------------------------ \n Alternative = \"non-inferior\" \n Degrees of freedom = 100 \n Non-centrality parameter = 2.512 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Comparing two means\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(design1)\n```\n\n::: {.cell-output-display}\n![](sample-size_files/figure-revealjs/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## ANCOVA\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\ndesign3 <- pwrss.f.ancova(\n  eta2 = 0.10, \n  n.levels = c(2,3),\n  power = .80, \n  alpha = 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Two-way Analysis of Variance (ANOVA) \n  H0: 'eta2' or 'f2' = 0 \n  HA: 'eta2' or 'f2' > 0 \n --------------------------------------\n Factor A: 2 levels \n Factor B: 3 levels \n --------------------------------------\n effect power n.total   ncp df1    df2\n      A   0.8      73 8.081   1 66.729\n      B   0.8      90 9.987   2 83.885\n  A x B   0.8      90 9.987   2 83.885\n --------------------------------------\n Type I error rate: 0.05\n```\n\n\n:::\n:::\n\n\n## ANCOVA\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(design3)\n```\n\n::: {.cell-output-display}\n![](sample-size_files/figure-revealjs/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Mean Difference (*t* Tests) Parametric Tests\n\n### Independent Samples *t* Test\n\nMore often than not, unstandardized means and standard deviations are reported in publications for descriptive purposes. Another reason is that they are more intuitive and interpretable (e.g. depression scale). Assume that for the first and second groups expected means are 30 and 28, and expected standard deviations are 12 and 8, respectively.\n\n## Calculate Statistical Power\n\nWhat is the statistical power given that the sample size for the second group is 50 (`n2 = 50`) and groups have equal sample sizes (`kappa = n1 / n2 = 1`)?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, kappa = 1, \n               n2 = 50, alpha = 0.05,\n               alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Difference between Two means \n (Independent Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.163 \n  n1 = 50 \n  n2 = 50 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 98 \n Non-centrality parameter = 0.981 \n Type I error rate = 0.05 \n Type II error rate = 0.837 \n```\n\n\n:::\n:::\n\n\n## Calculate Minimum Required Sample Size\n\nWhat is the minimum required sample size given that groups have equal sample sizes (`kappa = 1`)? ($\\kappa = n_1 / n_2$)\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, kappa = 1, \n               power = .80, alpha = 0.05,\n               alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Difference between Two means \n (Independent Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 410 \n  n2 = 410 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 818 \n Non-centrality parameter = 2.808 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Calculate Minimum Required Sample Size\n\nIt is sufficient to put pooled standard deviation for `sd1` because `sd2 = sd1` by default. In this case, for a pooled standard deviation of 10.198 the minimum required sample size can be calculated as\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 10.198, kappa = 1,\n               power = .80, alpha = 0.05,\n               alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Difference between Two means \n (Independent Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 410 \n  n2 = 410 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 818 \n Non-centrality parameter = 2.808 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Calculate Minimum Required Sample Size\n\nIt is sufficient to put Cohen's *d* or Hedge's *g* (standardized difference between two groups) for `mu1` because `mu2 = 0`, `sd1 = 1`, and `sd2 = sd1` by default. For example, for an effect size as small as 0.196 (based on previous example) the minimum required sample size can be calculated as\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.t.2means(mu1 = 0.196, kappa = 1,\n               power = .80, alpha = 0.05, \n               alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Difference between Two means \n (Independent Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 410 \n  n2 = 410 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 818 \n Non-centrality parameter = 2.806 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Paired Samples *t* Test\n\nAssume for the first (e.g. pretest) and second (e.g. posttest) time points expected means are 30 and 28 (a reduction of 2 points), and expected standard deviations are 12 and 8, respectively. Also assume a correlation of 0.50 between first and second measurements (by default `paired.r = 0.50`). What is the minimum required sample size?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, \n               paired = TRUE, paired.r = 0.50,\n               power = .80, alpha = 0.05,\n               alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Difference between Two means \n (Paired Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 222 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 221 \n Non-centrality parameter = 2.816 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Calculate Minimum Required Sample Size\n\nIt is sufficient to put standard deviation of the difference for `sd1` because `sd2 = sd1` by default. In this case, for a standard deviation of difference of 10.583 the minimum required sample size can be calculated as\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 10.583, \n               paired = TRUE, paired.r = 0.50,\n               power = .80, alpha = 0.05,\n               alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Difference between Two means \n (Paired Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 222 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 221 \n Non-centrality parameter = 2.816 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Calculate Minimum Required Sample Size\n\nIt is sufficient to put Cohen's *d* or Hedge's *g* (standardized difference between two time points) for `mu1` because `mu2 = 0`, `sd1 = sqrt(1/(2*(1-paired.r)))`, and `sd2 = sd1` by default. For example, for an effect size as small as 0.1883 (based on previous example) the minimum required sample size can be calculated as\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.t.2means(mu1 = 0.1883, paired = TRUE, paired.r = 0.50,\n               power = .80, alpha = 0.05, \n               alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Difference between Two means \n (Paired Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 224 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 223 \n Non-centrality parameter = 2.818 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Non-parametric Tests\n\n-   Means might be compared for variables that aren’t normally distributed, due to small samples or inherently non-normal population distributions (e.g. uniform, exponential).\n\n-   In such cases, t-tests may give biased results.\n\n-   For non-parametric tests, use `pwrss.np.2groups()` instead of `pwrss.t.2means()`, with the same arguments.\n\n## Non-parametric Tests\n\n-   You can specify the parent distribution (e.g. `\"normal\"`, `\"uniform\"`, `\"exponential\"`) using the `dist` argument.\n\n<!-- -->\n\n-   Although the function uses means and standard deviations as inputs, it actually tests differences in mean ranks.\n\n-   Mean differences are converted to Cohen’s *d* and then to probability of superiority, making it easier to compare and switch between parametric and non-parametric tests.\n\n## Independent Samples (Wilcoxon-Mann-Whitney Test)\n\nThe example below uses the same parameters as the example in the independent *t* test section.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.np.2groups(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, kappa = 1, \n               power = .80, alpha = 0.05,\n               alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Non-parametric Difference between Two Groups (Independent samples) \n Mann-Whitney U or Wilcoxon Rank-sum Test \n (a.k.a Wilcoxon-Mann-Whitney Test) \n Method: GUENTHER \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 429 \n  n2 = 429 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = 2.805 \n Degrees of freedom = 816.21 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Paired Samples (Wilcoxon Signed-rank Test)\n\nThe example below uses the same parameters as the example in the paired (dependent) *t* test section.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.np.2groups(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, \n               paired = TRUE, paired.r = 0.50,\n               power = .80, alpha = 0.05,\n               alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Non-parametric Difference between Two Groups (Dependent samples) \n Wilcoxon signed-rank Test for Matched Pairs \n Method: GUENTHER \n ------------------------------ \n  Statistical power = 0.8 \n  n = 233 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = 2.814 \n Degrees of freedom = 220.7 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\nIt is sufficient to put Cohen's *d* or Hedge's *g* (standardized difference between two groups or measurements) for `mu1` without specifying `mu2`, `sd1`, and `sd2`.\n\n## Non-inferiority, Superiority, and Equivalence\n\nThese tests are useful for testing practically significant difference (non-inferiority/superiority) or practically null difference (equivalence).\n\n## Parametric Tests\n\n**Non-inferiority**: The mean of group 1 is practically not smaller than the mean of group 2. The `mu1 - mu2` difference can be as small as -1 (`margin = -1`) but it will still be considered non-inferior. What is the minimum required sample size?\n\n## Parametric Tests\n\nWhen higher values of an outcome is better the margin takes NEGATIVE values; whereas when lower values of the outcome is better margin takes POSITIVE values.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, \n               margin = -1, power = 0.80,\n               alternative = \"non-inferior\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Difference between Two means \n (Independent Samples t Test) \n H0: mu1 - mu2 <= margin \n HA: mu1 - mu2 > margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 144 \n  n2 = 144 \n ------------------------------ \n Alternative = \"non-inferior\" \n Degrees of freedom = 286 \n Non-centrality parameter = 2.496 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Parametric Tests\n\n**Superiority**: The mean of group 1 is practically greater than the mean of group 2. The `mu1 - mu2` difference is at least greater than 1 (`margin = 1`). What is the minimum required sample size?\n\nWhen higher values of an outcome is better margin takes POSITIVE values; whereas when lower values of the outcome is better margin takes NEGATIVE values.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, \n               margin = 1, power = 0.80,\n               alternative = \"superior\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Difference between Two means \n (Independent Samples t Test) \n H0: mu1 - mu2 <= margin \n HA: mu1 - mu2 > margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1287 \n  n2 = 1287 \n ------------------------------ \n Alternative = \"superior\" \n Degrees of freedom = 2572 \n Non-centrality parameter = 2.487 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Parametric Tests\n\n**Equivalence**: The mean of group 1 is practically same as mean of group 2. The `mu1 - mu2` difference can be as small as -1 and as high as 1 (`margin = 1`). What is the minimum required sample size?\n\nSpecify the absolute value for the margin.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.t.2means(mu1 = 30, mu2 = 30, sd1 = 12, sd2 = 8, \n               margin = 1, power = 0.80,\n               alternative = \"equivalent\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Difference between Two means \n (Independent Samples t Test) \n H0: |mu1 - mu2| >= margin \n HA: |mu1 - mu2| < margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1783 \n  n2 = 1783 \n ------------------------------ \n Alternative = \"equivalent\" \n Degrees of freedom = 3564 \n Non-centrality parameter = -2.928 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Non-parametric Tests\n\n**Non-inferiority**: The mean of group 1 is practically not smaller than the mean of group 2. The `mu1 - mu2` difference can be as small as -1 (`margin = -1`). What is the minimum required sample size?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.np.2groups(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, \n               margin = -1, power = 0.80,\n               alternative = \"non-inferior\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Non-parametric Difference between Two Groups (Independent samples) \n Mann-Whitney U or Wilcoxon Rank-sum Test \n (a.k.a Wilcoxon-Mann-Whitney Test) \n Method: GUENTHER \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 151 \n  n2 = 151 \n ------------------------------ \n Alternative = \"non-inferior\" \n Non-centrality parameter = 2.492 \n Degrees of freedom = 285.13 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Non-parametric Tests\n\n**Superiority**: The mean of group 1 is practically greater than the mean of group 2. The `mu1 - mu2` difference is at least greater than 1 (`margin = 1`). What is the minimum required sample size?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.np.2groups(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, \n               margin = 1, power = 0.80,\n               alternative = \"superior\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Non-parametric Difference between Two Groups (Independent samples) \n Mann-Whitney U or Wilcoxon Rank-sum Test \n (a.k.a Wilcoxon-Mann-Whitney Test) \n Method: GUENTHER \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1348 \n  n2 = 1348 \n ------------------------------ \n Alternative = \"superior\" \n Non-centrality parameter = 2.487 \n Degrees of freedom = 2571.3 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Non-parametric Tests\n\n**Equivalence**: The mean of group 1 is practically same as mean of group 2. The `mu1 - mu2` difference can be as small as -1 and as high as 1 (`margin = 1`). What is the minimum required sample size?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.np.2groups(mu1 = 30, mu2 = 30, sd1 = 12, sd2 = 8, \n               margin = 1, power = 0.80,\n               alternative = \"equivalent\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Non-parametric Difference between Two Groups (Independent samples) \n Mann-Whitney U or Wilcoxon Rank-sum Test \n (a.k.a Wilcoxon-Mann-Whitney Test) \n Method: GUENTHER \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1867 \n  n2 = 1867 \n ------------------------------ \n Alternative = \"equivalent\" \n Non-centrality parameter = -2.927 \n Degrees of freedom = 3561.91 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n# Linear Regression (*F* and *t* Tests)\n\n## Linear Regression (*F* and *t* Tests)\n\n### Omnibus $F$ Test\n\n#### $R^2 > 0$ in Linear Regression\n\nOmnibus F test in multiple liner regression is used to test whether $R^2$ is greater than 0 (zero). Assume that we want to predict a continuous variable $Y$ using $X_{1}$, $X_{2}$, and $X_{2}$ variables (a combination of binary or continuous).\n\n$$\\begin{eqnarray}\n  Y &=& \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3} + r, \\quad r \\thicksim N(0,\\sigma^2) \\newline\n  \\end{eqnarray}$$\n\n## Linear Regression (*F* and *t* Tests)\n\nWe are expecting that these three variables explain 30% of the variance in the outcome ($R^2 = 0.30$ or `r2 = 0.30` in the code). What is the minimum required sample size?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.f.reg(r2 = 0.30, k = 3, power = 0.80, alpha = 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Linear Regression (F test) \n R-squared Deviation from 0 (zero) \n H0: r2 = 0 \n HA: r2 > 0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 30 \n ------------------------------ \n Numerator degrees of freedom = 3 \n Denominator degrees of freedom = 25.653 \n Non-centrality parameter = 12.709 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Single Regression Coefficient (*z* or *t* Test)\n\n### Standardized versus Unstandardized Input\n\nIn the earlier example, assume that we want to predict a continuous variable $Y$ using a continuous predictor $X_{1}$ but control for $X_{2}$, and $X_{2}$ variables (a combination of binary or continuous). We are mainly interested in the effect of $X_{1}$ and expect a standardized regression coefficient of $\\beta_{1} = 0.20$.\n\n$$\\begin{eqnarray}\n  Y &=& \\beta_{0} + \\color{red} {\\beta_{1} X_{1}} + \\beta_{2}X_{2} + \\beta_{3}X_{3} + r, \\quad r \\thicksim N(0,\\sigma^2) \\newline\n  \\end{eqnarray}$$\n\n## Single Regression Coefficient (*z* or *t* Test)\n\nAgain, we are expecting that these three variables explain 30% of the variance in the outcome ($R^2 = 0.30$). What is the minimum required sample size? It is sufficient to provide standardized regression coefficient for `beta1` because `sdx = 1` and `sdy = 1` by default.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.t.reg(beta1 = 0.20, k = 3, r2 = 0.30, \n            power = .80, alpha = 0.05, alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Linear Regression Coefficient (t Test) \n H0: beta1 = beta0 \n HA: beta1 != beta0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 140 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 135.331 \n Non-centrality parameter = 2.822 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Single Regression Coefficient (*z* or *t* Test)\n\nFor unstandardized coefficients specify `sdy` and `sdx`. Assume we are expecting an unstandardized regression coefficient of `beta1 = 0.60`, a standard deviation of `sdy = 12` for the outcome and a standard deviation of `sdx = 4` for the main predictor. What is the minimum required sample size?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.t.reg(beta1 = 0.60, sdy = 12, sdx = 4, k = 3, r2 = 0.30, \n            power = .80, alpha = 0.05, alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Linear Regression Coefficient (t Test) \n H0: beta1 = beta0 \n HA: beta1 != beta0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 140 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 135.331 \n Non-centrality parameter = 2.822 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Single Regression Coefficient (*z* or *t* Test)\n\n-   When the main predictor is binary (e.g. treatment/control), its standardized regression coefficient equals Cohen’s d.\n\n-   The predictor’s standard deviation is \\$\\\\sqrt{p(1-p)}\\$, where \\$p\\$ is the group proportion; for \\$p = 0.50\\$, what’s the minimum required sample size?\n\n-   Provide Cohen’s d for β₁ and set sdx = sqrt(p\\*(1-p)) when calculating.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\np <- 0.50\npwrss.t.reg(beta1 = 0.20, k = 3, r2 = 0.30, sdx = sqrt(p*(1-p)),\n            power = .80, alpha = 0.05, alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Linear Regression Coefficient (t Test) \n H0: beta1 = beta0 \n HA: beta1 != beta0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 552 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 547.355 \n Non-centrality parameter = 2.807 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Non-inferiority, Superiority, and Equivalence\n\nThese tests are useful for testing practically significant effects (non-inferiority/superiority) or practically null effects (equivalence).\n\n## **Non-inferiority**:\n\nThe intervention is expected to be non-inferior to some earlier or other interventions. Assume that the effect of an earlier or some other intervention is `beta0 = 0.10`. The `beta1 - beta0` is expected to be positive and should be at least -0.05 (`margin = -0.05`). What is the minimum required sample size?\n\n## **Non-inferiority**:\n\nThis is the case when higher values of an outcome is better. When lower values of an outcome is better the `beta1 - beta0` difference is expected to be NEGATIVE and the `margin` takes POSITIVE values.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\np <- 0.50\npwrss.t.reg(beta1 = 0.20, beta0 = 0.10, margin = -0.05, \n            k = 3, r2 = 0.30, sdx = sqrt(p*(1-p)),\n            power = .80, alpha = 0.05, alternative = \"non-inferior\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Linear Regression Coefficient (t Test) \n H0: beta1 - beta0 <= margin \n HA: beta1 - beta0 > margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 771 \n ------------------------------ \n Alternative = \"non-inferior\" \n Degrees of freedom = 766.745 \n Non-centrality parameter = 2.489 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## **Superiority**\n\nThe intervention is expected to be superior to some earlier or other interventions. Assume that the effect of an earlier or some other intervention is `beta0 = 0.10`. The `beta1 - beta0` is expected to be positive and should be at least 0.05 (`margin = 0.05`). What is the minimum required sample size?\n\n## **Superiority**\n\nThis is the case when higher values of an outcome is better. When lower values of an outcome is better `beta1 - beta0` difference is expected to be NEGATIVE and the `margin` takes NEGATIVE values.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\np <- 0.50\npwrss.t.reg(beta1 = 0.20, beta0 = 0.10, margin = 0.05, \n            k = 3, r2 = 0.30, sdx = sqrt(p*(1-p)),\n            power = .80, alpha = 0.05, alternative = \"superior\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Linear Regression Coefficient (t Test) \n H0: beta1 - beta0 <= margin \n HA: beta1 - beta0 > margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 6926 \n ------------------------------ \n Alternative = \"superior\" \n Degrees of freedom = 6921.818 \n Non-centrality parameter = 2.487 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## **Equivalence**\n\nThe intervention is expected to be equivalent to some earlier or other interventions. Assume the effect of an earlier or some other intervention is `beta0 = 0.20`. The `beta1 - beta0` is expected to be within -0.05 and 0.05 (`margin = 0.05`). What is the minimum required sample size?\n\n## **Equivalence**\n\n`margin` always takes positive values for equivalence. Specify the absolute value.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\np <- 0.50\npwrss.t.reg(beta1 = 0.20, beta0 = 0.20, margin = 0.05, \n            k = 3, r2 = 0.30, sdx = sqrt(p*(1-p)),\n            power = .80, alpha = 0.05, alternative = \"equivalent\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Linear Regression Coefficient (t Test) \n H0: |beta1 - beta0| >= margin \n HA: |beta1 - beta0| < margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 9593 \n ------------------------------ \n Alternative = \"equivalent\" \n Degrees of freedom = 9588.862 \n Non-centrality parameter = -2.927 2.927 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n# Logistic Regression (Wald's *z* Test)\n\n## Logistic Regression (Wald's *z* Test)\n\nIn logistic regression a binary outcome variable (0/1: failed/passed, dead/alive, absent/present) is modeled by predicting probability of being in group 1 ($P_1$) via logit transformation (natural logarithm of odds). The base probability $P_0$ is the overall probability of being in group 1 without influence of predictors in the model (null). Under alternative hypothesis, the probability of being in group 1 ($P_1$) deviate from $P_0$ depending on the value of the predictor; whereas under null it is same as the $P_0$.\n\n## Logistic Regression (Wald's *z* Test)\n\nA model with one main predictor ($X_1$) and two other covariates ($X_2$ and $X_3$) can be constructed as\n\n$$\\begin{eqnarray}\n  ln(\\frac{P_1}{1- P_1}) &=& \\beta_{0} + \\color{red} {\\beta_{1} X_{1}} + \\beta_{2}X_{2} + \\beta_{3}X_{3} \\newline\n  \\end{eqnarray}$$\n\n## Logistic Regression (Wald's *z* Test)\n\nTherefore the odds ratio is defined as $$OR = exp(\\beta_1) = \\frac{P_1}{1- P_1} / \\frac{P_0}{1- P_0}$$\n\n## Logistic Regression (Wald's *z* Test)\n\n### Example:\n\n-   A squared multiple correlation of 0.20 between $X_1$ and other covariates (`r2.other.x = 0.20` in the code). It can be found in the form of adjusted R-square via regressing $X_1$ on $X_2$ and $X_3$. Higher values require larger sample sizes. The default is 0 (zero).\n-   A base probability of $P_0 = 0.15$. This is the rate when predictor $X_1 = 0$ or when $\\beta_1 = 0$.\n-   Increasing $X_1$ from 0 to 1 reduces the probability of being in group 1 from 0.15 to 0.10 ($P_1 = 0.10$).\n\n## Logistic Regression (Wald's *z* Test)\n\nWhat is the minimum required sample size? There are three types of specification to statistical power or sample size calculations; (i) probability specification, (ii) odds ratio specification, and (iii) regression coefficient specification (as in standard software output).\n\n## Logistic Regression (Wald's *z* Test)\n\n**Probability specification**:\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.logreg(p0 = 0.15, p1 = 0.10, r2.other.x = 0.20,\n               power = 0.80, alpha = 0.05, \n               dist = \"normal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Logistic Regression Coefficient \n (Large Sample Approx. Wald's z Test) \n H0: beta1 = 0 \n HA: beta1 != 0 \n Distribution of X = 'normal' \n Method = DEMIDENKO(VC) \n ------------------------------ \n  Statistical power = 0.8 \n  n = 365 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.766 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Logistic Regression (Wald's *z* Test)\n\n**Odds ratio specification**: $$OR = \\frac{P_1}{1-P1} / \\frac{P_0}{1-P_0} = \\frac{0.10}{1-0.10} / \\frac{0.15}{1-0.15} = 0.6296$$\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.logreg(p0 = 0.15, odds.ratio = 0.6296, r2.other.x = 0.20,\n               alpha = 0.05, power = 0.80,\n               dist = \"normal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Logistic Regression Coefficient \n (Large Sample Approx. Wald's z Test) \n H0: beta1 = 0 \n HA: beta1 != 0 \n Distribution of X = 'normal' \n Method = DEMIDENKO(VC) \n ------------------------------ \n  Statistical power = 0.8 \n  n = 365 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.766 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Logistic Regression (Wald's *z* Test)\n\n**Regression coefficient specification**: $$\\beta_1 = ln(\\frac{P_1}{1-P1} / \\frac{P_0}{1-P_0}) = ln(0.6296) = -0.4626$$\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.logreg(p0 = 0.15, beta1 = -0.4626, r2.other.x = 0.20,\n               alpha = 0.05, power = 0.80,\n               dist = \"normal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Logistic Regression Coefficient \n (Large Sample Approx. Wald's z Test) \n H0: beta1 = 0 \n HA: beta1 != 0 \n Distribution of X = 'normal' \n Method = DEMIDENKO(VC) \n ------------------------------ \n  Statistical power = 0.8 \n  n = 365 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.766 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Logistic Regression (Wald's *z* Test)\n\n**Change the distribution's parameters for predictor X**:\n\nThe mean and standard deviation of a normally distributed main predictor is 0 and 1 by default. They can be modified. In the following example the mean is 25 and the standard deviation is 8.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\ndist.x <- list(dist = \"normal\", mean = 25, sd = 8)\n\npwrss.z.logreg(p0 = 0.15, beta1 = -0.4626, r2.other.x = 0.20,\n               alpha = 0.05, power = 0.80,\n               dist = dist.x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Logistic Regression Coefficient \n (Large Sample Approx. Wald's z Test) \n H0: beta1 = 0 \n HA: beta1 != 0 \n Distribution of X = 'normal' \n Method = DEMIDENKO(VC) \n ------------------------------ \n  Statistical power = 0.8 \n  n = 2435 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.423 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Logistic Regression (Wald's *z* Test)\n\n**Change the distribution family of predictor X**:\n\nMore distribution types are supported by the function. For example, the main predictor can be binary (e.g. treatment/control groups). Often half of the sample is assigned to the treatment group and the other half to the control (`prob = 0.50` by default).\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.logreg(p0 = 0.15, beta1 = -0.4626, r2.other.x = 0.20,\n               alpha = 0.05, power = 0.80,\n               dist = \"bernoulli\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Logistic Regression Coefficient \n (Large Sample Approx. Wald's z Test) \n H0: beta1 = 0 \n HA: beta1 != 0 \n Distribution of X = 'bernoulli' \n Method = DEMIDENKO(VC) \n ------------------------------ \n  Statistical power = 0.8 \n  n = 1723 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.789 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Logistic Regression (Wald's *z* Test)\n\n**Change the treatment group allocation rate of the binary predictor X** (`prob = 0.40`):\n\nSometimes treatment groups cost more per subject or are harder to recruit than control groups, making an unbalanced sample practical. For example, with 40% of subjects in the treatment group, what is the minimum required sample size?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\ndist.x <- list(dist = \"bernoulli\", prob = 0.40)\n\npwrss.z.logreg(p0 = 0.15, beta1 = -0.4626, r2.other.x = 0.20,\n               alpha = 0.05, power = 0.80,\n               dist = dist.x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Logistic Regression Coefficient \n (Large Sample Approx. Wald's z Test) \n H0: beta1 = 0 \n HA: beta1 != 0 \n Distribution of X = 'bernoulli' \n Method = DEMIDENKO(VC) \n ------------------------------ \n  Statistical power = 0.8 \n  n = 1826 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.766 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n# Analysis of (Co)Variance (*F* Test)\n\n## ANOVA: Analysis of Variance ANCOVA: Analysis of Covariance\n\n-   **One-way ANOVA/ANCOVA:** Compares means across groups; ANCOVA adjusts for covariates.\n\n-   **Two- or Three-way ANOVA/ANCOVA:** Examines interactions between factors; ANCOVA adjusts interactions for covariates and determines required sample size for complex designs.\n\n## One-way\n\n### ANOVA\n\nA researcher is expecting a difference of Cohen's *d* = 0.50 between treatment and control groups (two levels) translating into $\\eta^2 = 0.059$ (`eta2 = 0.059`). Means are not adjusted for any covariates. What is the minimum required sample size?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.f.ancova(eta2 = 0.059, n.levels = 2,\n               power = .80, alpha = 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n One-way Analysis of Variance (ANOVA) \n  H0: 'eta2' or 'f2' = 0 \n  HA: 'eta2' or 'f2' > 0 \n --------------------------------------\n Factor A: 2 levels \n --------------------------------------\n effect power n.total   ncp df1     df2\n      A   0.8     128 7.971   1 125.132\n --------------------------------------\n Type I error rate: 0.05\n```\n\n\n:::\n:::\n\n\n## One-way\n\n### ANCOVA\n\nA researcher is expecting an adjusted difference of Cohen's *d* = 0.45 between treatment and control groups (`n.levels = 2`) after controlling for the pretest (`n.cov = 1`) translating into partial $\\eta^2 = 0.048$ (`eta2 = 0.048`). What is the minimum required sample size?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.f.ancova(eta2 = 0.048, n.levels = 2, n.cov = 1,\n               alpha = 0.05, power = .80)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n One-way Analysis of Covariance (ANCOVA) \n  H0: 'eta2' or 'f2' = 0 \n  HA: 'eta2' or 'f2' > 0 \n --------------------------------------\n Factor A: 2 levels \n --------------------------------------\n effect power n.total   ncp df1     df2\n      A   0.8     158 7.948   1 154.626\n --------------------------------------\n Type I error rate: 0.05\n```\n\n\n:::\n:::\n\n\n`n.cov` (or `n.covariates`) argument has trivial effect on the results. The difference between ANOVA and ANCOVA procedure depends on whether the effect (`eta2`) is unadjusted or covariate-adjusted.\n\n## Two-way\n\n### ANOVA\n\nA researcher is expecting a partial $\\eta^2 = 0.03$ (`eta2 = 0.03`) for interaction of treatment/control (Factor A: two levels) with gender (Factor B: two levels). Thus, `n.levels = c(2,2)`. What is the minimum required sample size?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.f.ancova(eta2 = 0.03, n.levels = c(2,2),\n               alpha = 0.05, power = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Two-way Analysis of Variance (ANOVA) \n  H0: 'eta2' or 'f2' = 0 \n  HA: 'eta2' or 'f2' > 0 \n --------------------------------------\n Factor A: 2 levels \n Factor B: 2 levels \n --------------------------------------\n effect power n.total   ncp df1    df2\n      A   0.8     256 7.909   1 251.73\n      B   0.8     256 7.909   1 251.73\n  A x B   0.8     256 7.909   1 251.73\n --------------------------------------\n Type I error rate: 0.05\n```\n\n\n:::\n:::\n\n\n## Two-way\n\n### ANCOVA\n\nA researcher is expecting a partial $\\eta^2 = 0.02$ (`eta2 = 0.02`) for interaction of treatment/control (Factor A) with gender (Factor B) adjusted for the pretest (`n.cov = 1`). What is the minimum required sample size?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.f.ancova(eta2 = 0.02, n.levels = c(2,2), n.cov = 1,\n               alpha = 0.05, power = .80)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Two-way Analysis of Covariance (ANCOVA) \n  H0: 'eta2' or 'f2' = 0 \n  HA: 'eta2' or 'f2' > 0 \n --------------------------------------\n Factor A: 2 levels \n Factor B: 2 levels \n --------------------------------------\n effect power n.total   ncp df1     df2\n      A   0.8     387 7.889   1 381.539\n      B   0.8     387 7.889   1 381.539\n  A x B   0.8     387 7.889   1 381.539\n --------------------------------------\n Type I error rate: 0.05\n```\n\n\n:::\n:::\n\n\n# Correlation(s) (*z* Test)\n\n## One Correlation\n\n**One-sided Test**: Assume that the expected correlation is 0.20 and it is greater than 0.10. What is the minimum required sample size?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.corr(r = 0.20, r0 = 0.10,\n             power = 0.80, alpha = 0.05, \n             alternative = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n A Correlation against a Constant (z Test) \n H0: r = r0 \n HA: r > r0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 593 \n ------------------------------ \n Alternative = \"greater\" \n Non-centrality parameter = 2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## One Correlation\n\n**Two-sided Test**: Assume that the expected correlation is 0.20 and it is different from 0 (zero). The correlation could be 0.20 as well as -0.20. What is the minimum required sample size?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.corr(r = 0.20, r0 = 0,\n             power = 0.80, alpha = 0.05, \n             alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n A Correlation against a Constant (z Test) \n H0: r = r0 \n HA: r != r0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 194 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = 2.802 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Difference between Two Correlations (Independent)\n\nAssume that the expected correlations in the first and second groups are 0.30 and 0.20, respectively (`r1 = 0.30` and `r2 = 0.20`).\n\n## Difference between Two Correlations (Independent)\n\n**One-sided Test**: Expecting `r1 - r2` greater than 0 (zero). The difference could be 0.10 but could not be -0.10. What is the minimum required sample size?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.2corrs(r1 = 0.30, r2 = 0.20,\n               power = .80, alpha = 0.05, \n               alternative = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Difference between Two Correlations \n (Independent Samples z Test) \n H0: r1 = r2 \n HA: r1 > r2 \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1088 \n  n2 = 1088 \n ------------------------------ \n Alternative = \"greater\" \n Non-centrality parameter = 2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Difference between Two Correlations (Independent)\n\n**Two-sided Test**: Expecting `r1 - r2` different from 0 (zero). The difference could be -0.10 as well as 0.10. What is the minimum required sample size?\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.2corrs(r1 = 0.30, r2 = 0.20,\n               power = .80, alpha = 0.05, \n               alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Difference between Two Correlations \n (Independent Samples z Test) \n H0: r1 = r2 \n HA: r1 != r2 \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1380 \n  n2 = 1380 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = 2.802 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n# Proportion(s) (*z* Test)\n\n## Proportion(s) (*z* Test)\n\n## One Proportion\n\nIn the following examples `p` is the proportion under alternative hypothesis and `p0` is the proportion under null hypothesis.\n\n## Proportion(s) (*z* Test)\n\n**One-sided Test**: Expecting `p - p0` smaller than 0 (zero).\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# normal approximation\npwrss.z.prop(p = 0.45, p0 = 0.50,\n             alpha = 0.05, power = 0.80,\n             alternative = \"less\",\n             arcsin.trans = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Approach: Normal Approximation \n A Proportion against a Constant (z Test) \n H0: p = p0 \n HA: p < p0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 613 \n ------------------------------ \n Alternative = \"less\" \n Non-centrality parameter = -2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n\n```{.r .cell-code}\n# arcsine transformation\npwrss.z.prop(p = 0.45, p0 = 0.50,\n             alpha = 0.05, power = 0.80,\n             alternative = \"less\",\n             arcsin.trans = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Approach: Arcsine Transformation \n A Proportion against a Constant (z Test) \n H0: p = p0 \n HA: p < p0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 617 \n ------------------------------ \n Alternative = \"less\" \n Non-centrality parameter = -2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Proportion(s) (*z* Test)\n\n**Two-sided Test**: Expecting `p - p0` smaller than 0 (zero) or greater than 0 (zero).\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.prop(p = 0.45, p0 = 0.50,\n             alpha = 0.05, power = 0.80,\n             alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Approach: Normal Approximation \n A Proportion against a Constant (z Test) \n H0: p = p0 \n HA: p != p0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 778 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.802 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Proportion(s) (*z* Test)\n\n**Non-inferiority Test**: The case when smaller proportion is better. Expecting `p - p0` smaller than 0.01.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.prop(p = 0.45, p0 = 0.50, margin = 0.01,\n             alpha = 0.05, power = 0.80,\n             alternative = \"non-inferior\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Approach: Normal Approximation \n A Proportion against a Constant (z Test) \n H0: p - p0 <= margin \n HA: p - p0 > margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 426 \n ------------------------------ \n Alternative = \"non-inferior\" \n Non-centrality parameter = -2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Proportion(s) (*z* Test)\n\n**Non-inferiority Test**: The case when bigger proportion is better. Expecting `p - p0` greater than -0.01.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.prop(p = 0.55, p0 = 0.50, margin = -0.01,\n             alpha = 0.05, power = 0.80,\n             alternative = \"non-inferior\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Approach: Normal Approximation \n A Proportion against a Constant (z Test) \n H0: p - p0 <= margin \n HA: p - p0 > margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 426 \n ------------------------------ \n Alternative = \"non-inferior\" \n Non-centrality parameter = 2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Proportion(s) (*z* Test)\n\n**Superiority Test**: The case when smaller proportion is better. Expecting `p - p0` smaller than -0.01.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.prop(p = 0.45, p0 = 0.50, margin = -0.01,\n             alpha = 0.05, power = 0.80,\n             alternative = \"superior\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Approach: Normal Approximation \n A Proportion against a Constant (z Test) \n H0: p - p0 <= margin \n HA: p - p0 > margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 957 \n ------------------------------ \n Alternative = \"superior\" \n Non-centrality parameter = -2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Proportion(s) (*z* Test)\n\n**Superiority Test**: The case when bigger proportion is better. Expecting `p - p0` greater than 0.01.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.prop(p = 0.55, p0 = 0.50, margin = 0.01,\n             alpha = 0.05, power = 0.80,\n             alternative = \"superior\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Approach: Normal Approximation \n A Proportion against a Constant (z Test) \n H0: p - p0 <= margin \n HA: p - p0 > margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 957 \n ------------------------------ \n Alternative = \"superior\" \n Non-centrality parameter = 2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Proportion(s) (*z* Test)\n\n**Equivalence Test**: Expecting `p - p0` between -0.01 and 0.01.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.prop(p = 0.50, p0 = 0.50, margin = 0.01,\n             alpha = 0.05, power = 0.80,\n             alternative = \"equivalent\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Approach: Normal Approximation \n A Proportion against a Constant (z Test) \n H0: |p - p0| >= margin \n HA: |p - p0| < margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 21410 \n ------------------------------ \n Alternative = \"equivalent\" \n Non-centrality parameter = -2.926 2.926 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n# Difference between Two Proportions (Independent)\n\n## Difference between Two Proportions (Independent)\n\nIn the following examples `p1` and `p2` are proportions for the first and second groups under alternative hypothesis. The null hypothesis state `p1 = p2` or `p1 - p2 = 0`.\n\n## Difference between Two Proportions\n\n**One-sided Test:** Expecting `p1 - p2` smaller than 0 (zero).\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# normal approximation\npwrss.z.2props(p1 = 0.45, p2 = 0.50,\n               alpha = 0.05, power = 0.80,\n               alternative = \"less\",\n               arcsin.trans = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Approach: Normal Approximation \n Difference between Two Proportions \n (Independent Samples z Test) \n H0: p1 = p2 \n HA: p1 < p2 \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1231 \n  n2 = 1231 \n ------------------------------ \n Alternative = \"less\" \n Non-centrality parameter = -2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Difference between Two Proportions\n\n**Two-sided Test:** Expecting `p1 - p2` smaller than 0 (zero) or greater than 0 (zero).\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.2props(p1 = 0.45, p2 = 0.50,\n               alpha = 0.05, power = 0.80,\n               alternative = \"not equal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Approach: Normal Approximation \n Difference between Two Proportions \n (Independent Samples z Test) \n H0: p1 = p2 \n HA: p1 != p2 \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1562 \n  n2 = 1562 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.802 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Difference between Two Proportions\n\n**Non-inferiority Test**: The case when smaller proportion is better. Expecting `p1 - p2` smaller than 0.01.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.2props(p1 = 0.45, p2 = 0.50, margin = 0.01,\n               alpha = 0.05, power = 0.80,\n               alternative = \"non-inferior\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Approach: Normal Approximation \n Difference between Two Proportions \n (Independent Samples z Test) \n H0: p1 - p2 <= margin \n HA: p1 - p2 > margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 855 \n  n2 = 855 \n ------------------------------ \n Alternative = \"non-inferior\" \n Non-centrality parameter = -2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Difference between Two Proportions\n\n**Non-inferiority Test**: The case when bigger proportion is better. Expecting `p1 - p2` greater than -0.01.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.2props(p1 = 0.55, p2 = 0.50,  margin = -0.01,\n               alpha = 0.05, power = 0.80,\n               alternative = \"non-inferior\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Approach: Normal Approximation \n Difference between Two Proportions \n (Independent Samples z Test) \n H0: p1 - p2 <= margin \n HA: p1 - p2 > margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 855 \n  n2 = 855 \n ------------------------------ \n Alternative = \"non-inferior\" \n Non-centrality parameter = 2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Difference between Two Proportions\n\n**Superiority Test**: The case when smaller proportion is better. Expecting `p1 - p2` smaller than -0.01.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.2props(p1 = 0.45, p2 = 0.50, margin = -0.01,\n               alpha = 0.05, power = 0.80,\n               alternative = \"superior\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Approach: Normal Approximation \n Difference between Two Proportions \n (Independent Samples z Test) \n H0: p1 - p2 <= margin \n HA: p1 - p2 > margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1923 \n  n2 = 1923 \n ------------------------------ \n Alternative = \"superior\" \n Non-centrality parameter = -2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Difference between Two Proportions\n\n**Superiority Test**: The case when bigger proportion is better. Expecting `p1 - p2` greater than 0.01.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.2props(p1 = 0.55, p2 = 0.50, margin = 0.01,\n               alpha = 0.05, power = 0.80,\n               alternative = \"superior\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Approach: Normal Approximation \n Difference between Two Proportions \n (Independent Samples z Test) \n H0: p1 - p2 <= margin \n HA: p1 - p2 > margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1923 \n  n2 = 1923 \n ------------------------------ \n Alternative = \"superior\" \n Non-centrality parameter = 2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n\n\n## Difference between Two Proportions\n\n**Equivalence Test**: Expecting `p1 - p2` between -0.01 and 0.01.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\npwrss.z.2props(p1 = 0.50, p2 = 0.50, margin = 0.01,\n               alpha = 0.05, power = 0.80,\n               alternative = \"equivalent\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Approach: Normal Approximation \n Difference between Two Proportions \n (Independent Samples z Test) \n H0: |p1 - p2| >= margin \n HA: |p1 - p2| < margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 42820 \n  n2 = 42820 \n ------------------------------ \n Alternative = \"equivalent\" \n Non-centrality parameter = -2.926 2.926 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n```\n\n\n:::\n:::\n",
    "supporting": [
      "sample-size_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}