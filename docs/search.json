[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "Meeting times:\nLocation: Room 201",
    "crumbs": [
      "Course Schedule"
    ]
  },
  {
    "objectID": "schedule.html#day-01-monday",
    "href": "schedule.html#day-01-monday",
    "title": "Course Schedule",
    "section": "Day 01 – Monday",
    "text": "Day 01 – Monday\n\n\n\n\n\n\n\nTime\nSection\n\n\n\n\n09:00 am - 10:00 am\nModule 0, 1, 2\n\n\n10:00 am - 10:30 am\nCoffee break\n\n\n10:30 am - 12:00 pm\nmodule 3 and 4\n\n\n12:00 pm - 01:30 pm\nLunch (2nd floor lobby)\n\n\n01:30 pm - 03:00 pm\nmodule 4\n\n\n03:00 pm - 03:30 pm\nCoffee break\n\n\n03:30 pm - 05:00 pm\nData science walkthrough 1, exercise 1 work time, questions\n\n\n05:00 pm - 07:00 pm\nNetworking night and poster session, Randal Rollins P01",
    "crumbs": [
      "Course Schedule"
    ]
  },
  {
    "objectID": "schedule.html#day-02-tuesday",
    "href": "schedule.html#day-02-tuesday",
    "title": "Course Schedule",
    "section": "Day 02 – Tuesday",
    "text": "Day 02 – Tuesday\n\n\n\n\n\n\n\nTime\nSection\n\n\n\n\n09:00 am - 10:30 am\nFunctional programming, useful packages\n\n\n10:30 am - 11:00 am\nCoffee break\n\n\n11:00 am - 12:00 pm\nS3/Formulas, arrow\n\n\n12:00 pm - 01:00 pm\nLunch (2nd floor lobby); Lunch and Learn!\n\n\n01:00 pm - 03:00 pm\nPower (maybe power simulation), catch-up time\n\n\n03:00 pm - 03:30 pm\nCoffee break\n\n\n03:30 pm - 05:00 pm\nExercise 2, data science walkthrough",
    "crumbs": [
      "Course Schedule"
    ]
  },
  {
    "objectID": "schedule.html#day-03-wednesday",
    "href": "schedule.html#day-03-wednesday",
    "title": "Course Schedule",
    "section": "Day 03 – Wednesday",
    "text": "Day 03 – Wednesday\n\n\n\n\n\n\n\nTime\nSection\n\n\n\n\n09:00 am - 10:30 am\nReview or advanced topics\n\n\n10:30 am - 11:00 am\nCoffee break\n\n\n11:00 am - 12:00 pm\nReview or advanced topics",
    "crumbs": [
      "Course Schedule"
    ]
  },
  {
    "objectID": "modules/sample-size.html#lets-import-our-package-for-these-calculations",
    "href": "modules/sample-size.html#lets-import-our-package-for-these-calculations",
    "title": "Power and Sample Size",
    "section": "Lets import our package for these calculations",
    "text": "Lets import our package for these calculations\nThe pwrss package provides flexible functions for calculating statistical power, required sample size, or detectable effect sizes across a wide range of models, including t-tests, ANOVA, correlation, regression, and generalized linear models. It is designed to handle complex scenarios, such as adjusting for covariates or specifying model-specific parameters like odds ratios or variance explained. This makes it a valuable tool for researchers planning studies to ensure sufficient power and precision in hypothesis testing.\n\nlibrary(pwrss)\n\n\nAttaching package: 'pwrss'\n\n\nThe following object is masked from 'package:stats':\n\n    power.t.test",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#generic-functions",
    "href": "modules/sample-size.html#generic-functions",
    "title": "Power and Sample Size",
    "section": "Generic Functions",
    "text": "Generic Functions\n\nThese functions compute statistical power and can plot Type I and II errors if test statistics and degrees of freedom are known.\nThey’re useful because z, t, $\\chi^{2}$, and F stats with degrees of freedom are often reported in publications or software outputs.\nPower can be calculated from test statistics as noncentrality parameters, but post-hoc power estimates should be interpreted cautiously.",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#t-test",
    "href": "modules/sample-size.html#t-test",
    "title": "Power and Sample Size",
    "section": "t Test",
    "text": "t Test\n\npower.t.test(ncp = 1.96, df = 99, alpha = 0.05,\n             alternative = \"equivalent\", plot = TRUE)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#t-test-output",
    "href": "modules/sample-size.html#t-test-output",
    "title": "Power and Sample Size",
    "section": "t Test",
    "text": "t Test\n\n\n\n\n\n\n\n\n     power ncp.alt ncp.null.1 ncp.null.2 alpha df   t.crit.1  t.crit.2\n 0.2371389       0      -1.96       1.96  0.05 99 -0.3155295 0.3155295",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#z-test",
    "href": "modules/sample-size.html#z-test",
    "title": "Power and Sample Size",
    "section": "z Test",
    "text": "z Test\n\npower.z.test(ncp = 1.96, alpha = 0.05, \n             alternative = \"not equal\", plot = TRUE)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#z-test-output",
    "href": "modules/sample-size.html#z-test-output",
    "title": "Power and Sample Size",
    "section": "z Test",
    "text": "z Test\n\n\n\n\n\n\n\n\n     power ncp.alt ncp.null alpha  z.crit.1 z.crit.2\n 0.5000586    1.96        0  0.05 -1.959964 1.959964",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#chi2-test",
    "href": "modules/sample-size.html#chi2-test",
    "title": "Power and Sample Size",
    "section": "\\(\\chi^2\\) Test",
    "text": "\\(\\chi^2\\) Test\n\npower.chisq.test(ncp = 15, df = 20,\n                 alpha = 0.05, plot = TRUE)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#chi2-test-output",
    "href": "modules/sample-size.html#chi2-test-output",
    "title": "Power and Sample Size",
    "section": "\\(\\chi^2\\) Test",
    "text": "\\(\\chi^2\\) Test\n\n\n\n\n\n\n\n\n     power ncp.alt ncp.null alpha df chisq.crit\n 0.6110368      15        0  0.05 20   31.41043",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#f-test",
    "href": "modules/sample-size.html#f-test",
    "title": "Power and Sample Size",
    "section": "F Test",
    "text": "F Test\n\npower.f.test(ncp = 3, df1 = 2, df2 = 98,\n             alpha = 0.05, plot = TRUE)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#f-test-output",
    "href": "modules/sample-size.html#f-test-output",
    "title": "Power and Sample Size",
    "section": "F Test",
    "text": "F Test\n\n\n\n\n\n\n\n\n     power ncp.alt ncp.null alpha df1 df2   f.crit\n 0.3128778       3        0  0.05   2  98 3.089203",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#multiple-parameters",
    "href": "modules/sample-size.html#multiple-parameters",
    "title": "Power and Sample Size",
    "section": "Multiple Parameters",
    "text": "Multiple Parameters\nMultiple parameters are allowed but plots should be turned off (plot = FALSE).",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#t-test-1",
    "href": "modules/sample-size.html#t-test-1",
    "title": "Power and Sample Size",
    "section": "t Test",
    "text": "t Test\n\npower.t.test(\n  ncp = c(0.50, 1.00, 1.50, 2.00, 2.50), \n  plot = FALSE,\n  df = 99, \n  alpha = 0.05, \n  alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#t-test-1-output",
    "href": "modules/sample-size.html#t-test-1-output",
    "title": "Power and Sample Size",
    "section": "t Test",
    "text": "t Test\n\n      power ncp.alt ncp.null alpha df  t.crit.1 t.crit.2\n 0.07852973     0.5        0  0.05 99 -1.984217 1.984217\n 0.16769955     1.0        0  0.05 99 -1.984217 1.984217\n 0.31785490     1.5        0  0.05 99 -1.984217 1.984217\n 0.50826481     2.0        0  0.05 99 -1.984217 1.984217\n 0.69698027     2.5        0  0.05 99 -1.984217 1.984217",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#z-test-1",
    "href": "modules/sample-size.html#z-test-1",
    "title": "Power and Sample Size",
    "section": "z Test",
    "text": "z Test\n\npower.z.test(\n  alpha = c(0.001, 0.010, 0.025, 0.050), \n  plot = FALSE,\n  ncp = 1.96, \n  alternative = \"greater\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#z-test-1-output",
    "href": "modules/sample-size.html#z-test-1-output",
    "title": "Power and Sample Size",
    "section": "z Test",
    "text": "z Test\n\n     power ncp.alt ncp.null alpha   z.crit\n 0.1291892    1.96        0 0.001 3.090232\n 0.3570528    1.96        0 0.010 2.326348\n 0.5000144    1.96        0 0.025 1.959964\n 0.6236747    1.96        0 0.050 1.644854",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#chi2-test-1",
    "href": "modules/sample-size.html#chi2-test-1",
    "title": "Power and Sample Size",
    "section": "\\(\\chi^2\\) Test",
    "text": "\\(\\chi^2\\) Test\n\npower.chisq.test(\n  df = c(80, 90, 100, 120, 150, 200), \n  plot = FALSE, \n  ncp = 2, \n  alpha = 0.05)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#chi2-test-1-output",
    "href": "modules/sample-size.html#chi2-test-1-output",
    "title": "Power and Sample Size",
    "section": "\\(\\chi^2\\) Test",
    "text": "\\(\\chi^2\\) Test\n\n      power ncp.alt ncp.null alpha  df chisq.crit\n 0.06989507       2        0  0.05  80   101.8795\n 0.06856779       2        0  0.05  90   113.1453\n 0.06746196       2        0  0.05 100   124.3421\n 0.06571411       2        0  0.05 120   146.5674\n 0.06382959       2        0  0.05 150   179.5806\n 0.06175379       2        0  0.05 200   233.9943",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#type-i-and-type-ii-error-plots",
    "href": "modules/sample-size.html#type-i-and-type-ii-error-plots",
    "title": "Power and Sample Size",
    "section": "Type I and Type II Error Plots",
    "text": "Type I and Type II Error Plots\nWe use the plot() function (S3 method) is a wrapper around the generic functions above. Assign results of any pwrss function to an R object and pass it to plot() function.",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#comparing-two-means",
    "href": "modules/sample-size.html#comparing-two-means",
    "title": "Power and Sample Size",
    "section": "Comparing two means",
    "text": "Comparing two means\n\ndesign1 &lt;- pwrss.t.2means(\n  mu1 = 0.20, \n  margin = -0.05, \n  paired = TRUE,\n  power = 0.80, \n  alpha = 0.05,\n  alternative = \"non-inferior\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#comparing-two-means-output",
    "href": "modules/sample-size.html#comparing-two-means-output",
    "title": "Power and Sample Size",
    "section": "Comparing two means",
    "text": "Comparing two means\n\n Difference between Two means \n (Paired Samples t Test) \n H0: mu1 - mu2 &lt;= margin \n HA: mu1 - mu2 &gt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 101 \n ------------------------------ \n Alternative = \"non-inferior\" \n Degrees of freedom = 100 \n Non-centrality parameter = 2.512 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#comparing-two-means-1",
    "href": "modules/sample-size.html#comparing-two-means-1",
    "title": "Power and Sample Size",
    "section": "Comparing two means",
    "text": "Comparing two means\n\nplot(design1)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#ancova",
    "href": "modules/sample-size.html#ancova",
    "title": "Power and Sample Size",
    "section": "ANCOVA",
    "text": "ANCOVA\n\ndesign3 &lt;- pwrss.f.ancova(\n  eta2 = 0.10, \n  n.levels = c(2,3),\n  power = .80, \n  alpha = 0.05)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#ancova-output",
    "href": "modules/sample-size.html#ancova-output",
    "title": "Power and Sample Size",
    "section": "ANCOVA",
    "text": "ANCOVA\n\n Two-way Analysis of Variance (ANOVA) \n  H0: 'eta2' or 'f2' = 0 \n  HA: 'eta2' or 'f2' &gt; 0 \n --------------------------------------\n Factor A: 2 levels \n Factor B: 3 levels \n --------------------------------------\n effect power n.total   ncp df1    df2\n      A   0.8      73 8.081   1 66.729\n      B   0.8      90 9.987   2 83.885\n  A x B   0.8      90 9.987   2 83.885\n --------------------------------------\n Type I error rate: 0.05",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#ancova-1",
    "href": "modules/sample-size.html#ancova-1",
    "title": "Power and Sample Size",
    "section": "ANCOVA",
    "text": "ANCOVA\n\nplot(design3)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#mean-difference-t-tests-parametric-tests",
    "href": "modules/sample-size.html#mean-difference-t-tests-parametric-tests",
    "title": "Power and Sample Size",
    "section": "Mean Difference (t Tests) Parametric Tests",
    "text": "Mean Difference (t Tests) Parametric Tests\nIndependent Samples t Test\nMore often than not, unstandardized means and standard deviations are reported in publications for descriptive purposes. Another reason is that they are more intuitive and interpretable (e.g. depression scale). Assume that for the first and second groups expected means are 30 and 28, and expected standard deviations are 12 and 8, respectively.",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#calculate-statistical-power",
    "href": "modules/sample-size.html#calculate-statistical-power",
    "title": "Power and Sample Size",
    "section": "Calculate Statistical Power",
    "text": "Calculate Statistical Power\nWhat is the statistical power given that the sample size for the second group is 50 (n2 = 50) and groups have equal sample sizes (kappa = n1 / n2 = 1)?\n\npwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, kappa = 1, \n               n2 = 50, alpha = 0.05,\n               alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#calculate-statistical-power-output",
    "href": "modules/sample-size.html#calculate-statistical-power-output",
    "title": "Power and Sample Size",
    "section": "Calculate Statistical Power",
    "text": "Calculate Statistical Power\n\n Difference between Two means \n (Independent Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.163 \n  n1 = 50 \n  n2 = 50 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 98 \n Non-centrality parameter = 0.981 \n Type I error rate = 0.05 \n Type II error rate = 0.837",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#calculate-minimum-required-sample-size",
    "href": "modules/sample-size.html#calculate-minimum-required-sample-size",
    "title": "Power and Sample Size",
    "section": "Calculate Minimum Required Sample Size",
    "text": "Calculate Minimum Required Sample Size\nWhat is the minimum required sample size given that groups have equal sample sizes (kappa = 1)? (\\(\\kappa = n_1 / n_2\\))\n\npwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, kappa = 1, \n               power = .80, alpha = 0.05,\n               alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#calculate-minimum-required-sample-size-output",
    "href": "modules/sample-size.html#calculate-minimum-required-sample-size-output",
    "title": "Power and Sample Size",
    "section": "Calculate Minimum Required Sample Size",
    "text": "Calculate Minimum Required Sample Size\n\n Difference between Two means \n (Independent Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 410 \n  n2 = 410 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 818 \n Non-centrality parameter = 2.808 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#calculate-minimum-required-sample-size-1",
    "href": "modules/sample-size.html#calculate-minimum-required-sample-size-1",
    "title": "Power and Sample Size",
    "section": "Calculate Minimum Required Sample Size",
    "text": "Calculate Minimum Required Sample Size\nIt is sufficient to put pooled standard deviation for sd1 because sd2 = sd1 by default. In this case, for a pooled standard deviation of 10.198 the minimum required sample size can be calculated as\n\npwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 10.198, kappa = 1,\n               power = .80, alpha = 0.05,\n               alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#calculate-minimum-required-sample-size-1-output",
    "href": "modules/sample-size.html#calculate-minimum-required-sample-size-1-output",
    "title": "Power and Sample Size",
    "section": "Calculate Minimum Required Sample Size",
    "text": "Calculate Minimum Required Sample Size\n\n Difference between Two means \n (Independent Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 410 \n  n2 = 410 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 818 \n Non-centrality parameter = 2.808 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#calculate-minimum-required-sample-size-2",
    "href": "modules/sample-size.html#calculate-minimum-required-sample-size-2",
    "title": "Power and Sample Size",
    "section": "Calculate Minimum Required Sample Size",
    "text": "Calculate Minimum Required Sample Size\nIt is sufficient to put Cohen’s d or Hedge’s g (standardized difference between two groups) for mu1 because mu2 = 0, sd1 = 1, and sd2 = sd1 by default. For example, for an effect size as small as 0.196 (based on previous example) the minimum required sample size can be calculated as\n\npwrss.t.2means(mu1 = 0.196, kappa = 1,\n               power = .80, alpha = 0.05, \n               alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#calculate-minimum-required-sample-size-2-output",
    "href": "modules/sample-size.html#calculate-minimum-required-sample-size-2-output",
    "title": "Power and Sample Size",
    "section": "Calculate Minimum Required Sample Size",
    "text": "Calculate Minimum Required Sample Size\n\n Difference between Two means \n (Independent Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 410 \n  n2 = 410 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 818 \n Non-centrality parameter = 2.806 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#paired-samples-t-test",
    "href": "modules/sample-size.html#paired-samples-t-test",
    "title": "Power and Sample Size",
    "section": "Paired Samples t Test",
    "text": "Paired Samples t Test\nAssume for the first (e.g. pretest) and second (e.g. posttest) time points expected means are 30 and 28 (a reduction of 2 points), and expected standard deviations are 12 and 8, respectively. Also assume a correlation of 0.50 between first and second measurements (by default paired.r = 0.50). What is the minimum required sample size?\n\npwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, \n               paired = TRUE, paired.r = 0.50,\n               power = .80, alpha = 0.05,\n               alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#paired-samples-t-test-output",
    "href": "modules/sample-size.html#paired-samples-t-test-output",
    "title": "Power and Sample Size",
    "section": "Paired Samples t Test",
    "text": "Paired Samples t Test\n\n Difference between Two means \n (Paired Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 222 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 221 \n Non-centrality parameter = 2.816 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#calculate-minimum-required-sample-size-3",
    "href": "modules/sample-size.html#calculate-minimum-required-sample-size-3",
    "title": "Power and Sample Size",
    "section": "Calculate Minimum Required Sample Size",
    "text": "Calculate Minimum Required Sample Size\nIt is sufficient to put standard deviation of the difference for sd1 because sd2 = sd1 by default. In this case, for a standard deviation of difference of 10.583 the minimum required sample size can be calculated as\n\npwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 10.583, \n               paired = TRUE, paired.r = 0.50,\n               power = .80, alpha = 0.05,\n               alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#calculate-minimum-required-sample-size-3-output",
    "href": "modules/sample-size.html#calculate-minimum-required-sample-size-3-output",
    "title": "Power and Sample Size",
    "section": "Calculate Minimum Required Sample Size",
    "text": "Calculate Minimum Required Sample Size\n\n Difference between Two means \n (Paired Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 222 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 221 \n Non-centrality parameter = 2.816 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#calculate-minimum-required-sample-size-4",
    "href": "modules/sample-size.html#calculate-minimum-required-sample-size-4",
    "title": "Power and Sample Size",
    "section": "Calculate Minimum Required Sample Size",
    "text": "Calculate Minimum Required Sample Size\nIt is sufficient to put Cohen’s d or Hedge’s g (standardized difference between two time points) for mu1 because mu2 = 0, sd1 = sqrt(1/(2*(1-paired.r))), and sd2 = sd1 by default. For example, for an effect size as small as 0.1883 (based on previous example) the minimum required sample size can be calculated as\n\npwrss.t.2means(mu1 = 0.1883, paired = TRUE, paired.r = 0.50,\n               power = .80, alpha = 0.05, \n               alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#calculate-minimum-required-sample-size-4-output",
    "href": "modules/sample-size.html#calculate-minimum-required-sample-size-4-output",
    "title": "Power and Sample Size",
    "section": "Calculate Minimum Required Sample Size",
    "text": "Calculate Minimum Required Sample Size\n\n Difference between Two means \n (Paired Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 224 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 223 \n Non-centrality parameter = 2.818 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#non-parametric-tests",
    "href": "modules/sample-size.html#non-parametric-tests",
    "title": "Power and Sample Size",
    "section": "Non-parametric Tests",
    "text": "Non-parametric Tests\n\nMeans might be compared for variables that aren’t normally distributed, due to small samples or inherently non-normal population distributions (e.g. uniform, exponential).\nIn such cases, t-tests may give biased results.\nFor non-parametric tests, use pwrss.np.2groups() instead of pwrss.t.2means(), with the same arguments.",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#non-parametric-tests-1",
    "href": "modules/sample-size.html#non-parametric-tests-1",
    "title": "Power and Sample Size",
    "section": "Non-parametric Tests",
    "text": "Non-parametric Tests\n\nYou can specify the parent distribution (e.g. \"normal\", \"uniform\", \"exponential\") using the dist argument.\n\n\n\nAlthough the function uses means and standard deviations as inputs, it actually tests differences in mean ranks.\nMean differences are converted to Cohen’s d and then to probability of superiority, making it easier to compare and switch between parametric and non-parametric tests.",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#independent-samples-wilcoxon-mann-whitney-test",
    "href": "modules/sample-size.html#independent-samples-wilcoxon-mann-whitney-test",
    "title": "Power and Sample Size",
    "section": "Independent Samples (Wilcoxon-Mann-Whitney Test)",
    "text": "Independent Samples (Wilcoxon-Mann-Whitney Test)\nThe example below uses the same parameters as the example in the independent t test section.\n\npwrss.np.2groups(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, kappa = 1, \n               power = .80, alpha = 0.05,\n               alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#independent-samples-wilcoxon-mann-whitney-test-output",
    "href": "modules/sample-size.html#independent-samples-wilcoxon-mann-whitney-test-output",
    "title": "Power and Sample Size",
    "section": "Independent Samples (Wilcoxon-Mann-Whitney Test)",
    "text": "Independent Samples (Wilcoxon-Mann-Whitney Test)\n\n Non-parametric Difference between Two Groups (Independent samples) \n Mann-Whitney U or Wilcoxon Rank-sum Test \n (a.k.a Wilcoxon-Mann-Whitney Test) \n Method: GUENTHER \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 429 \n  n2 = 429 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = 2.805 \n Degrees of freedom = 816.21 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#paired-samples-wilcoxon-signed-rank-test",
    "href": "modules/sample-size.html#paired-samples-wilcoxon-signed-rank-test",
    "title": "Power and Sample Size",
    "section": "Paired Samples (Wilcoxon Signed-rank Test)",
    "text": "Paired Samples (Wilcoxon Signed-rank Test)\nThe example below uses the same parameters as the example in the paired (dependent) t test section.\n\npwrss.np.2groups(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, \n               paired = TRUE, paired.r = 0.50,\n               power = .80, alpha = 0.05,\n               alternative = \"not equal\")\n\n\nIt is sufficient to put Cohen’s d or Hedge’s g (standardized difference between two groups or measurements) for mu1 without specifying mu2, sd1, and sd2.",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#paired-samples-wilcoxon-signed-rank-test-output",
    "href": "modules/sample-size.html#paired-samples-wilcoxon-signed-rank-test-output",
    "title": "Power and Sample Size",
    "section": "Paired Samples (Wilcoxon Signed-rank Test)",
    "text": "Paired Samples (Wilcoxon Signed-rank Test)\n\n Non-parametric Difference between Two Groups (Dependent samples) \n Wilcoxon signed-rank Test for Matched Pairs \n Method: GUENTHER \n ------------------------------ \n  Statistical power = 0.8 \n  n = 233 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = 2.814 \n Degrees of freedom = 220.7 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#non-inferiority-superiority-and-equivalence",
    "href": "modules/sample-size.html#non-inferiority-superiority-and-equivalence",
    "title": "Power and Sample Size",
    "section": "Non-inferiority, Superiority, and Equivalence",
    "text": "Non-inferiority, Superiority, and Equivalence\nThese tests are useful for testing practically significant difference (non-inferiority/superiority) or practically null difference (equivalence).",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#parametric-tests",
    "href": "modules/sample-size.html#parametric-tests",
    "title": "Power and Sample Size",
    "section": "Parametric Tests",
    "text": "Parametric Tests\nNon-inferiority: The mean of group 1 is practically not smaller than the mean of group 2. The mu1 - mu2 difference can be as small as -1 (margin = -1) but it will still be considered non-inferior. What is the minimum required sample size?",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#parametric-tests-1",
    "href": "modules/sample-size.html#parametric-tests-1",
    "title": "Power and Sample Size",
    "section": "Parametric Tests",
    "text": "Parametric Tests\nWhen higher values of an outcome is better the margin takes NEGATIVE values; whereas when lower values of the outcome is better margin takes POSITIVE values.\n\npwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, \n               margin = -1, power = 0.80,\n               alternative = \"non-inferior\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#parametric-tests-1-output",
    "href": "modules/sample-size.html#parametric-tests-1-output",
    "title": "Power and Sample Size",
    "section": "Parametric Tests",
    "text": "Parametric Tests\n\n Difference between Two means \n (Independent Samples t Test) \n H0: mu1 - mu2 &lt;= margin \n HA: mu1 - mu2 &gt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 144 \n  n2 = 144 \n ------------------------------ \n Alternative = \"non-inferior\" \n Degrees of freedom = 286 \n Non-centrality parameter = 2.496 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#parametric-tests-2",
    "href": "modules/sample-size.html#parametric-tests-2",
    "title": "Power and Sample Size",
    "section": "Parametric Tests",
    "text": "Parametric Tests\nSuperiority: The mean of group 1 is practically greater than the mean of group 2. The mu1 - mu2 difference is at least greater than 1 (margin = 1). What is the minimum required sample size?\nWhen higher values of an outcome is better margin takes POSITIVE values; whereas when lower values of the outcome is better margin takes NEGATIVE values.\n\npwrss.t.2means(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, \n               margin = 1, power = 0.80,\n               alternative = \"superior\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#parametric-tests-2-output",
    "href": "modules/sample-size.html#parametric-tests-2-output",
    "title": "Power and Sample Size",
    "section": "Parametric Tests",
    "text": "Parametric Tests\n\n Difference between Two means \n (Independent Samples t Test) \n H0: mu1 - mu2 &lt;= margin \n HA: mu1 - mu2 &gt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1287 \n  n2 = 1287 \n ------------------------------ \n Alternative = \"superior\" \n Degrees of freedom = 2572 \n Non-centrality parameter = 2.487 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#parametric-tests-3",
    "href": "modules/sample-size.html#parametric-tests-3",
    "title": "Power and Sample Size",
    "section": "Parametric Tests",
    "text": "Parametric Tests\nEquivalence: The mean of group 1 is practically same as mean of group 2. The mu1 - mu2 difference can be as small as -1 and as high as 1 (margin = 1). What is the minimum required sample size?\nSpecify the absolute value for the margin.\n\npwrss.t.2means(mu1 = 30, mu2 = 30, sd1 = 12, sd2 = 8, \n               margin = 1, power = 0.80,\n               alternative = \"equivalent\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#parametric-tests-3-output",
    "href": "modules/sample-size.html#parametric-tests-3-output",
    "title": "Power and Sample Size",
    "section": "Parametric Tests",
    "text": "Parametric Tests\n\n Difference between Two means \n (Independent Samples t Test) \n H0: |mu1 - mu2| &gt;= margin \n HA: |mu1 - mu2| &lt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1783 \n  n2 = 1783 \n ------------------------------ \n Alternative = \"equivalent\" \n Degrees of freedom = 3564 \n Non-centrality parameter = -2.928 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#non-parametric-tests-2",
    "href": "modules/sample-size.html#non-parametric-tests-2",
    "title": "Power and Sample Size",
    "section": "Non-parametric Tests",
    "text": "Non-parametric Tests\nNon-inferiority: The mean of group 1 is practically not smaller than the mean of group 2. The mu1 - mu2 difference can be as small as -1 (margin = -1). What is the minimum required sample size?\n\npwrss.np.2groups(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, \n               margin = -1, power = 0.80,\n               alternative = \"non-inferior\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#non-parametric-tests-2-output",
    "href": "modules/sample-size.html#non-parametric-tests-2-output",
    "title": "Power and Sample Size",
    "section": "Non-parametric Tests",
    "text": "Non-parametric Tests\n\n Non-parametric Difference between Two Groups (Independent samples) \n Mann-Whitney U or Wilcoxon Rank-sum Test \n (a.k.a Wilcoxon-Mann-Whitney Test) \n Method: GUENTHER \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 151 \n  n2 = 151 \n ------------------------------ \n Alternative = \"non-inferior\" \n Non-centrality parameter = 2.492 \n Degrees of freedom = 285.13 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#non-parametric-tests-3",
    "href": "modules/sample-size.html#non-parametric-tests-3",
    "title": "Power and Sample Size",
    "section": "Non-parametric Tests",
    "text": "Non-parametric Tests\nSuperiority: The mean of group 1 is practically greater than the mean of group 2. The mu1 - mu2 difference is at least greater than 1 (margin = 1). What is the minimum required sample size?\n\npwrss.np.2groups(mu1 = 30, mu2 = 28, sd1 = 12, sd2 = 8, \n               margin = 1, power = 0.80,\n               alternative = \"superior\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#non-parametric-tests-3-output",
    "href": "modules/sample-size.html#non-parametric-tests-3-output",
    "title": "Power and Sample Size",
    "section": "Non-parametric Tests",
    "text": "Non-parametric Tests\n\n Non-parametric Difference between Two Groups (Independent samples) \n Mann-Whitney U or Wilcoxon Rank-sum Test \n (a.k.a Wilcoxon-Mann-Whitney Test) \n Method: GUENTHER \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1348 \n  n2 = 1348 \n ------------------------------ \n Alternative = \"superior\" \n Non-centrality parameter = 2.487 \n Degrees of freedom = 2571.3 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#non-parametric-tests-4",
    "href": "modules/sample-size.html#non-parametric-tests-4",
    "title": "Power and Sample Size",
    "section": "Non-parametric Tests",
    "text": "Non-parametric Tests\nEquivalence: The mean of group 1 is practically same as mean of group 2. The mu1 - mu2 difference can be as small as -1 and as high as 1 (margin = 1). What is the minimum required sample size?\n\npwrss.np.2groups(mu1 = 30, mu2 = 30, sd1 = 12, sd2 = 8, \n               margin = 1, power = 0.80,\n               alternative = \"equivalent\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#non-parametric-tests-4-output",
    "href": "modules/sample-size.html#non-parametric-tests-4-output",
    "title": "Power and Sample Size",
    "section": "Non-parametric Tests",
    "text": "Non-parametric Tests\n\n Non-parametric Difference between Two Groups (Independent samples) \n Mann-Whitney U or Wilcoxon Rank-sum Test \n (a.k.a Wilcoxon-Mann-Whitney Test) \n Method: GUENTHER \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1867 \n  n2 = 1867 \n ------------------------------ \n Alternative = \"equivalent\" \n Non-centrality parameter = -2.927 \n Degrees of freedom = 3561.91 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#linear-regression-f-and-t-tests-1",
    "href": "modules/sample-size.html#linear-regression-f-and-t-tests-1",
    "title": "Power and Sample Size",
    "section": "Linear Regression (F and t Tests)",
    "text": "Linear Regression (F and t Tests)\nOmnibus \\(F\\) Test\n\\(R^2 &gt; 0\\) in Linear Regression\nOmnibus F test in multiple liner regression is used to test whether \\(R^2\\) is greater than 0 (zero). Assume that we want to predict a continuous variable \\(Y\\) using \\(X_{1}\\), \\(X_{2}\\), and \\(X_{2}\\) variables (a combination of binary or continuous).\n\\[\\begin{eqnarray}\n  Y &=& \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3} + r, \\quad r \\thicksim N(0,\\sigma^2) \\newline\n  \\end{eqnarray}\\]",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#linear-regression-f-and-t-tests-2",
    "href": "modules/sample-size.html#linear-regression-f-and-t-tests-2",
    "title": "Power and Sample Size",
    "section": "Linear Regression (F and t Tests)",
    "text": "Linear Regression (F and t Tests)\nWe are expecting that these three variables explain 30% of the variance in the outcome (\\(R^2 = 0.30\\) or r2 = 0.30 in the code). What is the minimum required sample size?\n\npwrss.f.reg(r2 = 0.30, k = 3, power = 0.80, alpha = 0.05)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#linear-regression-f-and-t-tests-2-output",
    "href": "modules/sample-size.html#linear-regression-f-and-t-tests-2-output",
    "title": "Power and Sample Size",
    "section": "Linear Regression (F and t Tests)",
    "text": "Linear Regression (F and t Tests)\n\n Linear Regression (F test) \n R-squared Deviation from 0 (zero) \n H0: r2 = 0 \n HA: r2 &gt; 0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 30 \n ------------------------------ \n Numerator degrees of freedom = 3 \n Denominator degrees of freedom = 25.653 \n Non-centrality parameter = 12.709 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#single-regression-coefficient-z-or-t-test",
    "href": "modules/sample-size.html#single-regression-coefficient-z-or-t-test",
    "title": "Power and Sample Size",
    "section": "Single Regression Coefficient (z or t Test)",
    "text": "Single Regression Coefficient (z or t Test)\nStandardized versus Unstandardized Input\nIn the earlier example, assume that we want to predict a continuous variable \\(Y\\) using a continuous predictor \\(X_{1}\\) but control for \\(X_{2}\\), and \\(X_{2}\\) variables (a combination of binary or continuous). We are mainly interested in the effect of \\(X_{1}\\) and expect a standardized regression coefficient of \\(\\beta_{1} = 0.20\\).\n\\[\\begin{eqnarray}\n  Y &=& \\beta_{0} + \\color{red} {\\beta_{1} X_{1}} + \\beta_{2}X_{2} + \\beta_{3}X_{3} + r, \\quad r \\thicksim N(0,\\sigma^2) \\newline\n  \\end{eqnarray}\\]",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#single-regression-coefficient-z-or-t-test-1",
    "href": "modules/sample-size.html#single-regression-coefficient-z-or-t-test-1",
    "title": "Power and Sample Size",
    "section": "Single Regression Coefficient (z or t Test)",
    "text": "Single Regression Coefficient (z or t Test)\nAgain, we are expecting that these three variables explain 30% of the variance in the outcome (\\(R^2 = 0.30\\)). What is the minimum required sample size? It is sufficient to provide standardized regression coefficient for beta1 because sdx = 1 and sdy = 1 by default.\n\npwrss.t.reg(beta1 = 0.20, k = 3, r2 = 0.30, \n            power = .80, alpha = 0.05, alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#single-regression-coefficient-z-or-t-test-1-output",
    "href": "modules/sample-size.html#single-regression-coefficient-z-or-t-test-1-output",
    "title": "Power and Sample Size",
    "section": "Single Regression Coefficient (z or t Test)",
    "text": "Single Regression Coefficient (z or t Test)\n\n Linear Regression Coefficient (t Test) \n H0: beta1 = beta0 \n HA: beta1 != beta0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 140 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 135.331 \n Non-centrality parameter = 2.822 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#single-regression-coefficient-z-or-t-test-2",
    "href": "modules/sample-size.html#single-regression-coefficient-z-or-t-test-2",
    "title": "Power and Sample Size",
    "section": "Single Regression Coefficient (z or t Test)",
    "text": "Single Regression Coefficient (z or t Test)\nFor unstandardized coefficients specify sdy and sdx. Assume we are expecting an unstandardized regression coefficient of beta1 = 0.60, a standard deviation of sdy = 12 for the outcome and a standard deviation of sdx = 4 for the main predictor. What is the minimum required sample size?\n\npwrss.t.reg(beta1 = 0.60, sdy = 12, sdx = 4, k = 3, r2 = 0.30, \n            power = .80, alpha = 0.05, alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#single-regression-coefficient-z-or-t-test-2-output",
    "href": "modules/sample-size.html#single-regression-coefficient-z-or-t-test-2-output",
    "title": "Power and Sample Size",
    "section": "Single Regression Coefficient (z or t Test)",
    "text": "Single Regression Coefficient (z or t Test)\n\n Linear Regression Coefficient (t Test) \n H0: beta1 = beta0 \n HA: beta1 != beta0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 140 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 135.331 \n Non-centrality parameter = 2.822 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#single-regression-coefficient-z-or-t-test-3",
    "href": "modules/sample-size.html#single-regression-coefficient-z-or-t-test-3",
    "title": "Power and Sample Size",
    "section": "Single Regression Coefficient (z or t Test)",
    "text": "Single Regression Coefficient (z or t Test)\n\nWhen the main predictor is binary (e.g. treatment/control), its standardized regression coefficient equals Cohen’s d.\nThe predictor’s standard deviation is $\\sqrt{p(1-p)}$, where $p$ is the group proportion; for $p = 0.50$, what’s the minimum required sample size?\nProvide Cohen’s d for β₁ and set sdx = sqrt(p*(1-p)) when calculating.\n\n\np &lt;- 0.50\npwrss.t.reg(beta1 = 0.20, k = 3, r2 = 0.30, sdx = sqrt(p*(1-p)),\n            power = .80, alpha = 0.05, alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#single-regression-coefficient-z-or-t-test-3-output",
    "href": "modules/sample-size.html#single-regression-coefficient-z-or-t-test-3-output",
    "title": "Power and Sample Size",
    "section": "Single Regression Coefficient (z or t Test)",
    "text": "Single Regression Coefficient (z or t Test)\n\n Linear Regression Coefficient (t Test) \n H0: beta1 = beta0 \n HA: beta1 != beta0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 552 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 547.355 \n Non-centrality parameter = 2.807 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#non-inferiority-superiority-and-equivalence-1",
    "href": "modules/sample-size.html#non-inferiority-superiority-and-equivalence-1",
    "title": "Power and Sample Size",
    "section": "Non-inferiority, Superiority, and Equivalence",
    "text": "Non-inferiority, Superiority, and Equivalence\nThese tests are useful for testing practically significant effects (non-inferiority/superiority) or practically null effects (equivalence).",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#non-inferiority",
    "href": "modules/sample-size.html#non-inferiority",
    "title": "Power and Sample Size",
    "section": "Non-inferiority:",
    "text": "Non-inferiority:\nThe intervention is expected to be non-inferior to some earlier or other interventions. Assume that the effect of an earlier or some other intervention is beta0 = 0.10. The beta1 - beta0 is expected to be positive and should be at least -0.05 (margin = -0.05). What is the minimum required sample size?",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#non-inferiority-1",
    "href": "modules/sample-size.html#non-inferiority-1",
    "title": "Power and Sample Size",
    "section": "Non-inferiority:",
    "text": "Non-inferiority:\nThis is the case when higher values of an outcome is better. When lower values of an outcome is better the beta1 - beta0 difference is expected to be NEGATIVE and the margin takes POSITIVE values.\n\np &lt;- 0.50\npwrss.t.reg(beta1 = 0.20, beta0 = 0.10, margin = -0.05, \n            k = 3, r2 = 0.30, sdx = sqrt(p*(1-p)),\n            power = .80, alpha = 0.05, alternative = \"non-inferior\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#non-inferiority-1-output",
    "href": "modules/sample-size.html#non-inferiority-1-output",
    "title": "Power and Sample Size",
    "section": "Non-inferiority:",
    "text": "Non-inferiority:\n\n Linear Regression Coefficient (t Test) \n H0: beta1 - beta0 &lt;= margin \n HA: beta1 - beta0 &gt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 771 \n ------------------------------ \n Alternative = \"non-inferior\" \n Degrees of freedom = 766.745 \n Non-centrality parameter = 2.489 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#superiority",
    "href": "modules/sample-size.html#superiority",
    "title": "Power and Sample Size",
    "section": "Superiority",
    "text": "Superiority\nThe intervention is expected to be superior to some earlier or other interventions. Assume that the effect of an earlier or some other intervention is beta0 = 0.10. The beta1 - beta0 is expected to be positive and should be at least 0.05 (margin = 0.05). What is the minimum required sample size?",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#superiority-1",
    "href": "modules/sample-size.html#superiority-1",
    "title": "Power and Sample Size",
    "section": "Superiority",
    "text": "Superiority\nThis is the case when higher values of an outcome is better. When lower values of an outcome is better beta1 - beta0 difference is expected to be NEGATIVE and the margin takes NEGATIVE values.\n\np &lt;- 0.50\npwrss.t.reg(beta1 = 0.20, beta0 = 0.10, margin = 0.05, \n            k = 3, r2 = 0.30, sdx = sqrt(p*(1-p)),\n            power = .80, alpha = 0.05, alternative = \"superior\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#superiority-1-output",
    "href": "modules/sample-size.html#superiority-1-output",
    "title": "Power and Sample Size",
    "section": "Superiority",
    "text": "Superiority\n\n Linear Regression Coefficient (t Test) \n H0: beta1 - beta0 &lt;= margin \n HA: beta1 - beta0 &gt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 6926 \n ------------------------------ \n Alternative = \"superior\" \n Degrees of freedom = 6921.818 \n Non-centrality parameter = 2.487 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#equivalence",
    "href": "modules/sample-size.html#equivalence",
    "title": "Power and Sample Size",
    "section": "Equivalence",
    "text": "Equivalence\nThe intervention is expected to be equivalent to some earlier or other interventions. Assume the effect of an earlier or some other intervention is beta0 = 0.20. The beta1 - beta0 is expected to be within -0.05 and 0.05 (margin = 0.05). What is the minimum required sample size?",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#equivalence-1",
    "href": "modules/sample-size.html#equivalence-1",
    "title": "Power and Sample Size",
    "section": "Equivalence",
    "text": "Equivalence\nmargin always takes positive values for equivalence. Specify the absolute value.\n\np &lt;- 0.50\npwrss.t.reg(beta1 = 0.20, beta0 = 0.20, margin = 0.05, \n            k = 3, r2 = 0.30, sdx = sqrt(p*(1-p)),\n            power = .80, alpha = 0.05, alternative = \"equivalent\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#equivalence-1-output",
    "href": "modules/sample-size.html#equivalence-1-output",
    "title": "Power and Sample Size",
    "section": "Equivalence",
    "text": "Equivalence\n\n Linear Regression Coefficient (t Test) \n H0: |beta1 - beta0| &gt;= margin \n HA: |beta1 - beta0| &lt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 9593 \n ------------------------------ \n Alternative = \"equivalent\" \n Degrees of freedom = 9588.862 \n Non-centrality parameter = -2.927 2.927 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-1",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-1",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\nIn logistic regression a binary outcome variable (0/1: failed/passed, dead/alive, absent/present) is modeled by predicting probability of being in group 1 (\\(P_1\\)) via logit transformation (natural logarithm of odds). The base probability \\(P_0\\) is the overall probability of being in group 1 without influence of predictors in the model (null). Under alternative hypothesis, the probability of being in group 1 (\\(P_1\\)) deviate from \\(P_0\\) depending on the value of the predictor; whereas under null it is same as the \\(P_0\\).",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-2",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-2",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\nA model with one main predictor (\\(X_1\\)) and two other covariates (\\(X_2\\) and \\(X_3\\)) can be constructed as\n\\[\\begin{eqnarray}\n  ln(\\frac{P_1}{1- P_1}) &=& \\beta_{0} + \\color{red} {\\beta_{1} X_{1}} + \\beta_{2}X_{2} + \\beta_{3}X_{3} \\newline\n  \\end{eqnarray}\\]",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-3",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-3",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\nTherefore the odds ratio is defined as \\[OR = exp(\\beta_1) = \\frac{P_1}{1- P_1} / \\frac{P_0}{1- P_0}\\]",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-4",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-4",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\nExample:\n\nA squared multiple correlation of 0.20 between \\(X_1\\) and other covariates (r2.other.x = 0.20 in the code). It can be found in the form of adjusted R-square via regressing \\(X_1\\) on \\(X_2\\) and \\(X_3\\). Higher values require larger sample sizes. The default is 0 (zero).\nA base probability of \\(P_0 = 0.15\\). This is the rate when predictor \\(X_1 = 0\\) or when \\(\\beta_1 = 0\\).\nIncreasing \\(X_1\\) from 0 to 1 reduces the probability of being in group 1 from 0.15 to 0.10 (\\(P_1 = 0.10\\)).",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-5",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-5",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\nWhat is the minimum required sample size? There are three types of specification to statistical power or sample size calculations; (i) probability specification, (ii) odds ratio specification, and (iii) regression coefficient specification (as in standard software output).",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-6",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-6",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\nProbability specification:\n\npwrss.z.logreg(p0 = 0.15, p1 = 0.10, r2.other.x = 0.20,\n               power = 0.80, alpha = 0.05, \n               dist = \"normal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-6-output",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-6-output",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\n\n Logistic Regression Coefficient \n (Large Sample Approx. Wald's z Test) \n H0: beta1 = 0 \n HA: beta1 != 0 \n Distribution of X = 'normal' \n Method = DEMIDENKO(VC) \n ------------------------------ \n  Statistical power = 0.8 \n  n = 365 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.766 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-7",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-7",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\nOdds ratio specification: \\[OR = \\frac{P_1}{1-P1} / \\frac{P_0}{1-P_0} = \\frac{0.10}{1-0.10} / \\frac{0.15}{1-0.15} = 0.6296\\]\n\npwrss.z.logreg(p0 = 0.15, odds.ratio = 0.6296, r2.other.x = 0.20,\n               alpha = 0.05, power = 0.80,\n               dist = \"normal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-7-output",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-7-output",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\n\n Logistic Regression Coefficient \n (Large Sample Approx. Wald's z Test) \n H0: beta1 = 0 \n HA: beta1 != 0 \n Distribution of X = 'normal' \n Method = DEMIDENKO(VC) \n ------------------------------ \n  Statistical power = 0.8 \n  n = 365 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.766 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-8",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-8",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\nRegression coefficient specification: \\[\\beta_1 = ln(\\frac{P_1}{1-P1} / \\frac{P_0}{1-P_0}) = ln(0.6296) = -0.4626\\]\n\npwrss.z.logreg(p0 = 0.15, beta1 = -0.4626, r2.other.x = 0.20,\n               alpha = 0.05, power = 0.80,\n               dist = \"normal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-8-output",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-8-output",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\n\n Logistic Regression Coefficient \n (Large Sample Approx. Wald's z Test) \n H0: beta1 = 0 \n HA: beta1 != 0 \n Distribution of X = 'normal' \n Method = DEMIDENKO(VC) \n ------------------------------ \n  Statistical power = 0.8 \n  n = 365 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.766 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-9",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-9",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\nChange the distribution’s parameters for predictor X:\nThe mean and standard deviation of a normally distributed main predictor is 0 and 1 by default. They can be modified. In the following example the mean is 25 and the standard deviation is 8.\n\ndist.x &lt;- list(dist = \"normal\", mean = 25, sd = 8)\n\npwrss.z.logreg(p0 = 0.15, beta1 = -0.4626, r2.other.x = 0.20,\n               alpha = 0.05, power = 0.80,\n               dist = dist.x)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-9-output",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-9-output",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\n\n Logistic Regression Coefficient \n (Large Sample Approx. Wald's z Test) \n H0: beta1 = 0 \n HA: beta1 != 0 \n Distribution of X = 'normal' \n Method = DEMIDENKO(VC) \n ------------------------------ \n  Statistical power = 0.8 \n  n = 2435 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.423 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-10",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-10",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\nChange the distribution family of predictor X:\nMore distribution types are supported by the function. For example, the main predictor can be binary (e.g. treatment/control groups). Often half of the sample is assigned to the treatment group and the other half to the control (prob = 0.50 by default).\n\npwrss.z.logreg(p0 = 0.15, beta1 = -0.4626, r2.other.x = 0.20,\n               alpha = 0.05, power = 0.80,\n               dist = \"bernoulli\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-10-output",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-10-output",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\n\n Logistic Regression Coefficient \n (Large Sample Approx. Wald's z Test) \n H0: beta1 = 0 \n HA: beta1 != 0 \n Distribution of X = 'bernoulli' \n Method = DEMIDENKO(VC) \n ------------------------------ \n  Statistical power = 0.8 \n  n = 1723 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.789 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-11",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-11",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\nChange the treatment group allocation rate of the binary predictor X (prob = 0.40):\nSometimes treatment groups cost more per subject or are harder to recruit than control groups, making an unbalanced sample practical. For example, with 40% of subjects in the treatment group, what is the minimum required sample size?\n\ndist.x &lt;- list(dist = \"bernoulli\", prob = 0.40)\n\npwrss.z.logreg(p0 = 0.15, beta1 = -0.4626, r2.other.x = 0.20,\n               alpha = 0.05, power = 0.80,\n               dist = dist.x)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#logistic-regression-walds-z-test-11-output",
    "href": "modules/sample-size.html#logistic-regression-walds-z-test-11-output",
    "title": "Power and Sample Size",
    "section": "Logistic Regression (Wald’s z Test)",
    "text": "Logistic Regression (Wald’s z Test)\n\n Logistic Regression Coefficient \n (Large Sample Approx. Wald's z Test) \n H0: beta1 = 0 \n HA: beta1 != 0 \n Distribution of X = 'bernoulli' \n Method = DEMIDENKO(VC) \n ------------------------------ \n  Statistical power = 0.8 \n  n = 1826 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.766 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#anova-analysis-of-variance-ancova-analysis-of-covariance",
    "href": "modules/sample-size.html#anova-analysis-of-variance-ancova-analysis-of-covariance",
    "title": "Power and Sample Size",
    "section": "ANOVA: Analysis of Variance ANCOVA: Analysis of Covariance",
    "text": "ANOVA: Analysis of Variance ANCOVA: Analysis of Covariance\n\nOne-way ANOVA/ANCOVA: Compares means across groups; ANCOVA adjusts for covariates.\nTwo- or Three-way ANOVA/ANCOVA: Examines interactions between factors; ANCOVA adjusts interactions for covariates and determines required sample size for complex designs.",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#one-way",
    "href": "modules/sample-size.html#one-way",
    "title": "Power and Sample Size",
    "section": "One-way",
    "text": "One-way\nANOVA\nA researcher is expecting a difference of Cohen’s d = 0.50 between treatment and control groups (two levels) translating into \\(\\eta^2 = 0.059\\) (eta2 = 0.059). Means are not adjusted for any covariates. What is the minimum required sample size?\n\npwrss.f.ancova(eta2 = 0.059, n.levels = 2,\n               power = .80, alpha = 0.05)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#one-way-output",
    "href": "modules/sample-size.html#one-way-output",
    "title": "Power and Sample Size",
    "section": "One-way",
    "text": "One-way\n\n One-way Analysis of Variance (ANOVA) \n  H0: 'eta2' or 'f2' = 0 \n  HA: 'eta2' or 'f2' &gt; 0 \n --------------------------------------\n Factor A: 2 levels \n --------------------------------------\n effect power n.total   ncp df1     df2\n      A   0.8     128 7.971   1 125.132\n --------------------------------------\n Type I error rate: 0.05",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#one-way-1",
    "href": "modules/sample-size.html#one-way-1",
    "title": "Power and Sample Size",
    "section": "One-way",
    "text": "One-way\nANCOVA\nA researcher is expecting an adjusted difference of Cohen’s d = 0.45 between treatment and control groups (n.levels = 2) after controlling for the pretest (n.cov = 1) translating into partial \\(\\eta^2 = 0.048\\) (eta2 = 0.048). What is the minimum required sample size?\n\npwrss.f.ancova(eta2 = 0.048, n.levels = 2, n.cov = 1,\n               alpha = 0.05, power = .80)\n\n\nn.cov (or n.covariates) argument has trivial effect on the results. The difference between ANOVA and ANCOVA procedure depends on whether the effect (eta2) is unadjusted or covariate-adjusted.",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#one-way-1-output",
    "href": "modules/sample-size.html#one-way-1-output",
    "title": "Power and Sample Size",
    "section": "One-way",
    "text": "One-way\n\n One-way Analysis of Covariance (ANCOVA) \n  H0: 'eta2' or 'f2' = 0 \n  HA: 'eta2' or 'f2' &gt; 0 \n --------------------------------------\n Factor A: 2 levels \n --------------------------------------\n effect power n.total   ncp df1     df2\n      A   0.8     158 7.948   1 154.626\n --------------------------------------\n Type I error rate: 0.05",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#two-way",
    "href": "modules/sample-size.html#two-way",
    "title": "Power and Sample Size",
    "section": "Two-way",
    "text": "Two-way\nANOVA\nA researcher is expecting a partial \\(\\eta^2 = 0.03\\) (eta2 = 0.03) for interaction of treatment/control (Factor A: two levels) with gender (Factor B: two levels). Thus, n.levels = c(2,2). What is the minimum required sample size?\n\npwrss.f.ancova(eta2 = 0.03, n.levels = c(2,2),\n               alpha = 0.05, power = 0.80)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#two-way-output",
    "href": "modules/sample-size.html#two-way-output",
    "title": "Power and Sample Size",
    "section": "Two-way",
    "text": "Two-way\n\n Two-way Analysis of Variance (ANOVA) \n  H0: 'eta2' or 'f2' = 0 \n  HA: 'eta2' or 'f2' &gt; 0 \n --------------------------------------\n Factor A: 2 levels \n Factor B: 2 levels \n --------------------------------------\n effect power n.total   ncp df1    df2\n      A   0.8     256 7.909   1 251.73\n      B   0.8     256 7.909   1 251.73\n  A x B   0.8     256 7.909   1 251.73\n --------------------------------------\n Type I error rate: 0.05",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#two-way-1",
    "href": "modules/sample-size.html#two-way-1",
    "title": "Power and Sample Size",
    "section": "Two-way",
    "text": "Two-way\nANCOVA\nA researcher is expecting a partial \\(\\eta^2 = 0.02\\) (eta2 = 0.02) for interaction of treatment/control (Factor A) with gender (Factor B) adjusted for the pretest (n.cov = 1). What is the minimum required sample size?\n\npwrss.f.ancova(eta2 = 0.02, n.levels = c(2,2), n.cov = 1,\n               alpha = 0.05, power = .80)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#two-way-1-output",
    "href": "modules/sample-size.html#two-way-1-output",
    "title": "Power and Sample Size",
    "section": "Two-way",
    "text": "Two-way\n\n Two-way Analysis of Covariance (ANCOVA) \n  H0: 'eta2' or 'f2' = 0 \n  HA: 'eta2' or 'f2' &gt; 0 \n --------------------------------------\n Factor A: 2 levels \n Factor B: 2 levels \n --------------------------------------\n effect power n.total   ncp df1     df2\n      A   0.8     387 7.889   1 381.539\n      B   0.8     387 7.889   1 381.539\n  A x B   0.8     387 7.889   1 381.539\n --------------------------------------\n Type I error rate: 0.05",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#one-correlation",
    "href": "modules/sample-size.html#one-correlation",
    "title": "Power and Sample Size",
    "section": "One Correlation",
    "text": "One Correlation\nOne-sided Test: Assume that the expected correlation is 0.20 and it is greater than 0.10. What is the minimum required sample size?\n\npwrss.z.corr(r = 0.20, r0 = 0.10,\n             power = 0.80, alpha = 0.05, \n             alternative = \"greater\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#one-correlation-output",
    "href": "modules/sample-size.html#one-correlation-output",
    "title": "Power and Sample Size",
    "section": "One Correlation",
    "text": "One Correlation\n\n A Correlation against a Constant (z Test) \n H0: r = r0 \n HA: r &gt; r0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 593 \n ------------------------------ \n Alternative = \"greater\" \n Non-centrality parameter = 2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#one-correlation-1",
    "href": "modules/sample-size.html#one-correlation-1",
    "title": "Power and Sample Size",
    "section": "One Correlation",
    "text": "One Correlation\nTwo-sided Test: Assume that the expected correlation is 0.20 and it is different from 0 (zero). The correlation could be 0.20 as well as -0.20. What is the minimum required sample size?\n\npwrss.z.corr(r = 0.20, r0 = 0,\n             power = 0.80, alpha = 0.05, \n             alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#one-correlation-1-output",
    "href": "modules/sample-size.html#one-correlation-1-output",
    "title": "Power and Sample Size",
    "section": "One Correlation",
    "text": "One Correlation\n\n A Correlation against a Constant (z Test) \n H0: r = r0 \n HA: r != r0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 194 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = 2.802 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-correlations-independent",
    "href": "modules/sample-size.html#difference-between-two-correlations-independent",
    "title": "Power and Sample Size",
    "section": "Difference between Two Correlations (Independent)",
    "text": "Difference between Two Correlations (Independent)\nAssume that the expected correlations in the first and second groups are 0.30 and 0.20, respectively (r1 = 0.30 and r2 = 0.20).",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-correlations-independent-1",
    "href": "modules/sample-size.html#difference-between-two-correlations-independent-1",
    "title": "Power and Sample Size",
    "section": "Difference between Two Correlations (Independent)",
    "text": "Difference between Two Correlations (Independent)\nOne-sided Test: Expecting r1 - r2 greater than 0 (zero). The difference could be 0.10 but could not be -0.10. What is the minimum required sample size?\n\npwrss.z.2corrs(r1 = 0.30, r2 = 0.20,\n               power = .80, alpha = 0.05, \n               alternative = \"greater\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-correlations-independent-1-output",
    "href": "modules/sample-size.html#difference-between-two-correlations-independent-1-output",
    "title": "Power and Sample Size",
    "section": "Difference between Two Correlations (Independent)",
    "text": "Difference between Two Correlations (Independent)\n\n Difference between Two Correlations \n (Independent Samples z Test) \n H0: r1 = r2 \n HA: r1 &gt; r2 \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1088 \n  n2 = 1088 \n ------------------------------ \n Alternative = \"greater\" \n Non-centrality parameter = 2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-correlations-independent-2",
    "href": "modules/sample-size.html#difference-between-two-correlations-independent-2",
    "title": "Power and Sample Size",
    "section": "Difference between Two Correlations (Independent)",
    "text": "Difference between Two Correlations (Independent)\nTwo-sided Test: Expecting r1 - r2 different from 0 (zero). The difference could be -0.10 as well as 0.10. What is the minimum required sample size?\n\npwrss.z.2corrs(r1 = 0.30, r2 = 0.20,\n               power = .80, alpha = 0.05, \n               alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-correlations-independent-2-output",
    "href": "modules/sample-size.html#difference-between-two-correlations-independent-2-output",
    "title": "Power and Sample Size",
    "section": "Difference between Two Correlations (Independent)",
    "text": "Difference between Two Correlations (Independent)\n\n Difference between Two Correlations \n (Independent Samples z Test) \n H0: r1 = r2 \n HA: r1 != r2 \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1380 \n  n2 = 1380 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = 2.802 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#proportions-z-test-1",
    "href": "modules/sample-size.html#proportions-z-test-1",
    "title": "Power and Sample Size",
    "section": "Proportion(s) (z Test)",
    "text": "Proportion(s) (z Test)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#one-proportion",
    "href": "modules/sample-size.html#one-proportion",
    "title": "Power and Sample Size",
    "section": "One Proportion",
    "text": "One Proportion\nIn the following examples p is the proportion under alternative hypothesis and p0 is the proportion under null hypothesis.",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#proportions-z-test-2",
    "href": "modules/sample-size.html#proportions-z-test-2",
    "title": "Power and Sample Size",
    "section": "Proportion(s) (z Test)",
    "text": "Proportion(s) (z Test)\nOne-sided Test: Expecting p - p0 smaller than 0 (zero).\n\n# normal approximation\npwrss.z.prop(p = 0.45, p0 = 0.50,\n             alpha = 0.05, power = 0.80,\n             alternative = \"less\",\n             arcsin.trans = FALSE)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#proportions-z-test-2-output",
    "href": "modules/sample-size.html#proportions-z-test-2-output",
    "title": "Power and Sample Size",
    "section": "Proportion(s) (z Test)",
    "text": "Proportion(s) (z Test)\n\n Approach: Normal Approximation \n A Proportion against a Constant (z Test) \n H0: p = p0 \n HA: p &lt; p0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 613 \n ------------------------------ \n Alternative = \"less\" \n Non-centrality parameter = -2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n\n# arcsine transformation\npwrss.z.prop(p = 0.45, p0 = 0.50,\n             alpha = 0.05, power = 0.80,\n             alternative = \"less\",\n             arcsin.trans = TRUE)\n\n Approach: Arcsine Transformation \n A Proportion against a Constant (z Test) \n H0: p = p0 \n HA: p &lt; p0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 617 \n ------------------------------ \n Alternative = \"less\" \n Non-centrality parameter = -2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#proportions-z-test-3",
    "href": "modules/sample-size.html#proportions-z-test-3",
    "title": "Power and Sample Size",
    "section": "Proportion(s) (z Test)",
    "text": "Proportion(s) (z Test)\nTwo-sided Test: Expecting p - p0 smaller than 0 (zero) or greater than 0 (zero).\n\npwrss.z.prop(p = 0.45, p0 = 0.50,\n             alpha = 0.05, power = 0.80,\n             alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#proportions-z-test-3-output",
    "href": "modules/sample-size.html#proportions-z-test-3-output",
    "title": "Power and Sample Size",
    "section": "Proportion(s) (z Test)",
    "text": "Proportion(s) (z Test)\n\n Approach: Normal Approximation \n A Proportion against a Constant (z Test) \n H0: p = p0 \n HA: p != p0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 778 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.802 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#proportions-z-test-4",
    "href": "modules/sample-size.html#proportions-z-test-4",
    "title": "Power and Sample Size",
    "section": "Proportion(s) (z Test)",
    "text": "Proportion(s) (z Test)\nNon-inferiority Test: The case when smaller proportion is better. Expecting p - p0 smaller than 0.01.\n\npwrss.z.prop(p = 0.45, p0 = 0.50, margin = 0.01,\n             alpha = 0.05, power = 0.80,\n             alternative = \"non-inferior\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#proportions-z-test-4-output",
    "href": "modules/sample-size.html#proportions-z-test-4-output",
    "title": "Power and Sample Size",
    "section": "Proportion(s) (z Test)",
    "text": "Proportion(s) (z Test)\n\n Approach: Normal Approximation \n A Proportion against a Constant (z Test) \n H0: p - p0 &lt;= margin \n HA: p - p0 &gt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 426 \n ------------------------------ \n Alternative = \"non-inferior\" \n Non-centrality parameter = -2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#proportions-z-test-5",
    "href": "modules/sample-size.html#proportions-z-test-5",
    "title": "Power and Sample Size",
    "section": "Proportion(s) (z Test)",
    "text": "Proportion(s) (z Test)\nNon-inferiority Test: The case when bigger proportion is better. Expecting p - p0 greater than -0.01.\n\npwrss.z.prop(p = 0.55, p0 = 0.50, margin = -0.01,\n             alpha = 0.05, power = 0.80,\n             alternative = \"non-inferior\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#proportions-z-test-5-output",
    "href": "modules/sample-size.html#proportions-z-test-5-output",
    "title": "Power and Sample Size",
    "section": "Proportion(s) (z Test)",
    "text": "Proportion(s) (z Test)\n\n Approach: Normal Approximation \n A Proportion against a Constant (z Test) \n H0: p - p0 &lt;= margin \n HA: p - p0 &gt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 426 \n ------------------------------ \n Alternative = \"non-inferior\" \n Non-centrality parameter = 2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#proportions-z-test-6",
    "href": "modules/sample-size.html#proportions-z-test-6",
    "title": "Power and Sample Size",
    "section": "Proportion(s) (z Test)",
    "text": "Proportion(s) (z Test)\nSuperiority Test: The case when smaller proportion is better. Expecting p - p0 smaller than -0.01.\n\npwrss.z.prop(p = 0.45, p0 = 0.50, margin = -0.01,\n             alpha = 0.05, power = 0.80,\n             alternative = \"superior\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#proportions-z-test-6-output",
    "href": "modules/sample-size.html#proportions-z-test-6-output",
    "title": "Power and Sample Size",
    "section": "Proportion(s) (z Test)",
    "text": "Proportion(s) (z Test)\n\n Approach: Normal Approximation \n A Proportion against a Constant (z Test) \n H0: p - p0 &lt;= margin \n HA: p - p0 &gt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 957 \n ------------------------------ \n Alternative = \"superior\" \n Non-centrality parameter = -2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#proportions-z-test-7",
    "href": "modules/sample-size.html#proportions-z-test-7",
    "title": "Power and Sample Size",
    "section": "Proportion(s) (z Test)",
    "text": "Proportion(s) (z Test)\nSuperiority Test: The case when bigger proportion is better. Expecting p - p0 greater than 0.01.\n\npwrss.z.prop(p = 0.55, p0 = 0.50, margin = 0.01,\n             alpha = 0.05, power = 0.80,\n             alternative = \"superior\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#proportions-z-test-7-output",
    "href": "modules/sample-size.html#proportions-z-test-7-output",
    "title": "Power and Sample Size",
    "section": "Proportion(s) (z Test)",
    "text": "Proportion(s) (z Test)\n\n Approach: Normal Approximation \n A Proportion against a Constant (z Test) \n H0: p - p0 &lt;= margin \n HA: p - p0 &gt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 957 \n ------------------------------ \n Alternative = \"superior\" \n Non-centrality parameter = 2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#proportions-z-test-8",
    "href": "modules/sample-size.html#proportions-z-test-8",
    "title": "Power and Sample Size",
    "section": "Proportion(s) (z Test)",
    "text": "Proportion(s) (z Test)\nEquivalence Test: Expecting p - p0 between -0.01 and 0.01.\n\npwrss.z.prop(p = 0.50, p0 = 0.50, margin = 0.01,\n             alpha = 0.05, power = 0.80,\n             alternative = \"equivalent\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#proportions-z-test-8-output",
    "href": "modules/sample-size.html#proportions-z-test-8-output",
    "title": "Power and Sample Size",
    "section": "Proportion(s) (z Test)",
    "text": "Proportion(s) (z Test)\n\n Approach: Normal Approximation \n A Proportion against a Constant (z Test) \n H0: |p - p0| &gt;= margin \n HA: |p - p0| &lt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n = 21410 \n ------------------------------ \n Alternative = \"equivalent\" \n Non-centrality parameter = -2.926 2.926 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-proportions-independent-1",
    "href": "modules/sample-size.html#difference-between-two-proportions-independent-1",
    "title": "Power and Sample Size",
    "section": "Difference between Two Proportions (Independent)",
    "text": "Difference between Two Proportions (Independent)\nIn the following examples p1 and p2 are proportions for the first and second groups under alternative hypothesis. The null hypothesis state p1 = p2 or p1 - p2 = 0.",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-proportions",
    "href": "modules/sample-size.html#difference-between-two-proportions",
    "title": "Power and Sample Size",
    "section": "Difference between Two Proportions",
    "text": "Difference between Two Proportions\nOne-sided Test: Expecting p1 - p2 smaller than 0 (zero).\n\n# normal approximation\npwrss.z.2props(p1 = 0.45, p2 = 0.50,\n               alpha = 0.05, power = 0.80,\n               alternative = \"less\",\n               arcsin.trans = FALSE)",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-proportions-output",
    "href": "modules/sample-size.html#difference-between-two-proportions-output",
    "title": "Power and Sample Size",
    "section": "Difference between Two Proportions",
    "text": "Difference between Two Proportions\n\n Approach: Normal Approximation \n Difference between Two Proportions \n (Independent Samples z Test) \n H0: p1 = p2 \n HA: p1 &lt; p2 \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1231 \n  n2 = 1231 \n ------------------------------ \n Alternative = \"less\" \n Non-centrality parameter = -2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-proportions-1",
    "href": "modules/sample-size.html#difference-between-two-proportions-1",
    "title": "Power and Sample Size",
    "section": "Difference between Two Proportions",
    "text": "Difference between Two Proportions\nTwo-sided Test: Expecting p1 - p2 smaller than 0 (zero) or greater than 0 (zero).\n\npwrss.z.2props(p1 = 0.45, p2 = 0.50,\n               alpha = 0.05, power = 0.80,\n               alternative = \"not equal\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-proportions-1-output",
    "href": "modules/sample-size.html#difference-between-two-proportions-1-output",
    "title": "Power and Sample Size",
    "section": "Difference between Two Proportions",
    "text": "Difference between Two Proportions\n\n Approach: Normal Approximation \n Difference between Two Proportions \n (Independent Samples z Test) \n H0: p1 = p2 \n HA: p1 != p2 \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1562 \n  n2 = 1562 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.802 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-proportions-2",
    "href": "modules/sample-size.html#difference-between-two-proportions-2",
    "title": "Power and Sample Size",
    "section": "Difference between Two Proportions",
    "text": "Difference between Two Proportions\nNon-inferiority Test: The case when smaller proportion is better. Expecting p1 - p2 smaller than 0.01.\n\npwrss.z.2props(p1 = 0.45, p2 = 0.50, margin = 0.01,\n               alpha = 0.05, power = 0.80,\n               alternative = \"non-inferior\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-proportions-2-output",
    "href": "modules/sample-size.html#difference-between-two-proportions-2-output",
    "title": "Power and Sample Size",
    "section": "Difference between Two Proportions",
    "text": "Difference between Two Proportions\n\n Approach: Normal Approximation \n Difference between Two Proportions \n (Independent Samples z Test) \n H0: p1 - p2 &lt;= margin \n HA: p1 - p2 &gt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 855 \n  n2 = 855 \n ------------------------------ \n Alternative = \"non-inferior\" \n Non-centrality parameter = -2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-proportions-3",
    "href": "modules/sample-size.html#difference-between-two-proportions-3",
    "title": "Power and Sample Size",
    "section": "Difference between Two Proportions",
    "text": "Difference between Two Proportions\nNon-inferiority Test: The case when bigger proportion is better. Expecting p1 - p2 greater than -0.01.\n\npwrss.z.2props(p1 = 0.55, p2 = 0.50,  margin = -0.01,\n               alpha = 0.05, power = 0.80,\n               alternative = \"non-inferior\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-proportions-3-output",
    "href": "modules/sample-size.html#difference-between-two-proportions-3-output",
    "title": "Power and Sample Size",
    "section": "Difference between Two Proportions",
    "text": "Difference between Two Proportions\n\n Approach: Normal Approximation \n Difference between Two Proportions \n (Independent Samples z Test) \n H0: p1 - p2 &lt;= margin \n HA: p1 - p2 &gt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 855 \n  n2 = 855 \n ------------------------------ \n Alternative = \"non-inferior\" \n Non-centrality parameter = 2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-proportions-4",
    "href": "modules/sample-size.html#difference-between-two-proportions-4",
    "title": "Power and Sample Size",
    "section": "Difference between Two Proportions",
    "text": "Difference between Two Proportions\nSuperiority Test: The case when smaller proportion is better. Expecting p1 - p2 smaller than -0.01.\n\npwrss.z.2props(p1 = 0.45, p2 = 0.50, margin = -0.01,\n               alpha = 0.05, power = 0.80,\n               alternative = \"superior\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-proportions-4-output",
    "href": "modules/sample-size.html#difference-between-two-proportions-4-output",
    "title": "Power and Sample Size",
    "section": "Difference between Two Proportions",
    "text": "Difference between Two Proportions\n\n Approach: Normal Approximation \n Difference between Two Proportions \n (Independent Samples z Test) \n H0: p1 - p2 &lt;= margin \n HA: p1 - p2 &gt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1923 \n  n2 = 1923 \n ------------------------------ \n Alternative = \"superior\" \n Non-centrality parameter = -2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-proportions-5",
    "href": "modules/sample-size.html#difference-between-two-proportions-5",
    "title": "Power and Sample Size",
    "section": "Difference between Two Proportions",
    "text": "Difference between Two Proportions\nSuperiority Test: The case when bigger proportion is better. Expecting p1 - p2 greater than 0.01.\n\npwrss.z.2props(p1 = 0.55, p2 = 0.50, margin = 0.01,\n               alpha = 0.05, power = 0.80,\n               alternative = \"superior\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-proportions-5-output",
    "href": "modules/sample-size.html#difference-between-two-proportions-5-output",
    "title": "Power and Sample Size",
    "section": "Difference between Two Proportions",
    "text": "Difference between Two Proportions\n\n Approach: Normal Approximation \n Difference between Two Proportions \n (Independent Samples z Test) \n H0: p1 - p2 &lt;= margin \n HA: p1 - p2 &gt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 1923 \n  n2 = 1923 \n ------------------------------ \n Alternative = \"superior\" \n Non-centrality parameter = 2.486 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-proportions-6",
    "href": "modules/sample-size.html#difference-between-two-proportions-6",
    "title": "Power and Sample Size",
    "section": "Difference between Two Proportions",
    "text": "Difference between Two Proportions\nEquivalence Test: Expecting p1 - p2 between -0.01 and 0.01.\n\npwrss.z.2props(p1 = 0.50, p2 = 0.50, margin = 0.01,\n               alpha = 0.05, power = 0.80,\n               alternative = \"equivalent\")",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/sample-size.html#difference-between-two-proportions-6-output",
    "href": "modules/sample-size.html#difference-between-two-proportions-6-output",
    "title": "Power and Sample Size",
    "section": "Difference between Two Proportions",
    "text": "Difference between Two Proportions\n\n Approach: Normal Approximation \n Difference between Two Proportions \n (Independent Samples z Test) \n H0: |p1 - p2| &gt;= margin \n HA: |p1 - p2| &lt; margin \n ------------------------------ \n  Statistical power = 0.8 \n  n1 = 42820 \n  n2 = 42820 \n ------------------------------ \n Alternative = \"equivalent\" \n Non-centrality parameter = -2.926 2.926 \n Type I error rate = 0.05 \n Type II error rate = 0.2",
    "crumbs": [
      "Modules",
      "Sample size"
    ]
  },
  {
    "objectID": "modules/power-sim.html#learning-goals",
    "href": "modules/power-sim.html#learning-goals",
    "title": "Module XX: Power 2 (Unlimited Power)",
    "section": "Learning goals",
    "text": "Learning goals\n\nCompare analytic power methods with power simulations.\nBe able to draw simulated power curves for complicated models",
    "crumbs": [
      "Modules",
      "Power simulation"
    ]
  },
  {
    "objectID": "modules/power-sim.html#calculating-power",
    "href": "modules/power-sim.html#calculating-power",
    "title": "Module XX: Power 2 (Unlimited Power)",
    "section": "Calculating power",
    "text": "Calculating power\n\nFor this example, we’ll use birthweight data from the MASS package, called birthwt. You can load this data using the data() function.\n\n\n# Saves a global object called birthwt\ndata(birthwt, package = \"MASS\")\n\n\nLet’s do a two sample \\(t\\)-test for birth weight (bwt) for mothers who smoke and don’t smoke (smoke).\n\n\nsmoke_test &lt;- t.test(bwt ~ smoke, data = birthwt)\nsmoke_test\n\n\n    Welch Two Sample t-test\n\ndata:  bwt by smoke\nt = 2.7299, df = 170.1, p-value = 0.007003\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n  78.57486 488.97860\nsample estimates:\nmean in group 0 mean in group 1 \n       3055.696        2771.919 \n\n\n\n\nWe can also calculate the power of this test. It takes a little bit of finagling, but it’s not too hard.\nYou could also use G Power to do this, and it would probably be easier.\nSide note: aggregate is a great way to use formulas for grouped calculations.\n\n\ngroup_sample_sizes &lt;- aggregate(bwt ~ smoke, data = birthwt, FUN = length)\ngroup_means &lt;- aggregate(bwt ~ smoke, data = birthwt, FUN = mean)\ngroup_sds &lt;- aggregate(bwt ~ smoke, data = birthwt, FUN = sd)\npower.t.test(\n    # Use the average group sample size\n    n = mean(group_sample_sizes$bwt),\n    # d = TRUE difference in means / so we use our post-hoc observation\n    delta = diff(group_means$bwt),\n    # Pool the SD's\n    sd = sqrt(mean(group_sds$bwt ^ 2)),\n    # Alpha level of test\n    sig.level = 0.05,\n    # Details about the test\n    type = \"two.sample\",\n    alternative = \"two.sided\"\n)\n\n\n     Two-sample t test power calculation \n\n              n = 94.5\n          delta = 283.7767\n             sd = 707.6758\n      sig.level = 0.05\n          power = 0.7829694\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\nWe can an estimated power of about 78%. Of course this is post-hoc power so it’s basically useless.\n\n\n\n\nThe G Power calculation is actually a bit more flexible.\n\n\n\n\nFrom G Power where I typed the numbers in.\n\n\n\nThe estimate is a bit different because of how G Power deals with unequal sample sizes between groups.",
    "crumbs": [
      "Modules",
      "Power simulation"
    ]
  },
  {
    "objectID": "modules/power-sim.html#the-real-power-of-power",
    "href": "modules/power-sim.html#the-real-power-of-power",
    "title": "Module XX: Power 2 (Unlimited Power)",
    "section": "The real power of power",
    "text": "The real power of power\n\nPower is much more useful when it isn’t post-hoc. That is, we should use power to plan our studies, rather than calculate the power after we do a test.\nThe most common application of power is sample size estimation.\n\n\n\nIf we want to do a t-test between two groups, we can calculate the sample size we need to achieve a specific power and alpha level for a test, assuming some underlying effect size and variability.\nFor a study that requires an unpaired t-test, let’s calculate the required sample size to achieve typical targets of \\(\\alpha = 0.05\\) and \\(1 - \\beta = 0.8\\). For this example we’ll assume that our effect is standardized, so a clinically meaningful effect size is \\(0.1\\). We’ll assume for this example that the SD is \\(0.25\\).\n\n\n\n\nHint: you need to specify different arguments in power.t.test() this time.\n\n\npower.t.test(,\n    delta = ...,\n    sd = ...,\n    sig.level = ...,\n    power = ...,\n    # Details about the test\n    type = \"two.sample\",\n    alternative = \"two.sided\"\n)\n\n\n\n\nSolution: we need about 100 people in each group, so 200 total.\n\n\npower.t.test(,\n    delta = 0.1,\n    sd = 0.25,\n    sig.level = 0.05,\n    power = 0.80,\n    # Details about the test\n    type = \"two.sample\",\n    alternative = \"two.sided\"\n)\n\n\n     Two-sample t test power calculation \n\n              n = 99.08057\n          delta = 0.1\n             sd = 0.25\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\n\nWhat if we don’t know the effect size or standard deviation?",
    "crumbs": [
      "Modules",
      "Power simulation"
    ]
  },
  {
    "objectID": "modules/power-sim.html#power-curves",
    "href": "modules/power-sim.html#power-curves",
    "title": "Module XX: Power 2 (Unlimited Power)",
    "section": "Power curves",
    "text": "Power curves\n\nWe can often get a good idea about the SD from the literature or from knowledge about how our assay/measurement works, so we’ll ignore that for now (it is harder for observational studies than for experiments).\nBut in general we have no idea what the effect size (difference in means) should be before we do the study. So how do we get the power or sample size?\n\n\n\nWe can calculate the required sample size to achieve multiple different effect sizes. For the birthwt example, let’s pretend we didn’t run our study yet, but we know that the pooled sd should be around 700. Let’s calculate the sample size needed to achieve 80% power at multiple effect sizes.\nIn real life, maybe we can get this magical 700 number from previous studies or preliminary data.\n\n\n\n\nWe can use the tools we’ve learned to get many different sample size calculations.\n\n\neffect_sizes &lt;- seq(10, 500, 10)\nsample_size_calc &lt;- lapply(\n    effect_sizes,\n    \\(d) power.t.test(delta = d, sd = 700, sig.level = 0.05, power = 0.80)\n)\nstr(sample_size_calc, 1)\n\nList of 50\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n $ :List of 8\n  ..- attr(*, \"class\")= chr \"power.htest\"\n\n\n\n\n\nBut how do we get the sample size out of each power object thing? Fortunately power.t.test() returns an S3 object of class power.htest, and the documentation tells us how to get the quantities we want.\nWe also want to round the estimate up to the nearest whole number and (optionally, but I prefer this) multiply by two for the total sample size.\n\n\nsample_size &lt;- sapply(sample_size_calc, \\(x) ceiling(x$n) * 2)\n\n\n\n\nWe can now generate a “power curve”, a plot of the (log) sample size needed vs. the effect size.\n\n\nplot(\n    effect_sizes,\n    log10(sample_size),\n    xlab = \"Effect size to detect at alpha = 0.05, power = 0.8\",\n    ylab = \"Log10 sample size needed\",\n    type = \"l\"\n)",
    "crumbs": [
      "Modules",
      "Power simulation"
    ]
  },
  {
    "objectID": "modules/power-sim.html#you-try-it-power-contours",
    "href": "modules/power-sim.html#you-try-it-power-contours",
    "title": "Module XX: Power 2 (Unlimited Power)",
    "section": "You try it! Power contours",
    "text": "You try it! Power contours\n\nIf we’re willing to do a little bit more wrangling, we can also do this for multiple variances.\nHere’s some code to help you get started, and then you can try to calculate the sample sizes yourself. First, make a grid of all the effect size and variance combinations we want to try.\nNote that the more values of delta and SD you want to try, the grid can become huge really quickly which could take a long time to run.\n\n\npower_variables &lt;- expand.grid(\n    delta = effect_sizes,\n    sd = seq(100, 1000, 100)\n)\n\n\n\nNow because they are in a grid, you can do a 1-dimensional loop or lapply(), instead of having to do a nested one, just iterate over the number of rows in the grid.\n\n\npower_sim_sd &lt;-\n    lapply(\n        1:nrow(power_variables),\n        \\(i) power.t.test(\n            ...\n        )\n    )\n\n\n\n\nMy solution:\n\n\npower_sim_sd &lt;-\n    lapply(\n        1:nrow(power_variables),\n        \\(i) power.t.test(\n            delta = power_variables[i, \"delta\"],\n            sd = power_variables[i, \"sd\"],\n            sig.level = 0.05, power = 0.80\n        )\n    )\n\npower_variables$sample_size &lt;- sapply(power_sim_sd, \\(x) ceiling(x$n) * 2)\n\n\n\n\nThere are two main ways people will try to plot these curves. One is by plotting a different colored curve for each SD.\n\n\nplot(\n    NULL, NULL,\n    xlim = range(effect_sizes),\n    ylim = range(log10(power_variables$sample_size)),\n    xlab = \"Effect size to detect at alpha = 0.05, power = 0.8\",\n    ylab = \"Log10 sample size needed\",\n    type = \"l\"\n)\n\nvariance_levels &lt;- unique(power_variables$sd)\ncolors &lt;- colorRampPalette(c(\"lightblue\", \"darkblue\"))(length(variance_levels))\nfor (i in 1:length(variance_levels)) {\n    sub &lt;- subset(power_variables, sd == variance_levels[[i]])\n    lines(x = sub$delta, y = log10(sub$sample_size), col = colors[[i]])\n}\n\n\n\n\n\n\n\n\n\n\n\nIf you know ggplot2, this is way easier in ggplot2.\n\n\nlibrary(ggplot2)\nggplot(power_variables) +\n    aes(x = delta, y = log10(sample_size), color = sd, group = sd) +\n    geom_line() +\n    labs(\n    x = \"Effect size to detect at alpha = 0.05, power = 0.8\",\n    y = \"Log10 sample size needed\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nMathematicians love to plot this as a “contour plot”. These are basically impossible to read but look impressive.\nThis is insanely annoying to do in base R, so I’ll only show it in ggplot2.\n\n\nggplot(power_variables) +\n    aes(x = delta, y = sd, z = log10(sample_size)) +\n    geom_contour_filled() +\n    coord_cartesian(expand = FALSE) +\n    labs(\n        x = \"Effect size to detect at alpha = 0.05, power = 0.8\",\n        y = \"Assumed pooled SD\",\n        fill = \"Log10 sample sized need\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")",
    "crumbs": [
      "Modules",
      "Power simulation"
    ]
  },
  {
    "objectID": "modules/power-sim.html#power-simulations",
    "href": "modules/power-sim.html#power-simulations",
    "title": "Module XX: Power 2 (Unlimited Power)",
    "section": "Power simulations",
    "text": "Power simulations\n\nPower calculations are “non-analytic” for nearly all hypothesis tests, except for very simple ones.\nSimulation is a much more robust way to estimate power, because it works for any test.\nAs an example, we’ll first walk through simulating the power for the same t-test we just did.\nRemember that power is the chance that a false null hypothesis will be rejected.\n\n\n\nChoose the parameters for the simulation. These include all of the TRUE parameters for the data we hope to collect, the significance level, and the number of simulations.\n\n\nN_sims &lt;- 10000\nalpha &lt;- 0.05\n\n# T-test parameter\n# Number of samples\nn1 &lt;- 115\nn2 &lt;- 74\n# Group means\nmu1 &lt;- 2500\nmu2 &lt;- 2200\n# Group SD\nsd1 &lt;- 750\nsd2 &lt;- 650\n\n\nNow we generate N_sims number of datasets from a model described by the \\(t\\)-test: that is, we draw n1 observations from a normal distribution wiht mean mu1 and sd sd1, and likewise for group 2.\n\n\nsimulate_one_dataset &lt;- function(n1, n2, mu1, mu2, sd1, sd2) {\n    group1_y &lt;- rnorm(n1, mu1, sd1)\n    group2_y &lt;- rnorm(n2, mu2, sd2)\n    \n    out &lt;- list(group1 = group1_y, group2 = group2_y)\n    return(out)\n}\n\n# replicate() is just a fancy way to use lapply() when the function\n# you want to repeat doesn't depend on any iteration-specific argument\nsimulated_data &lt;- replicate(\n    N_sims,\n    simulate_one_dataset(n1, n2, mu1, mu2, sd1, sd2),\n    # Set simplify = FALSE or you get a weird array thats hard to understand\n    simplify = FALSE\n)\n\n\nNow we do a (Welch’s two-sample) t-test on each simulated dataset. Note that in our true data generating process, the null hypothesis is always false by design. (This is one of the times where it’s easier NOT to use a formula!)\n\n\nsimulated_t_tests &lt;- lapply(\n    simulated_data,\n    \\(d) t.test(d$group1, d$group2, conf.level = alpha)\n)\n\n\nNow we count the t-tests that correctly rejected the null hypothesis, i.e. \\(p \\leq \\alpha\\).\n\n\nsimulation_p_values &lt;- sapply(simulated_t_tests, \\(x) x$p.value)\nrejected_h0 &lt;- simulation_p_values &lt;= alpha\n\n\nNow we calculate the power of our test as the probability that a simulated test rejected H0. Note that we even compute a confidence interval for our power here, although its interpretation is a bit tricky it can tell us a range of powers that are consistent with our simulation.\n\n\n# We can use this function to get the exact CI (Clopper-Pearson) for us\n# Just ignore the p-value, it doesn't make sense!!\nbinom.test(sum(rejected_h0), N_sims)\n\n\n    Exact binomial test\n\ndata:  sum(rejected_h0) and N_sims\nnumber of successes = 8227, number of trials = 10000, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.8150708 0.8301408\nsample estimates:\nprobability of success \n                0.8227 \n\n\n\nInterpretation: the power of our test after 10,000 replicates was about 83%, and our CI is pretty tight around this number. If this wasn’t post-hoc, we would say that the planned sample sizes achieve our goal of \\(80\\%\\) power at \\(\\alpha = 0.05\\).",
    "crumbs": [
      "Modules",
      "Power simulation"
    ]
  },
  {
    "objectID": "modules/power-sim.html#you-try-it",
    "href": "modules/power-sim.html#you-try-it",
    "title": "Module XX: Power 2 (Unlimited Power)",
    "section": "You try it!",
    "text": "You try it!\n\nMore often, we want to know what sample size it takes to get 80% power for a fixed leve\nThe only way to do this by simulation is to try many sample sizes and do a simulation for each one.\nFor the t-test example (this time you can assume the sample size for the two groups is the same to make life easier), make a plot of power vs. sample size, using the same group means and group SDs.\n\n\n\nHint: write a function that simulates the data, does the t-tests, and gets which tests are rejections. Then, lapply() this function over multiple values of n (this is easy if you assume n1 and n2 are equal).\n\n\n\n\nHint: here’s an outline for my function, and the code for how I used it.\nI returned the entire vector of TRUE/FALSE for rejections from the function, because I thought this was a convenient way to handle the data. You can do the sum() or mean() inside of the function and return a single number if you prefer that.\n\n\nsimulate_t_test_power &lt;- function(N_sim, n1, n2, mu1, mu2, sd1, sd2, alpha = 0.05) {\n    simulated_data &lt;-\n        replicate(\n            N_sims,\n            simulate_one_dataset(n1, n2, mu1, mu2, sd1, sd2),\n            simplify = FALSE\n        )\n    \n    simulated_t_tests &lt;- lapply(\n        simulated_data,\n        \\(d) ...\n    )\n    \n    simulation_p_values &lt;- sapply(simulated_t_tests, ...)\n    rejected_h0 &lt;- ...\n    \n    return(rejected_h0)\n}\n\nn_per_group &lt;- seq(...)\n\nt_test_n_simulations &lt;- sapply(\n    n_per_group,\n    \\(n) simulate_t_test_power(\n        1000,\n        n, n,\n        mu1, mu2, sd1, sd2\n    )\n)\n\n\n\n\nHint: after I ran this function, here’s how I got the power for each sample size.\n\n\ncolMeans(t_test_n_simulations)\n\n\n\n\nHere’s my solution.\n\n\nsimulate_t_test_power &lt;- function(N_sim, n1, n2, mu1, mu2, sd1, sd2, alpha = 0.05) {\n    simulated_data &lt;-\n        replicate(\n            N_sims,\n            simulate_one_dataset(n1, n2, mu1, mu2, sd1, sd2),\n            simplify = FALSE\n        )\n    \n    simulated_t_tests &lt;- lapply(\n        simulated_data,\n        \\(d) t.test(d$group1, d$group2, conf.level = alpha)\n    )\n    \n    simulation_p_values &lt;- sapply(simulated_t_tests, \\(x) x$p.value)\n    rejected_h0 &lt;- simulation_p_values &lt;= alpha\n    \n    return(rejected_h0)\n}\n\n# You can do less of these\n# and make N_sim lower\n# if this takes too long\nn_per_group &lt;- seq(25, 250, 25)\n\nset.seed(102)\nt_test_n_simulations &lt;- sapply(\n    n_per_group,\n    \\(n) simulate_t_test_power(\n        1000,\n        n, n,\n        mu1, mu2, sd1, sd2\n    )\n)\n\n# sapply() gives us a matrix that we can easily apply() on to get summaries\n# or use colMeans() cause its super fast and easy\nres &lt;- colMeans(t_test_n_simulations)\nres\n\n [1] 0.3197 0.5670 0.7407 0.8495 0.9164 0.9576 0.9769 0.9880 0.9942 0.9976\n\n\n\n\n\nBy looking at these results, it’s clear where we can do a second simulation to hone in on a better sample size – between the 3rd and 4th levement of n_per_group we get to our target.\n\n\n# Interpolate between those two numbers, but don't repeat them\n# cause we already did those sims!\nn_per_group2 &lt;- seq(n_per_group[3], n_per_group[4], 1)\nn_per_group2_trim &lt;- n_per_group2[2:(length(n_per_group2) - 1)]\n\nset.seed(103)\nt_test_n_simulations2 &lt;- sapply(\n    n_per_group2_trim,\n    \\(n) simulate_t_test_power(\n        1000,\n        n, n,\n        mu1, mu2, sd1, sd2\n    )\n)\n\n# sapply() gives us a matrix that we can easily apply() on to get summaries\n# or use colMeans() cause its super fast and easy\nres2 &lt;- colMeans(t_test_n_simulations2)\nres2\n\n [1] 0.7448 0.7449 0.7681 0.7615 0.7647 0.7690 0.7764 0.7819 0.7814 0.7889\n[11] 0.7952 0.8072 0.8007 0.8061 0.8180 0.8135 0.8149 0.8316 0.8313 0.8361\n[21] 0.8410 0.8420 0.8463 0.8508\n\n\n\n\n\nNow I’ll show you a neat trick to combine these simulations together and make a plot.\n\n\nall_n &lt;- c(n_per_group, n_per_group2_trim)\nall_res &lt;- c(res, res2)\nsort_order &lt;- order(all_n)\n\nplot(\n    all_n[sort_order], all_res[sort_order],\n    type = \"l\",\n    xlab = \"Sample size\", ylab = \"Power\"\n)\n# Find the smallest sample size with a power above 0.8\nbest_samp_size &lt;- min(all_n[all_res &gt; 0.8])\nbss_power &lt;- all_res[all_n == best_samp_size]\n\nabline(h = bss_power, col = \"red\", lty = 2)\nabline(v = best_samp_size, col = \"red\", lty = 2)",
    "crumbs": [
      "Modules",
      "Power simulation"
    ]
  },
  {
    "objectID": "modules/power-sim.html#setting-up-a-power-simulation-for-logistic-regression",
    "href": "modules/power-sim.html#setting-up-a-power-simulation-for-logistic-regression",
    "title": "Module XX: Power 2 (Unlimited Power)",
    "section": "Setting up a power simulation for logistic regression",
    "text": "Setting up a power simulation for logistic regression\n\nSimulating power is a MUST when we need to use a more complex model. For example, we have to do simulation to get the power of logistic regression.\nI’ll walk you through the data generating code and the correct p-value to check, but I’ll let you fill in the code details.\n\n\n\nRecall that the model we fit for logistic regression is something like this.\n\n\\[\ny_i \\sim \\text{Bernoulli}(p_i) \\\\\n\\text{logit}(p_i) = \\beta_0 + \\beta_1 x_i \\\\\n\\]\n\nSo the things we need to assume are values for \\(\\beta_0\\) and \\(\\beta_1\\) and the allowed values or distribution of \\(x_i\\). And of course we need a sample size \\(n\\) that we’ll vary to see how the power changes.\nWe can interpret \\(\\beta_0\\) as the log-odds of the event occurring for an individual at baseline. Let’s assume our outcome is somewhat rare, say \\(10\\%\\) of people get it. We can convert this to log-odds with some R code.\n\n\n# qlogis turns a probability into a log-odds\n# plogis turns a log-odds into a probability\nqlogis(0.1)\n\n[1] -2.197225\n\n\n\n\n\nSo we’ll say \\(\\beta_0 = -2.2\\).\nNow we need to assume values for \\(x\\) and an effect size \\(\\beta_1\\). For simplicity, let’s say \\(x\\) is dichotomous, and if an individual has the exposure \\(x=1\\), their log-odds are \\(1.5\\times\\) the log-odds of someone with \\(x=0\\), which means \\(\\beta_1 = 1.5\\).\n\n\n\n\nFinally, since \\(x\\) is dichotomous, we need a baseline prevalence of \\(x\\). Let’s say the prevalence of \\(x\\) is \\(40\\%\\), pretty common but not quite half and half.\nSo then we can sample from the model.\n\n\ngenerate_logistic_model_data &lt;- function(n, beta_0, beta_1, exposure_prevalence) {\n    x &lt;- rbinom(n, size = 1, prob = exposure_prevalence)\n    logit_p &lt;- beta_0 + beta_1 * x\n    p &lt;- plogis(logit_p)\n    y &lt;- rbinom(n, size = 1, prob = p)\n    \n    out &lt;- list(x = x, y = y)\n    return(out)\n}\n\nset.seed(100)\ntest_logistic_data &lt;- generate_logistic_model_data(100, -2.2, 1.5, 0.5)\n\n\n\n\nNow let’s walk through the test we need to do.\nFirst we fit a glm.\n\n\ntest_logistic_fit &lt;- glm(\n    test_logistic_data$y ~ test_logistic_data$x,\n    family = \"binomial\"\n)\n\nsummary(test_logistic_fit)\n\n\nCall:\nglm(formula = test_logistic_data$y ~ test_logistic_data$x, family = \"binomial\")\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)           -2.7515     0.5955  -4.621 3.83e-06 ***\ntest_logistic_data$x   2.3461     0.6618   3.545 0.000392 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 107.855  on 99  degrees of freedom\nResidual deviance:  89.998  on 98  degrees of freedom\nAIC: 93.998\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nFrom the summary output, we see that the p-value for our test of interest is \\(0.000392\\). How do we get that out of the test?\n\n\n\n\nThe summary of the glm is an S3 object! But it’s a different class than the glm fit.\n\n\nglm_summary &lt;- summary(test_logistic_fit)\n\n# The coefficients are a matrix so we put in the indices that we want\nglm_summary$coefficients[2, 4]\n\n[1] 0.0003923781",
    "crumbs": [
      "Modules",
      "Power simulation"
    ]
  },
  {
    "objectID": "modules/power-sim.html#you-try-it-finish-the-power-simulation",
    "href": "modules/power-sim.html#you-try-it-finish-the-power-simulation",
    "title": "Module XX: Power 2 (Unlimited Power)",
    "section": "You try it! Finish the power simulation",
    "text": "You try it! Finish the power simulation\n\nNow that we’ve walked through the building blocks, finish the power simulation. Find an \\(n\\) where the power under our assumptions about the data is between \\(0.75\\) and \\(0.85\\), that range is close enough.\n\n\n\nHint: this kind of power simulation is easier if you break it down into components, write a function for each component, and then put them all together.\nI think you need three components: a data simulation function that returns a data set, a test function that returns a p-value for the given dataset (or returns accept/reject), and a function that puts those two parts together so you can repeat the whole thing.\n\n\n\n\nHint: here’s the test function.\n\n\nget_exposure_slope_p_value &lt;- function(simulated_dataset) {\n    logistic_fit &lt;- glm(\n        simulated_dataset$y ~ simulated_dataset$x,\n        family = \"binomial\"\n    )\n    \n    glm_summary &lt;- summary(logistic_fit)\n    \n    p_value &lt;- glm_summary$coefficients[2, 4]\n    \n    return(p_value)\n}\n\n\n\n\nHint: now you can put the two parts together and then get the rejections.\n\n\nlogistic_power_simulation &lt;- function(\n        N_sims, n, beta_0, beta_1, exposure_prevalence, alpha\n) {\n    simulated_datasets &lt;- replicate(\n        ...\n    )\n    \n    simulated_p_values &lt;- sapply(\n        ...\n    )\n    \n    rejections &lt;- ...\n    \n    return(rejections)\n}\n\n\n\n\nHint: here’s my completed function.\nCan you repeat it for multiple n values now?\n\n\nlogistic_power_simulation &lt;- function(\n        N_sims, n, beta_0, beta_1, exposure_prevalence, alpha\n) {\n    simulated_datasets &lt;- replicate(\n        N_sims,\n        generate_logistic_model_data(n, beta_0, beta_1, exposure_prevalence),\n        simplify = FALSE\n    )\n    \n    simulated_p_values &lt;- sapply(\n        simulated_datasets,\n        get_exposure_slope_p_value\n    )\n    \n    rejections &lt;- simulated_p_values &lt;= alpha\n    \n    return(rejections)\n}\n\n\n\n\nn_values &lt;- seq(25, 250, 25)\n\nset.seed(110)\nlogistic_sim &lt;- sapply(\n    n_values,\n    \\(n) logistic_power_simulation(\n        1000,\n        n,\n        -2.2, 1.5, 0.4, 0.05\n    )\n)\n\nlogistic_sim_power &lt;- colMeans(logistic_sim)\n\nplot(\n    n_values, logistic_sim_power,\n    xlab = \"Sample size\", ylab = \"Power\",\n    type = \"l\"\n)\n\n\n\n\n\n\n\n\n\nIf we look at the values, we see that a sample size of 100 gives us a power of about \\(81\\%\\) under these assumptions.",
    "crumbs": [
      "Modules",
      "Power simulation"
    ]
  },
  {
    "objectID": "modules/power-sim.html#final-note-parametrizations",
    "href": "modules/power-sim.html#final-note-parametrizations",
    "title": "Module XX: Power 2 (Unlimited Power)",
    "section": "Final note: parametrizations",
    "text": "Final note: parametrizations\n\nNotice that in the logistic regression simulation, we simulated our dataset using the exposure prevalence. If you know the exposure prevalence, that works fine.\nIf you don’t know the exposure prevalence, you can instead simulate a dataset with n_exposed and n_unexposed, or assume the sample size of the two groups has to be equal like we did for the t-test example.\nThese are basically the same, we just assumed that n_exposed / n_total = 0.4, but there’s lots of ways you can do the same thing.\nIn general, what matters the most for the power of a logistic regression is the number of events in each group, and you can write a simulation that takes that into account too.",
    "crumbs": [
      "Modules",
      "Power simulation"
    ]
  },
  {
    "objectID": "modules/power-sim.html#summary",
    "href": "modules/power-sim.html#summary",
    "title": "Module XX: Power 2 (Unlimited Power)",
    "section": "Summary",
    "text": "Summary\n\nPower is kind of confusing, we have to make really strong assumptions to do an a priori power analysis. But post hoc power analysis doesn’t really tell us anything.\nCalculating the power of a test directly is impossible for most tests.\nFortunately we can use the tools we learned in this class to do a power simulation, which can help us estimate power under various assumptions for complicated models.",
    "crumbs": [
      "Modules",
      "Power simulation"
    ]
  },
  {
    "objectID": "modules/bootstrapping.html#learning-goals",
    "href": "modules/bootstrapping.html#learning-goals",
    "title": "Module XX: Bootstrapping can be easy and fun",
    "section": "Learning goals",
    "text": "Learning goals\n\nUnderstand the basics of bootstrapping – be able to construct your own bootstrap resamples and calculate a bootstrap CI for a statistic.\nMake your own bootstraps with loops or functional programming.\nCombine your own knowledge of R programming techniques with the boot package for easy bootstrapping.\nUse the rsample package and advanced R programming techniques to iterate over lists of bootstrap resamples.",
    "crumbs": [
      "Modules",
      "Bootstrapping"
    ]
  },
  {
    "objectID": "modules/bootstrapping.html#what-is-bootstrapping",
    "href": "modules/bootstrapping.html#what-is-bootstrapping",
    "title": "Module XX: Bootstrapping can be easy and fun",
    "section": "What is bootstrapping?",
    "text": "What is bootstrapping?\n\nbootstrapping is a method for obtaining standard error estimates or confidence intervals by resampling.\nWe’ll specifically talk about the nonparametric bootstrap but there are entire books about bootstrap methods.\n\n\ncredit: https://commons.wikimedia.org/wiki/File:Illustration_bootstrap.svg",
    "crumbs": [
      "Modules",
      "Bootstrapping"
    ]
  },
  {
    "objectID": "modules/bootstrapping.html#what-is-bootstrapping-1",
    "href": "modules/bootstrapping.html#what-is-bootstrapping-1",
    "title": "Module XX: Bootstrapping can be easy and fun",
    "section": "What is bootstrapping?",
    "text": "What is bootstrapping?\n\nWe have some statistic \\(\\theta\\) that we want to estimate from the dataset.\nWe create \\(B\\) resamples of the dataset by randomly sampling from it, and then we calculate the statistic \\(\\theta_B\\) on each dataset.\nIf we create these resamples the correct way, the observed distribution of \\(\\theta_1, \\ldots, \\theta_B\\) will converge to the true sampling distribution of \\(\\theta\\).\nSpecifically, we create a resample by sampling \\(n\\) (our sample size) rows from the original dataset with replacement.",
    "crumbs": [
      "Modules",
      "Bootstrapping"
    ]
  },
  {
    "objectID": "modules/bootstrapping.html#what-is-bootstrapping-2",
    "href": "modules/bootstrapping.html#what-is-bootstrapping-2",
    "title": "Module XX: Bootstrapping can be easy and fun",
    "section": "What is bootstrapping?",
    "text": "What is bootstrapping?\n\nThat’s math jargon to say that sometimes doing this resampling thing will give us better confidence intervals than normality assumptions.\nUsually bootstrapping works well for small datasets, estimates with complex (or nonexistant) standard error formulas, or for statistics with highly skewed distributions.\n\n\ncredit: https://commons.wikimedia.org/wiki/File:Illustration_bootstrap.svg",
    "crumbs": [
      "Modules",
      "Bootstrapping"
    ]
  },
  {
    "objectID": "modules/bootstrapping.html#low-birthweight-data-and-the-data-function",
    "href": "modules/bootstrapping.html#low-birthweight-data-and-the-data-function",
    "title": "Module XX: Bootstrapping can be easy and fun",
    "section": "Low birthweight data and the data() function",
    "text": "Low birthweight data and the data() function\n\nFor this example, we’ll use a famous epidemiological dataset about babies with low birthweight.\nMany R packages have built-in datasets, which you can access with the data() function.\nThe low birth weight dataset, called birthwt, is in the MASS package, so you can access it with either MASS::birthwt or data(\"birthwt\", package = \"MASS\").\n\n\n# This will create the global variable \"birthwt\"\ndata(\"birthwt\", package = \"MASS\")",
    "crumbs": [
      "Modules",
      "Bootstrapping"
    ]
  },
  {
    "objectID": "modules/bootstrapping.html#bootstrapping-example",
    "href": "modules/bootstrapping.html#bootstrapping-example",
    "title": "Module XX: Bootstrapping can be easy and fun",
    "section": "Bootstrapping example",
    "text": "Bootstrapping example\n\nUse bootstrapping to calculate a 95% CI for the risk difference in low birth weight (low, 1 is low birth weight) between mothers who smoke (smoke = 1) and don’t (smoke = 0).\n\n\n\nFirst create the resamples. We need to use set.seed() to make sure our results are the same every time we run our code!\n\n\nset.seed(370)\nB &lt;- 1000\n# Alternatively use replicate() if you don't like the weird function here\n# or use a loop\nresamples &lt;- lapply(\n    1:B,\n    \\(n) birthwt[sample.int(nrow(birthwt), replace = TRUE), ]\n)\n\nstr(resamples, 1)\n\nList of 1000\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n $ :'data.frame':   189 obs. of  10 variables:\n  [list output truncated]\n\n\n\n\n\nNow write a function to calculate the statistic.\nMake sure to calculate the point estimate on the entire dataset as well.\n\n\nrd_smoking &lt;- function(resample) {\n    risk_smoke &lt;- mean(resample$low[resample$smoke == 1])\n    risk_no_smoke &lt;- mean(resample$low[resample$smoke == 0])\n    \n    rd &lt;- risk_smoke - risk_no_smoke\n    return(rd)\n}\n\npoint_estimate_rd &lt;- rd_smoking(birthwt)\npoint_estimate_rd\n\n[1] 0.1532315\n\n\n\n\n\nNow calculate the statistic on every resample.\n\n\nbootstrap_rd &lt;- sapply(resamples, rd_smoking)\nstr(bootstrap_rd)\n\n num [1:1000] 0.2051 0.0224 0.2446 0.1094 0.142 ...\n\n\n\n\n\nNow we have to calculate the SE. There are a few different ways. First is the normal method, where we calculate the standard error from the bootstrap distribution.\nThis is just the standard deviation of the bootstrap estimates. Then we calculate the normal-approximate CI as usual.\n\n\nboot_se &lt;- sd(bootstrap_rd)\npoint_estimate_rd + boot_se * c(-1.96, 1.96)\n\n[1] 0.006663599 0.299799385\n\n\n\n\n\nThat method is kind of bad, but common. A better method is the percentile method which uses the quantiles of the bootstrap estimates.\n\n\nquantile(bootstrap_rd, prob = c(0.025, 0.975))\n\n       2.5%       97.5% \n0.009800072 0.301920525 \n\n\n\nIn this case they’re the same because this RD is easy to estimate. But that won’t always be true!",
    "crumbs": [
      "Modules",
      "Bootstrapping"
    ]
  },
  {
    "objectID": "modules/bootstrapping.html#you-try-it",
    "href": "modules/bootstrapping.html#you-try-it",
    "title": "Module XX: Bootstrapping can be easy and fun",
    "section": "You try it!",
    "text": "You try it!\n\nLook up the formula for the standard error of the risk ratio.\nCalculate the risk ratio for low birthweight in mothers with (ht = 1) and without (ht = 0) a history of hypertension using the standard CI, and using a bootstrap CI with the percentile method.\n\n\n\nHint:\n\n\nrr_ht &lt;- function(resample) {\n    risk_ht_1 &lt;- mean(resample$low[resample$ht == 1])\n    risk_ht_0 &lt;- mean(resample$low[resample$ht == 0])\n    \n    rr &lt;- risk_ht_1 / risk_ht_0\n    return(rr)\n}\n\n\nHint: the formula for the SE of the log risk ratio is \\[\n\\sqrt{\\bigg( \\frac{1}{a} + \\frac{1}{c} \\bigg) - \\bigg( \\frac{1}{a+b} + \\frac{1}{c+d}  \\bigg)}\n\\]\n\nfor a \\(2\\times2\\) table.\n\n\n\n\n\n\nExposed\nUnexposed\n\n\n\n\nExposed\na\nb\n\n\nUnexposed\nc\nd\n\n\n\n\n\n\n\n\nSolution: first we calculate the point estimate and the Wald-type CI based on the SE formula.\n\n\npoint_estimate_rr &lt;- rr_ht(birthwt)\ncontigency_table_ht &lt;- table(\n    factor(birthwt$ht, c(1, 0), c(\"Exposed\", \"Unexposed\")),\n    factor(birthwt$smoke, c(1, 0), c(\"Case\", \"Control\"))\n)\n\ncalculate_log_rr_se &lt;- function(contigency_table) {\n    a &lt;- contigency_table[1, 1]\n    b &lt;- contigency_table[1, 2]\n    c &lt;- contigency_table[2, 1]\n    d &lt;- contigency_table[2, 2]\n    \n    se &lt;- sqrt((1/a + 1/c) - (1/(a+b) + 1/(c+d)))\n    return(se)\n}\nlog_rr_se &lt;- calculate_log_rr_se(contigency_table_ht)\n# It's on the log scale, so we have to calculate the CI like this!\nexp(log_rr_se * c(-1.96, 1.96) + log(point_estimate_rr))\n\n[1] 0.9915692 3.9760368\n\n\n\n\n\nSolution: now we do the bootstrap CI.\n\n\nrr_ht &lt;- function(resample) {\n    risk_ht_1 &lt;- mean(resample$low[resample$ht == 1])\n    risk_ht_0 &lt;- mean(resample$low[resample$ht == 0])\n    \n    rr &lt;- risk_ht_1 / risk_ht_0\n    return(rr)\n}\n\nbootstrap_rr &lt;- sapply(resamples, rr_ht)\nquantile(bootstrap_rr, c(0.025, 0.975))\n\n    2.5%    97.5% \n1.032770 3.237805",
    "crumbs": [
      "Modules",
      "Bootstrapping"
    ]
  },
  {
    "objectID": "modules/bootstrapping.html#bootstrapping-multiple-statistics-at-one-time",
    "href": "modules/bootstrapping.html#bootstrapping-multiple-statistics-at-one-time",
    "title": "Module XX: Bootstrapping can be easy and fun",
    "section": "Bootstrapping multiple statistics at one time",
    "text": "Bootstrapping multiple statistics at one time\n\nIf our bootstrap function returns a vector, we can get multiple statistics at once.\n\n\nget_smoking_stats &lt;- function(resample) {\n    risk_smoke &lt;- mean(resample$low[resample$smoke == 1])\n    risk_no_smoke &lt;- mean(resample$low[resample$smoke == 0])\n    \n    odds_smoke &lt;- risk_smoke / (1 - risk_smoke)\n    odds_no_smoke &lt;- risk_no_smoke / (1 - risk_no_smoke)\n    \n    rd &lt;- risk_smoke - risk_no_smoke\n    rr &lt;- risk_smoke / risk_no_smoke\n    or &lt;- odds_smoke / odds_no_smoke\n    \n    out &lt;- c(\n        \"Risk difference\" = rd,\n        \"Risk ratio\" = rr,\n        \"Odds ratio\" = or\n    )\n    return(out)\n}\n\nsmoking_stats &lt;- sapply(resamples, get_smoking_stats)\nstr(smoking_stats)\n\n num [1:3, 1:1000] 0.2051 2.0082 2.7044 0.0224 1.0879 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:3] \"Risk difference\" \"Risk ratio\" \"Odds ratio\"\n  ..$ : NULL\n\n\n\nThe output looks really confusing. But it’s a matrix so we can use apply()!\n\n\npoint_estimates &lt;- get_smoking_stats(birthwt)\nboot_cis &lt;- apply(smoking_stats, 1, \\(x) quantile(x, c(0.025, 0.975)))\n\n# Just some code to show everything neatly\nrbind(\n    \"Lower\" = boot_cis[1, ],\n    \"Point\" = point_estimates,\n    \"Upper\" = boot_cis[2, ]\n) |&gt;\n    t() |&gt;\n    round(digits = 2)\n\n\n\n\n\nLower\nPoint\nUpper\n\n\n\n\nRisk difference\n0.01\n0.15\n0.30\n\n\nRisk ratio\n1.04\n1.61\n2.57\n\n\nOdds ratio\n1.05\n2.02\n4.18",
    "crumbs": [
      "Modules",
      "Bootstrapping"
    ]
  },
  {
    "objectID": "modules/bootstrapping.html#errors-in-bootstrapping-and-the-boot-package",
    "href": "modules/bootstrapping.html#errors-in-bootstrapping-and-the-boot-package",
    "title": "Module XX: Bootstrapping can be easy and fun",
    "section": "Errors in bootstrapping and the boot package",
    "text": "Errors in bootstrapping and the boot package\n\nThe situations we just observed are pretty simple, and our bootstrap estimates are pretty trustworthy (they would be more trustworthy if we increased B, a good rule of thumb is 1,000 just for you, 10,000 for your boss, and 100,000 for a paper if it’s feasible).\nBut in some situations, bootstrapping can be biased. There is a fix for this called the “BCa” bootstrap.\nBCa is hard to do by hand, but easy to do with the boot package.",
    "crumbs": [
      "Modules",
      "Bootstrapping"
    ]
  },
  {
    "objectID": "modules/bootstrapping.html#the-boot-package",
    "href": "modules/bootstrapping.html#the-boot-package",
    "title": "Module XX: Bootstrapping can be easy and fun",
    "section": "The boot package",
    "text": "The boot package\n\nWe have to write our function a certain way for boot. It must have two arguments, the first is the data, and the second is a list of indices that are included in a resample.\n\n\nboot_smoking_stats &lt;- function(data, idx) {\n    resample &lt;- data[idx, ]\n    out &lt;- get_smoking_stats(resample)\n    return(out)\n}\n\nlibrary(boot)\nbootstraps_smoking &lt;- boot::boot(birthwt, boot_smoking_stats, R = 1000)\nbootstraps_smoking\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot::boot(data = birthwt, statistic = boot_smoking_stats, R = 1000)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 0.1532315 0.0009951909  0.07034402\nt2* 1.6076421 0.0444346127  0.35834007\nt3* 2.0219436 0.1290172225  0.71556649\n\n\n\n\nWe can see this calculates the point estimate, bias, and boostrap SE for us. The bias is what we didn’t know how to calculate before.\nWe can easily compare multiple CI methods with boot.\nFor technical reasons that are too complicated to talk about, when you do multiple stats at one time in boot, you must always manually set the index argument.\n\n\n# Four bootstrap CIs for the risk difference\nboot::boot.ci(bootstraps_smoking, index = 1)\n\nWarning in boot::boot.ci(bootstraps_smoking, index = 1): bootstrap variances\nneeded for studentized intervals\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = bootstraps_smoking, index = 1)\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 0.0144,  0.2901 )   ( 0.0200,  0.2876 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.0189,  0.2864 )   ( 0.0153,  0.2834 )  \nCalculations and Intervals on Original Scale\n\n\n\n\n\nOf course, we can use loops or FP to do this for all of our statistics.\n\n\nsmoking_point_estimates &lt;- bootstraps_smoking$t0\nsmoking_cis_list &lt;- lapply(\n    1:length(smoking_point_estimates),\n    \\(i) boot::boot.ci(bootstraps_smoking, type = \"bca\", index = i)\n)\nstr(smoking_cis_list, 1)\n\nList of 3\n $ :List of 4\n  ..- attr(*, \"class\")= chr \"bootci\"\n $ :List of 4\n  ..- attr(*, \"class\")= chr \"bootci\"\n $ :List of 4\n  ..- attr(*, \"class\")= chr \"bootci\"\n\n\n\nCleaning them up is kind of difficult, this is a weirdly formatted S3 object.\n\n\nsmoking_cis &lt;- sapply(\n    smoking_cis_list,\n    \\(x) x$bca[4:5]\n)\n\n# Same cleanup code as before\nrbind(\n    \"lower\" = smoking_cis[1, ],\n    \"point\" = smoking_point_estimates,\n    \"upper\" = smoking_cis[2, ]\n) |&gt;\n    t() |&gt;\n    round(digits = 2)\n\n\n\n\n\nlower\npoint\nupper\n\n\n\n\nRisk difference\n0.02\n0.15\n0.28\n\n\nRisk ratio\n1.05\n1.61\n2.42\n\n\nOdds ratio\n1.07\n2.02\n3.67",
    "crumbs": [
      "Modules",
      "Bootstrapping"
    ]
  },
  {
    "objectID": "modules/bootstrapping.html#you-try-it-1",
    "href": "modules/bootstrapping.html#you-try-it-1",
    "title": "Module XX: Bootstrapping can be easy and fun",
    "section": "You try it!",
    "text": "You try it!\n\nFit a logistic regression model to predict low birth weight using this dataset. Use any predictors in the dataset that you think are relevant.\nGet the standard confidence intervals using the profile method (that is, with confint()).\nGet bootstrap estimates for the coefficients.\nDo the bootstrap estimates lead you to the same interpretation, or a different interpretation?\n\n(No solution typed up for this problem, if we have time/interest we can cover it together.)",
    "crumbs": [
      "Modules",
      "Bootstrapping"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#learning-goals",
    "href": "modules/ODEs-optim.html#learning-goals",
    "title": "Module XX: FP Applications",
    "section": "Learning goals",
    "text": "Learning goals\n\nUse function-writing and functional programming techniques to deal with applied epidemiology problems.\nOptimize your own function with optim().\nSimulate and solve differential equations with deSolve.",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#what-is-optim",
    "href": "modules/ODEs-optim.html#what-is-optim",
    "title": "Module XX: FP Applications",
    "section": "What is optim()",
    "text": "What is optim()\n\nGeneric R interface to optimization algorithms.\nOptimization: for a function \\(f(x)\\), find \\(x^\\star\\) so that \\(f(x^\\star) \\geq f(x)\\) for any other \\(x\\).\nTakes a function to be minimized as an argument. (If you want the maximum, you need to put a negative sign.)",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#simple-example",
    "href": "modules/ODEs-optim.html#simple-example",
    "title": "Module XX: FP Applications",
    "section": "Simple example",
    "text": "Simple example\n\nFind the maximum value of \\(f(x) = \\sin(x)e^{-x^2}\\).\n\n\ncurve(\n    sin(x) * exp(-x^2),\n    from = -2, to = 2, n = 1000\n)\n\n\n\n\n\n\n\n\n\n\noptim(\n    par = your initial guess to start at,\n    fn = the function to optimize,\n    method = the method to use for optimization,\n    control = a list of other parameters\n)\n\n\n\n\nres &lt;- optim(\n    par = 0,\n    fn = \\(x) sin(x) * exp(-x^2),\n    method = \"BFGS\",\n    control = list(fnscale = -1) # This gets the maximum\n)\nres\n\n$par\n[1] 0.6532841\n\n$value\n[1] 0.396653\n\n$counts\nfunction gradient \n       8        7 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\n\n\ncurve(\n    sin(x) * exp(-x^2),\n    from = -2, to = 2, n = 1000\n)\nabline(h = res$value, col = \"red\")\nabline(v = res$par, col = \"red\")",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#arguments-explained",
    "href": "modules/ODEs-optim.html#arguments-explained",
    "title": "Module XX: FP Applications",
    "section": "Arguments explained",
    "text": "Arguments explained\n\npar is a vector of your parameter values. You need to pick starting values for the algorithm, usually the choice isn’t super important. (It can be for complicated problems.)\nfn is the function you want to minimize (or maximize with the fnscale = -1 control argument).\nmethod is the optimizer to use, this is a really technical choice. I prefer BFGS but Brent is also good for 1 parameter problems and Nelder-Mead is usually good for 2+ parameter problems if BFGS isn’t working.\ncontrol has a lot of complicated options that you usually won’t need, fnscale = -1 is the main one to know about.",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#why-use-optim",
    "href": "modules/ODEs-optim.html#why-use-optim",
    "title": "Module XX: FP Applications",
    "section": "Why use optim()",
    "text": "Why use optim()\n\nlm() and glm(), etc., do all of that stuff for me? Why do I need to do it by hand?\nSometimes you have a problem that doesn’t fit neatly into a pre-written model!\nWe’ll get to an example where this is true, but let’s build our way up.",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#optim-sampling-example",
    "href": "modules/ODEs-optim.html#optim-sampling-example",
    "title": "Module XX: FP Applications",
    "section": "Optim sampling example",
    "text": "Optim sampling example\n\nSuppose you’re in charge of monitoring mosquito traps for West Nile Virus at some health department (see, e.g., PMCID: PMC2631741).\nYou have 20 different traps you can check, and at each trap you select 10 mosquitoes for testing. We don’t have enough money or good enough equipment to test all of those mosquitoes, so we pool them together and get an overall yes (1) or no (0) for presence of mosquitoes.\nThis month, 7 out of 20 traps tested positive.\nEstimate the underlying mosquito infection risk from the pooled data using maximum likelihood.\n\n\n\nNote that you can do this as a binomial glm(). But this won’t be true in our next example.\nLet’s work through the math you need.\n\n\n\n\nGiven an infection rate, \\(p\\), the probability that pool \\(i\\) tests positive is \\[\nP(+) = 1 - (1 - p)^{10}\n\\]\nThe probability that 7 pools out of 20 tested positive is then \\[\n{{7}\\choose{20}} \\bigg(1 - (1 - p)^{10}\\bigg)^7 \\bigg((1 - p)^{10}\\bigg)^{20-7}\n\\]\nThis is written in R as dbinom(7, size = 20, prob = 1 - (1 - p)^10).\nFor technical reasons, it is easier to minimize the negative log likelihood than it is to directly maximize the likelihood.\nYou should write a function that gives the negative log likelihood and pass this function to optim() to find the best value of \\(p\\).\n\n\n\n\ninitial_guess &lt;- 7 / 20\n\n# You could write out the math for this function by hand if you wanted to\n# But dbinom uses some nice tricks to be faster and more accurate.\nnll &lt;- function(p) {\n  nll &lt;- -dbinom(7, size = 20, prob = 1 - (1 - p)^10, log = TRUE)\n  return(nll)\n}\n\noptim(\n    par = initial_guess,\n    fn = nll,\n    method = \"L-BFGS-B\", # Or \"Brent\"\n    lower = 0.001,\n    upper = 0.999\n)\n\n$par\n[1] 0.04217128\n\n$value\n[1] 1.690642\n\n$counts\nfunction gradient \n      15       15 \n\n$convergence\n[1] 0\n\n$message\n[1] \"CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH\"\n\n\n\nThe output tells us that \\(p \\approx 0.02\\) has the minimal negative log-likelihood at 1.69, so from the data we have, this is the most likely mosquito infection rate.",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#bonus-stage",
    "href": "modules/ODEs-optim.html#bonus-stage",
    "title": "Module XX: FP Applications",
    "section": "Bonus stage!",
    "text": "Bonus stage!\n\nUsually getting an approximate Wald-type confidence interval is very easy with optim().\nWe need to refit the model with hessian = TRUE. (Math terms: the Hessian returned here is the negative Fisher information matrix. The standard errors of the parameters are the square roots of the diagnol of the inverse Fisher information matrix.)\n\n\nres &lt;- optim(\n    par = 0.1,\n    fn = nll,\n    method = \"L-BFGS-B\",\n    lower = 0.00001,\n    upper = 0.99999,\n    hessian = TRUE\n)\nest &lt;- res$par\nse &lt;- sqrt(diag(solve(res$hessian)))\n\n# Approximate CI\nci_lower &lt;- est - 1.96 * se\nci_upper &lt;- est + 1.96 * se\nc(ci_lower, ci_upper)\n\n[1] 0.01137851 0.07296406\n\n\n\nSo in our report, we could say that we estimate an 4.2% WNV infection risk for mosquitoes in our area with a 95% CI of about 1.1% to 7.3%.",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#you-try-it",
    "href": "modules/ODEs-optim.html#you-try-it",
    "title": "Module XX: FP Applications",
    "section": "You try it!",
    "text": "You try it!\n\nLet’s extend the binomial example to a situation where we don’t have a built-in ML option in R.\nIn real life, your mosquito traps are probably heterogeneous. One process that can contribute to heterogeneity is zero-inflation: at some sites, there are no mosquitoes with WNV because WNV hasn’t been introduced to that area yet.\nSo now we know that if we observe a negative trap, it could be because there’s no WNV at all, or because we didn’t catch any mosquitoes with WNV. Call the probability that WNV has been introduced into the trap region \\(\\kappa\\).\nEstimate \\(p\\), the mosquito infection rate, and \\(\\kappa\\), the zero-inflation probability, assuming that 7 of 20 pools were positive and each pool had 10 mosquitos.\n\n\n\nWe introduce \\(\\kappa\\) into our model by now modeling the probability that a pool tests positive as \\[\nP(+) = \\kappa\\bigg(1 - (1 - p)^{10}\\bigg).\n\\]\nNow you just need to handle optimizing two parameters at one time. The input to your function needs to be a length two vector.\n\n\n\n\nHint: Your initial guess should be a length two vector (c(..., ...)) and your optimization function should look like this.\n\n\nnll_zip &lt;- function(par) {\n    p &lt;- par[1]\n    kappa &lt;- par[2]\n    \n    ...\n    \n    return(nll)\n}\n\n\n\n\nHint: when you call optim() you need to make sure that par, lower, and upper are all length-two vectors. You should still use the “L-BFGS-B” method for this problem.\n\n\n\n\nSolution:\n\n\nnll_zip &lt;- function(par) {\n  p_infect &lt;- par[1]\n  kappa &lt;- par[2]\n  \n  p_pos &lt;- kappa * (1 - (1 - p_infect) ^ 10)\n  \n  nll &lt;- -dbinom(7, 20, prob = p_pos, log = TRUE)\n  \n  return(nll)\n}\n\ninitial_guess &lt;- c(0.05, 0.5)\nres &lt;- optim(\n    par = initial_guess,\n    fn = nll_zip,\n    method = \"L-BFGS-B\",\n    lower = rep(0.001, 2),\n    upper = rep(0.999, 2),\n    hessian = TRUE\n)\nres\n\n$par\n[1] 0.1068171 0.5171114\n\n$value\n[1] 1.690642\n\n$counts\nfunction gradient \n      14       14 \n\n$convergence\n[1] 0\n\n$message\n[1] \"CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH\"\n\n$hessian\n         [,1]      [,2]\n[1,] 307.8070 111.32366\n[2,] 111.3237  40.27431\n\n\n\n\n\nThat’s all you had to do for this problem, but if you try to get the CI using the same method, you can see that this approximate method does a terrible job here.\nUnfortunately there’s no easy fix for this specific problem.\n\n\nest &lt;- res$par\nse &lt;- sqrt(diag(solve(res$hessian)))\n\n# Approximate CI\nci_lower &lt;- est - 1.96 * se\nci_upper &lt;- est + 1.96 * se\ncat(\n    paste0(\"Infection risk: \", round(est[[1]], 2), \" (\", round(ci_lower[[1]], 2),\n                 \", \", round(ci_upper[[1]], 2), \")\"),\n    paste0(\"Risk WNV in area: \", round(est[[2]], 2), \" (\", round(ci_lower[[2]], 2),\n                 \", \", round(ci_upper[[2]], 2), \")\"),\n    sep = \"\\n\"\n)\n\nInfection risk: 0.11 (-6.31, 6.52)\nRisk WNV in area: 0.52 (-17.22, 18.26)",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#bonus-problems",
    "href": "modules/ODEs-optim.html#bonus-problems",
    "title": "Module XX: FP Applications",
    "section": "Bonus problems",
    "text": "Bonus problems\n\nThe likelihood of a sample with multiple observations is the product of the likelihood for each datapoint. (Or, the negative log-likelihood of the sample is the sum of the negative log-likelihood of each datapoint.) Using the mos-nyc.csv dataset, which describes the number of positive pools out of total pools collected for some counties in New York State, estimate the mosquito infection rate under both the simple and mixture models. Assume these samples use 50 mosquitoes per pool instead of 10.\nFor an extra super bonus problem (not solved here), figure out how to combine your optim() code with what we learned about bootstrapping to compare the Wald-type CI from the Hessian with a bootstrap CI.",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#differential-equations-and-other-types-of-horoscopes",
    "href": "modules/ODEs-optim.html#differential-equations-and-other-types-of-horoscopes",
    "title": "Module XX: FP Applications",
    "section": "Differential equations and other types of horoscopes",
    "text": "Differential equations and other types of horoscopes\n\nA differential equation is an equation with a derivative in it. E.g: \\[\n\\frac{d^2x}{dt^2} + \\omega^2 x = 0\n\\] is a differential equation from physics.\nHere x is position, t is time, and \\(\\omega\\) is some constant.\nThis differential equation describes how springs and pendulums move over time.",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#spring-motion",
    "href": "modules/ODEs-optim.html#spring-motion",
    "title": "Module XX: FP Applications",
    "section": "Spring motion",
    "text": "Spring motion\n\nThis differential equation describes how a spring moves over time.\n\n\nknitr::include_graphics(here::here(\"images\", \"spring.jpg\"))\n\n\n\n\n\n\n\n\n\n\nThrough math magic, we can figure out that the curve traced out by the spring position has to be \\[\nf(x) = A\\cos(\\omega t) + B \\sin(\\omega t)\n\\]\nWhere \\(A\\) and \\(B\\) are real numbers that are determined by where the spring starts and its initial speed.\nSince we know this, we can make a plot of the spring movement.\n\n\n# Parameters\nomega &lt;- 4 * pi  # angular frequency = sqrt(k / m)\nA &lt;- -2          # initial displacement\nB &lt;- 1 / omega         # initial velocity divided by omega\n\n# Time vector\nt &lt;- seq(0, 2, by = 0.01)  # 0 to 2 seconds\n\n# Analytical solution\nx &lt;- A * cos(omega * t) + B * sin(omega * t)\n\n# Plot\nplot(t, x, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Time (seconds)\", ylab = \"Position\",\n     main = \"Spring movement\")\nabline(h = 0, col = \"gray\", lty = 2)\n\n\n\n\n\n\n\n\n\n\n\nSo what do we do if we don’t know the math magic?\nWe use a numerical solver. In R, the best one is in the deSolve package. These are tools that use good approximations to get the solution.\nThe only downside is that we have to write our code in a very specific way.\n\n\n# We have to do some transformations to make this DE work with deSolve\n# but you can ignore that for now or you can google \"transform a second order\n# DE into system of first order DEs\" if you want.\nlibrary(deSolve)\n\n# Function for our DE (system)\nspring_ode &lt;- function(t, state, parameters) {\n  with(as.list(c(state, parameters)), {\n    dx1 &lt;- x2\n    dx2 &lt;- -omega^2 * x1\n    \n    list(c(dx1, dx2))\n  })\n}\n\n# Initial state: x(0) = -2, dx/dt(0) = 1\nstate &lt;- c(x1 = -2, x2 = 1)\n\n# Time points to solve for\ntimes &lt;- seq(0, 2, by = 0.01)\n\n# Parameters list\nparameters &lt;- c(omega = omega)\n\n# Solve ODE\nout &lt;- ode(y = state, times = times, func = spring_ode, parms = parameters)\nstr(out)\n\n 'deSolve' num [1:201, 1:3] 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr [1:3] \"time\" \"x1\" \"x2\"\n - attr(*, \"istate\")= int [1:21] 2 222 449 NA 6 6 0 52 22 NA ...\n - attr(*, \"rstate\")= num [1:5] 0.01 0.01 2.01 0 0\n - attr(*, \"lengthvar\")= int 2\n - attr(*, \"type\")= chr \"lsoda\"\n\n\n\n\n\nWe can plot the solution to make sure it looks the same too.\n\n\n# Convert to data frame\nout_df &lt;- as.data.frame(out)\n\n# Plot displacement vs time\nplot(out_df$time, out_df$x1, type = \"l\", col = \"red\", lwd = 3,\n     xlab = \"Time (seconds)\", ylab = \"Position\",\n     main = \"Numerical Solution of spring movement\")\nlines(t, x, col = \"blue\", lty = 2, lwd = 2)\n\nabline(h = 0, col = \"gray\", lty = 2)\n\n\n\n\n\n\n\n\n\nThey are exactly the same.",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#ok-wait-a-second.",
    "href": "modules/ODEs-optim.html#ok-wait-a-second.",
    "title": "Module XX: FP Applications",
    "section": "OK, wait a second.",
    "text": "OK, wait a second.\n\nThis is obviously very exciting. But who cares about springs?\nIt turns out that for many biological/social processes, it’s easier to write down how things change than it is to write down exact formulas.\nAnd derivatives describe how things change.",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#compartment-models",
    "href": "modules/ODEs-optim.html#compartment-models",
    "title": "Module XX: FP Applications",
    "section": "Compartment models",
    "text": "Compartment models\n\nIf we divide up a population into states or compartments, usually we can write down how we expect people to move between those compartments.\nFor example, consider the famous SIR model.\n\n\nSource: DOI 10.3390/sym14122583\nYou don’t need any math magic to write down differential equations from this model, you just need some training on how to read it.\n\n\\[\n\\begin{aligned}\n\\frac{dS}{dt} &= -\\beta I S \\\\\n\\frac{dI}{dt} &= \\beta I S - \\gamma I \\\\\n\\frac{dR}{dt} &= \\gamma I\n\\end{aligned}\n\\]\n\nIt is a true fact about differential equations that most (systems/equations) are impossible to solve via math magic, and we must resort to numerical solutions.\nIt is impossible to solve the SIR system analytically with normal math magic, although it is possible using functions you’ve never heard of.",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#sir-model-in-desolve",
    "href": "modules/ODEs-optim.html#sir-model-in-desolve",
    "title": "Module XX: FP Applications",
    "section": "SIR model in deSolve",
    "text": "SIR model in deSolve\n\nLet’s walk through an example of solving SIR in deSolve.\n\n\n\nWe need the following things to give to DSAIDE: a set of time points to evaluate our system at, a list of parameters of the system, an initial condition/state that describes the component populations at time \\(0\\), and a function that uses all of those things to update the system.\n\n\n\n\nParameters: \\(\\beta\\) (the force of infection) and \\(\\gamma\\) (the recovery rate). I’ll arbitrarily set \\(\\beta = 0.01\\) and \\(\\gamma = 0.5\\) since I know it will make a good picture.\n\n\nsir_parms &lt;- c(beta = 0.01, gamma = 0.5)\n\n\n\n\nTime points: it doesn’t really matter what we choose as long as it goes far enough out to see the patterns we’re interest in and is fine-grained enough to get good detail. For this problem we’ll use times of \\(t = 0, 0.01, 0.02, \\ldots, 9.99, 10\\). Choosing the right ones usually takes some trial and error. Running more time points can take a really long time for complicated models.\n\n\nt_seq &lt;- seq(0, 10, 0.01)\n\n\n\n\nInitial state: we need how many susceptible, infected, and recovered people at time \\(0\\). Let’s say 1000 S, 1 I, and 0 R. Note you need at least one infected person to have an epidemic!\n\n\nsir_initial_condition &lt;- c(S = 1000, I = 1, R = 0)\n\n\n\n\nA function. This is the tricky bit, and we have to write it kind of weird. We can leave out that weird with() thing from before, but it does make your life easier so I recommend learning it.\n\n\nsir_ode &lt;- function(t, y, parameters) {\n    # If you don't use with() you have to do this\n    b &lt;- parameters[\"beta\"]\n    g &lt;- parameters[\"gamma\"]\n    \n    S &lt;- y[\"S\"]\n    I &lt;- y[\"I\"]\n    R &lt;- y[\"R\"]\n    \n    dS &lt;- -b * I * S\n    dI &lt;- b * I * S - g * I\n    dR &lt;- g * I\n    \n    # For deSolve the output always has to be a list like this\n    out &lt;- list(c(dS, dI, dR))\n    return(out)\n}\n\n\n\n\nNow we should be able to call deSolve and get a solution.\n\n\nsir_soln &lt;- deSolve::ode(\n    y = sir_initial_condition,\n    times = t_seq,\n    func = sir_ode,\n    parms = sir_parms\n)\nhead(sir_soln)\n\n\n\n\ntime\nS\nI\nR\n\n\n\n\n0.00\n1000.0000\n1.000000\n0.0000000\n\n\n0.01\n999.8951\n1.099654\n0.0052452\n\n\n0.02\n999.7798\n1.209224\n0.0110131\n\n\n0.03\n999.6529\n1.329698\n0.0173557\n\n\n0.04\n999.5135\n1.462153\n0.0243301\n\n\n0.05\n999.3602\n1.607779\n0.0319991\n\n\n\n\n\n\nLet’s plot the solution.\n\n\nplot(\n    NULL, NULL,\n    xlim = c(0, 10), ylim = c(0, 1000),\n    xlab = \"t\", ylab = \"n\"\n)\nmy_colors &lt;- c(\"deepskyblue2\", \"darkorange\", \"mediumseagreen\")\n\nlines(sir_soln[,\"time\"], sir_soln[,\"S\"], col = my_colors[1], lwd = 2, lty = 2)\nlines(sir_soln[,\"time\"], sir_soln[,\"I\"], col = my_colors[2], lwd = 2, lty = 2)\nlines(sir_soln[,\"time\"], sir_soln[,\"R\"], col = my_colors[3], lwd = 2, lty = 2)\nlegend(\n    \"topright\",\n    legend = c(\"S\", \"I\", \"R\"),\n    lty = 2, lwd = 2, col = my_colors\n)",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#you-try-it-1",
    "href": "modules/ODEs-optim.html#you-try-it-1",
    "title": "Module XX: FP Applications",
    "section": "You try it!",
    "text": "You try it!\n\nUpdate your model slightly so that people who are Recovered can become Susceptible again over time. This is called a SIRS model. It looks like this.\n\n\n\n\nSource: DOI 10.3390/sym14122583 with my edit\n\n\n\n\nHint: the system of equations implied by my incredible drawing is:\n\n\\[\n\\begin{aligned}\n\\frac{dS}{dt} &= -\\beta I S + \\zeta R\\\\\n\\frac{dI}{dt} &= \\beta I S - \\gamma I \\\\\n\\frac{dR}{dt} &= \\gamma I - \\zeta R\n\\end{aligned}\n\\]\n\n\n\nHint: you need to add another parameter and update the function, but you don’t need to change anything about the initial state or time values (if you don’t want to).\n\n\n\n\nSolution!\n\n\nsirs_initial_condition &lt;- sir_initial_condition\nsirs_t &lt;- seq(0, 1000, 0.01)\nsirs_parms &lt;- c(sir_parms, zeta = 0.25)\nsirs_ode &lt;- function(t, y, parameters) {\n    # If you don't use with() you have to do this\n    b &lt;- parameters[\"beta\"]\n    g &lt;- parameters[\"gamma\"]\n    z &lt;- parameters[\"zeta\"]\n    \n    S &lt;- y[\"S\"]\n    I &lt;- y[\"I\"]\n    R &lt;- y[\"R\"]\n    \n    dS &lt;- -b * I * S + z * R\n    dI &lt;- b * I * S - g * I\n    dR &lt;- g * I - z * R\n    \n    # For deSolve the output always has to be a list like this\n    out &lt;- list(c(dS, dI, dR))\n    return(out)\n}\n\nsirs_soln &lt;- deSolve::ode(\n    y = sirs_initial_condition,\n    times = sirs_t,\n    func = sirs_ode,\n    parms = sirs_parms\n)\nhead(sirs_soln)\n\n\n\n\ntime\nS\nI\nR\n\n\n\n\n0.00\n1000.0000\n1.000000\n0.0000000\n\n\n0.01\n999.8951\n1.099654\n0.0052388\n\n\n0.02\n999.7798\n1.209224\n0.0109865\n\n\n0.03\n999.6530\n1.329698\n0.0172938\n\n\n0.04\n999.5136\n1.462153\n0.0242165\n\n\n0.05\n999.3604\n1.607779\n0.0318156\n\n\n\n\n\n\nLet’s plot this solution the same way.\n\n\nplot(\n    NULL, NULL,\n    xlim = c(0, 10), ylim = c(0, 1000),\n    xlab = \"t\", ylab = \"n\",\n    main = \"SIRS model solution\"\n)\nmy_colors &lt;- c(\"deepskyblue2\", \"darkorange\", \"mediumseagreen\")\n\nlines(sirs_soln[,\"time\"], sirs_soln[,\"S\"], col = my_colors[1], lwd = 2, lty = 2)\nlines(sirs_soln[,\"time\"], sirs_soln[,\"I\"], col = my_colors[2], lwd = 2, lty = 2)\nlines(sirs_soln[,\"time\"], sirs_soln[,\"R\"], col = my_colors[3], lwd = 2, lty = 2)\nlegend(\n    \"topright\",\n    legend = c(\"S\", \"I\", \"R\"),\n    lty = 2, lwd = 2, col = my_colors\n)\n\n\n\n\n\n\n\n\n\nBy including the \\(\\zeta\\) parameter, we get an endemic equilibrium instead of a disease-free equilibrium.\nThis model can give you some other interesting oscillation patterns if you play with the parameters, but almost all parameter combinations will lead to a steady state.",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#you-try-it-problem-2",
    "href": "modules/ODEs-optim.html#you-try-it-problem-2",
    "title": "Module XX: FP Applications",
    "section": "You try it (problem 2)",
    "text": "You try it (problem 2)\n\nIf you’re ready for another challenge, try solving the equations for this model where the pathogen is environmental.\nThis model also includes a birth rate \\(n\\) (assuming all individuals are born into the susceptible compartment) and a death rate \\(m\\) that assumes people from the S, I, and R compartments all die at the same rate. If you want to, you can leave out any terms with \\(m\\) and \\(n\\) in them, they aren’t too important (or you can set them to 0 in your model).\n\n - Here are the equations implied by the model. \\[\n\\begin{aligned}\n\\frac{dS}{dt} &= n - b_{I}IS - b_{P}PS -mS \\\\\n\\frac{dI}{dt} &= b_{I}IS + b_{I}PS -gI - mI\\\\\n\\frac{dR}{dt} &= gI - mR \\\\\n\\frac{dP}{dt} &= qI - cP\n\\end{aligned}\n\\]\n\nNo solution typed out for this problem. You can work on it on your own and email me questions, or if we have time we can work on it together.",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#more-resources",
    "href": "modules/ODEs-optim.html#more-resources",
    "title": "Module XX: FP Applications",
    "section": "More resources",
    "text": "More resources\n\nDifferential equations are actually really fun. For a quick introduction (requires calculus) I recommend “Differential Equations in 24 Hours” by Scott Imhoff.\nFor a no-code no-calculus exploration of epidemiology DE’s, take a look at Andreas Handel’s R packages DSAIRM (within-host models) and DSAIDE (between-host models).\nOr there are MANY other SISMID courses about ODE modeling.",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/ODEs-optim.html#bonus-stage-desolve-and-optim",
    "href": "modules/ODEs-optim.html#bonus-stage-desolve-and-optim",
    "title": "Module XX: FP Applications",
    "section": "Bonus stage: deSolve AND optim",
    "text": "Bonus stage: deSolve AND optim\n\nIf you have observed epidemic data, you can actually estimate the parameters of an ODE system.\nIt’s much more difficult than estimating a linear model, though.\nSuppose we’ve observed the data in SIR-observed.csv and we want to estimate parameters assuming an SIR model fits the best. Note that we typically only observe case counts in real data, so that’s what is in this data.\nLet’s walk through a quick implementation of a model fit. First load the data.\n\n\ndat &lt;- read.csv(here::here(\"data\", \"SIR-observed.csv\"))\nhead(dat)\n\n\n\n\nday\ntotal_cases\n\n\n\n\n1\n1\n\n\n2\n5\n\n\n3\n22\n\n\n4\n44\n\n\n5\n109\n\n\n6\n271\n\n\n\n\n\n\nWe can use all the stuff we already built for an SIR model deSolve run. The hard part is figuring out how to fit it to our data.\n\n\n\nThe easiest way is to use the SSR on the case counts, so we’ll minimize \\[\nSSR = \\sum_{i = 1}^n \\bigg(I - \\hat{I}(\\beta, \\gamma)\\bigg)^2\n\\] where \\(\\hat{I}(\\beta, \\gamma)\\) are the case counts predicted by an SIR model with parameters \\(\\beta\\) and \\(\\gamma\\).\nSo we need to define a function for optim that takes the parameters as an argument, calculates the \\(\\hat{I}\\) values for those parameters, and then gets the SSR between the observed and predicted case counts.\n\n\nsir_pred &lt;- function(par) {\n    sir_parms &lt;- c(beta = par[1], gamma = par[2])\n    sir_initial_condition &lt;- c(S = 1000, I = 1, R = 0)\n    times &lt;- seq(1, 100, 1)\n    \n    sim &lt;- deSolve::ode(\n        y = sir_initial_condition,\n        times = times,\n        fun = sir_ode,\n        parms = sir_parms\n    )\n    \n    i_hat &lt;- sim[, \"I\"]\n    fit_ssr &lt;- sum((dat$total_cases - i_hat) ^ 2)\n    return(fit_ssr)\n}\n\n\nNow we can run optim().\n\n\nres &lt;- optim(\n    par = c(0.1, 0.1),\n    fn = sir_pred,\n    hessian = TRUE,\n    method = \"L-BFGS-B\",\n    lower = c(0, 0)\n)\n\npoint_sir &lt;- res$par\nse_sir &lt;- sqrt(diag(solve(res$hessian)))\nci_sir &lt;- point_sir + outer(se_sir, c(-1.96, 1.96))\n\nout &lt;- cbind(point_sir, ci_sir) |&gt; round(5)\nrownames(out) &lt;- c(\"beta\", \"gamma\")\ncolnames(out) &lt;- c(\"point\", \"lower\", \"upper\")\nout\n\n\n\n\n\npoint\nlower\nupper\n\n\n\n\nbeta\n3.62103\n3.47919\n3.76288\n\n\ngamma\n0.04126\n0.04124\n0.04128\n\n\n\n\n\n\nWe can see that we’re quite confidence in our estimates of \\(\\beta \\approx 3.6\\) and \\(\\gamma \\approx 0.04\\).\n\n\n\n\nThe problem with that is that those are nowhere close to the parameters I used to simulate the data (beta = 0.001, gamma = 0.05).\nThese differential equation models are hard to optimize! But there are other SISMID modules that focus on how to make these models work right, we just want you to be able to program them.\n\n\n\n\nPart of the problem is that this is very sensitive to the choice of optimizer. If we rewrite our function to implicitly have the 0 boundary constraint we can use unconstrained optimizers.\nSome are much worse for this problem (Nelder-Mead) and others (simulated annealing) can be better. In particular, simulated annealing is stochastic and will sometimes generate a very good answer and sometimes will not.\nGetting a handle on these kind of problems requires a lot of practice and involves a lot of stuff that we unfortunately don’t have time to talk about.\n\n\nsir_pred &lt;- function(par) {\n    sir_parms &lt;- c(beta = par[1], gamma = par[2])\n    sir_initial_condition &lt;- c(S = 1000, I = 1, R = 0)\n    times &lt;- seq(1, 100, 1)\n    \n    # New part\n    # Return a huge number if the paramaters are negative\n    if (any(par &lt;= 0)) return(1e12)\n    \n    sim &lt;- deSolve::ode(\n        y = sir_initial_condition,\n        times = times,\n        fun = sir_ode,\n        parms = sir_parms\n    )\n    \n    i_hat &lt;- sim[, \"I\"]\n    fit_ssr &lt;- sum((dat$total_cases - i_hat) ^ 2)\n    return(fit_ssr)\n}\n\nset.seed(103)\nres &lt;- optim(\n    par = c(0.1, 0.1),\n    fn = sir_pred,\n    hessian = TRUE,\n    # Use simulated annealing, which can be better for these kind of problems.\n    method = \"SANN\"\n)\n\npoint_sir &lt;- res$par\nse_sir &lt;- sqrt(diag(solve(res$hessian)))\nci_sir &lt;- point_sir + outer(se_sir, c(-1.96, 1.96))\n\nout &lt;- cbind(point_sir, ci_sir) |&gt; round(5)\nrownames(out) &lt;- c(\"beta\", \"gamma\")\ncolnames(out) &lt;- c(\"point\", \"lower\", \"upper\")\nout\n\n\n\n\n\npoint\nlower\nupper\n\n\n\n\nbeta\n0.00100\n0.00100\n0.00100\n\n\ngamma\n0.05441\n0.05438\n0.05445",
    "crumbs": [
      "Case studies / advanced topics",
      "Optim and ODEs"
    ]
  },
  {
    "objectID": "modules/Module05-FunctionalProgramming.html#learning-goals",
    "href": "modules/Module05-FunctionalProgramming.html#learning-goals",
    "title": "Module 5: FunctionalProgramming",
    "section": "Learning goals",
    "text": "Learning goals\n\nDefine functional programming\nUse functional programming tools in base R to repeat multiple functional calls.\nUse the purrr package for easily mapping and reducing functions.",
    "crumbs": [
      "Modules",
      "Functional programming"
    ]
  },
  {
    "objectID": "modules/Module05-FunctionalProgramming.html#what-is-functional-programming",
    "href": "modules/Module05-FunctionalProgramming.html#what-is-functional-programming",
    "title": "Module 5: FunctionalProgramming",
    "section": "What is functional programming?",
    "text": "What is functional programming?\n\n\nAll of our code is functional if it does its job?\n\n\n\n\nYes, but we are using the technical definition of functional programming: programming that is based on functions.\n\n\n\n\nSpecifically, we want to be able to compose functions, which you might remember hating in precalculus.\n\n\\[g \\circ f = g(f(x))\\]\n\nIn computer science, a functional is a function that accepts a function as an argument.",
    "crumbs": [
      "Modules",
      "Functional programming"
    ]
  },
  {
    "objectID": "modules/Module05-FunctionalProgramming.html#functional-vs.-imperative-programming",
    "href": "modules/Module05-FunctionalProgramming.html#functional-vs.-imperative-programming",
    "title": "Module 5: FunctionalProgramming",
    "section": "Functional vs. imperative programming",
    "text": "Functional vs. imperative programming\n\nSo far, we’ve been using imperative programming: we save variables, and we update them with new commands. Our code is structured as a list of instructions.\nIn functional programming, we write everything as a function, and we get the results we want by composing many functions.\n\n\n\nBut we’re just going to show you some useful parts of functional programming that you can include in your regular code.",
    "crumbs": [
      "Modules",
      "Functional programming"
    ]
  },
  {
    "objectID": "modules/Module05-FunctionalProgramming.html#ok-but-why",
    "href": "modules/Module05-FunctionalProgramming.html#ok-but-why",
    "title": "Module 5: FunctionalProgramming",
    "section": "Ok, but why?",
    "text": "Ok, but why?\n\nRecall our function from the previous Module.\n\n\nget_country_stats &lt;- function(df, iso3_code){\n    \n    country_data &lt;- subset(df, iso3c == iso3_code)\n    \n    # Get the summary statistics for this country\n    country_cases &lt;- country_data$measles_cases\n    country_quart &lt;- quantile(\n        country_cases, na.rm = TRUE, probs = c(0.25, 0.5, 0.75)\n    )\n    country_range &lt;- range(country_cases, na.rm = TRUE)\n    \n    country_name &lt;- unique(country_data$country)\n    \n    country_summary &lt;- data.frame(\n        country = country_name,\n        min = country_range[[1]],\n        Q1 = country_quart[[1]],\n        median = country_quart[[2]],\n        Q3 = country_quart[[3]],\n        max = country_range[[2]]\n    )\n    \n    return(country_summary)\n}\n\n\n\nWe could write a loop to get stats for many countries.\n\n\nmeas &lt;- readRDS(here::here(\"data\", \"measles_final.Rds\"))\ncountry_codes &lt;- c(\"IND\", \"PAK\", \"BGD\", \"NPL\")\n\n# Loop setup\nout &lt;- vector(mode = \"list\", length = length(country_codes))\nfor (i in 1:length(out)) {\n    out[[i]] &lt;- get_country_stats(meas, country_codes[[i]]) \n}\n\nout\n\n[[1]]\n  country  min      Q1  median       Q3    max\n1   India 3305 31135.5 47109.5 80797.25 252940\n\n[[2]]\n   country min     Q1 median    Q3   max\n1 Pakistan 386 2065.5 4075.5 17422 55543\n\n[[3]]\n     country min      Q1 median     Q3   max\n1 Bangladesh 203 2193.75 5270.5 9889.5 27327\n\n[[4]]\n  country min  Q1 median   Q3   max\n1   Nepal  59 190   1268 3100 13344\n\n\n\n\n\nBut if we use a functional programming tool called lapply, look how easy it is!\n\n\nout2 &lt;- lapply(country_codes, function(c) get_country_stats(meas, c))\nout2\n\n[[1]]\n  country  min      Q1  median       Q3    max\n1   India 3305 31135.5 47109.5 80797.25 252940\n\n[[2]]\n   country min     Q1 median    Q3   max\n1 Pakistan 386 2065.5 4075.5 17422 55543\n\n[[3]]\n     country min      Q1 median     Q3   max\n1 Bangladesh 203 2193.75 5270.5 9889.5 27327\n\n[[4]]\n  country min  Q1 median   Q3   max\n1   Nepal  59 190   1268 3100 13344\n\n\n\nFunctional programming techniques can help us avoid writing messy loops and clean up our code.",
    "crumbs": [
      "Modules",
      "Functional programming"
    ]
  },
  {
    "objectID": "modules/Module05-FunctionalProgramming.html#ok-but-why-1",
    "href": "modules/Module05-FunctionalProgramming.html#ok-but-why-1",
    "title": "Module 5: FunctionalProgramming",
    "section": "Ok, but why?",
    "text": "Ok, but why?\n\nR is a functional programming language at its core.\nIn R, functions are objects like everything else.\nYou never have to use FP, but it can help you write neater code.\nMore information: https://adv-r.hadley.nz/fp.html.",
    "crumbs": [
      "Modules",
      "Functional programming"
    ]
  },
  {
    "objectID": "modules/Module05-FunctionalProgramming.html#anonymous-functions",
    "href": "modules/Module05-FunctionalProgramming.html#anonymous-functions",
    "title": "Module 5: FunctionalProgramming",
    "section": "Anonymous functions",
    "text": "Anonymous functions\n\nIn this code, you can see the keyword function.\n\n\nout2 &lt;- lapply(country_codes, function(c) get_country_stats(meas, c))\n\n\n\nThe function function(c) get_country_stats(meas, c) is called an anonymous function (or sometimes a lambda).\nIt’s anonymous because it has no name assigned – this is just a shortcut for when you only need a function once.\nIn R, you can also write \\(c) ... as a shortcut for writing function(c) ... as an anonymous function.\n\n\n\n\nThis code works just as well, but can be a bit of a hassle.\n\n\nget_country_stats_from_meas &lt;- function(c) get_country_stats(meas, c)\nout3 &lt;- lapply(country_codes, get_country_stats_from_meas)",
    "crumbs": [
      "Modules",
      "Functional programming"
    ]
  },
  {
    "objectID": "modules/Module05-FunctionalProgramming.html#apply-and-friends",
    "href": "modules/Module05-FunctionalProgramming.html#apply-and-friends",
    "title": "Module 5: FunctionalProgramming",
    "section": "*apply() and friends",
    "text": "*apply() and friends\n\nR includes many functional programming tools collectively called *apply(). Here are the specific ones that are most useful:\nlapply(): repeat the function on a list of things.\nsapply(): same as lapply() but try to simplify the output to a matrix or vector.\napply(): repeat the function over the rows (margin = 1) or columns (margin = 2) of a matrix.\ntapply(): repeat the function over combinations of grouping factors.",
    "crumbs": [
      "Modules",
      "Functional programming"
    ]
  },
  {
    "objectID": "modules/Module05-FunctionalProgramming.html#a-tapply-example",
    "href": "modules/Module05-FunctionalProgramming.html#a-tapply-example",
    "title": "Module 5: FunctionalProgramming",
    "section": "A tapply() example",
    "text": "A tapply() example\n\ntapply(): repeat the function over combinations of grouping factors.\nWe want to get the average vaccine coverage for each country in the measles dataset – we need to separate this by the two vaccines as well.\nThis is pretty easy to do with tapply().\n\n\n\nmeas &lt;- readRDS(here::here(\"data\", \"measles_final.Rds\"))\nmeas_long &lt;- meas |&gt;\n    tidyr::pivot_longer(\n        dplyr::starts_with(\"MCV\"),\n        names_to = \"vaccine_antigen\",\n        values_to = \"vaccine_coverage\"\n    )\nout &lt;- tapply(\n    meas_long$vaccine_coverage,\n    list(meas_long$iso3c, meas_long$vaccine_antigen),\n    FUN = mean\n)\nhead(out)\n\n\n\n\n\nMCV1_coverage\nMCV2_coverage\n\n\n\n\nABW\nNA\nNA\n\n\nAFG\nNA\nNA\n\n\nAGO\nNA\nNA\n\n\nAIA\nNA\nNA\n\n\nALB\nNA\nNA\n\n\nAND\nNA\nNA\n\n\n\n\n\n\n\n\nUh oh.\n\n\n\n\nIf we want to handle the missing values, we can use an anonymous function.\n\n\nout &lt;- tapply(\n    meas_long$vaccine_coverage,\n    list(meas_long$iso3c, meas_long$vaccine_antigen),\n    FUN = \\(x) mean(x, na.rm = TRUE)\n)\nhead(out)\n\n\n\n\n\nMCV1_coverage\nMCV2_coverage\n\n\n\n\nABW\nNaN\nNaN\n\n\nAFG\n41.80952\n31.73684\n\n\nAGO\n47.17500\n22.75000\n\n\nAIA\nNaN\nNaN\n\n\nALB\n93.30233\n96.00000\n\n\nAND\n96.53846\n89.56250",
    "crumbs": [
      "Modules",
      "Functional programming"
    ]
  },
  {
    "objectID": "modules/Module05-FunctionalProgramming.html#you-try-it-with-lapply",
    "href": "modules/Module05-FunctionalProgramming.html#you-try-it-with-lapply",
    "title": "Module 5: FunctionalProgramming",
    "section": "You try it with lapply()!",
    "text": "You try it with lapply()!\n\nRead in all sheets of the QCRC_FINAL_Deidentified.xlsx data using a vector of sheet names and lapply.\n\n\n\nHint:\n\n\ndata_file &lt;- here::here(\"data\", \"QCRC_FINAL_Deidentified.xlsx\")\nsheet_names &lt;- readxl::excel_sheets(data_file)\nsheet_list &lt;- lapply(\n    sheet_names,\n    \\(name) ...\n)\n\n\n\n\ndata_file &lt;- here::here(\"data\", \"QCRC_FINAL_Deidentified.xlsx\")\nsheet_names &lt;- readxl::excel_sheets(data_file)\nsheet_list &lt;- lapply(\n    sheet_names,\n    \\(name) readxl::read_excel(data_file, sheet = name)\n)\n\nWarning: Expecting numeric in D6716 / R6716C4: got 'Date\\Time Correction'\n\n\nWarning: Expecting numeric in G7843 / R7843C7: got 'Date\\Time Correction'\n\n\nWarning: Expecting numeric in D7844 / R7844C4: got 'Date\\Time Correction'\n\n\nWarning: Expecting numeric in E7845 / R7845C5: got 'Date\\Time Correction'\n\n\nWarning: Expecting numeric in G10671 / R10671C7: got 'Date\\Time Correction'\n\n\nWarning: Expecting numeric in D10672 / R10672C4: got 'Date\\Time Correction'\n\n\nWarning: Expecting numeric in E10673 / R10673C5: got 'Date\\Time Correction'\n\n\nWarning: Expecting numeric in G11539 / R11539C7: got 'Date\\Time Correction'\n\n\nWarning: Expecting numeric in D11540 / R11540C4: got 'Date\\Time Correction'\n\n\nWarning: Expecting numeric in G17408 / R17408C7: got 'Date\\Time Correction'\n\n\nWarning: Expecting numeric in D17409 / R17409C4: got 'Date\\Time Correction'\n\n\nWarning: Expecting numeric in E17410 / R17410C5: got 'Date\\Time Correction'\n\nstr(sheet_list, 1)\n\nList of 10\n $ : tibble [288 × 37] (S3: tbl_df/tbl/data.frame)\n $ : tibble [42,606 × 6] (S3: tbl_df/tbl/data.frame)\n $ : tibble [288 × 8] (S3: tbl_df/tbl/data.frame)\n $ : tibble [15,919 × 29] (S3: tbl_df/tbl/data.frame)\n $ : tibble [143,220 × 11] (S3: tbl_df/tbl/data.frame)\n $ : tibble [18,281 × 11] (S3: tbl_df/tbl/data.frame)\n $ : tibble [5,252 × 5] (S3: tbl_df/tbl/data.frame)\n $ : tibble [869,554 × 7] (S3: tbl_df/tbl/data.frame)\n $ : tibble [288 × 71] (S3: tbl_df/tbl/data.frame)\n $ : tibble [3,612 × 36] (S3: tbl_df/tbl/data.frame)",
    "crumbs": [
      "Modules",
      "Functional programming"
    ]
  },
  {
    "objectID": "modules/Module05-FunctionalProgramming.html#an-apply-example",
    "href": "modules/Module05-FunctionalProgramming.html#an-apply-example",
    "title": "Module 5: FunctionalProgramming",
    "section": "An apply() example",
    "text": "An apply() example\n\nYou’ll use apply() less often because it’s specifically for matrices, where all data types have to be the same.\nFirst, make a matrix from the QCRC main dataset that only has the columns Died, 30D_Mortality, and 60D_Mortality.\n\n\nd_matrix &lt;- sheet_list[[1]] |&gt;\n    dplyr::select(Died, `30D_Mortality`, `60D_Mortality`) |&gt;\n    as.matrix()\n\nhead(d_matrix)\n\n\n\n\nDied\n30D_Mortality\n60D_Mortality\n\n\n\n\n1\n1\n1\n\n\n0\n0\n0\n\n\n0\n0\n0\n\n\n1\n1\n1\n\n\n1\n1\n1\n\n\n1\n1\n1\n\n\n\n\n\n\nNext use apply with the correct choice of MARGIN and FUN to get the proportion of cases that resulted in death at all three time points in one function call.\n\n\n\nHint:\n\n\napply(\n    d_matrix,\n    MARGIN = (1 or 2),\n    FUN = some_function\n)\n\n\nAnother hint: you can tell if you chose the correct MARGIN argument by counting the number of elements in the output. There should be 3, one for each column.\n\n\n\n\nSolution:\n\n\napply(\n    d_matrix,\n    MARGIN = 2,\n    FUN = mean\n)\n\n         Died 30D_Mortality 60D_Mortality \n    0.3402778     0.3263889     0.3333333",
    "crumbs": [
      "Modules",
      "Functional programming"
    ]
  },
  {
    "objectID": "modules/Module05-FunctionalProgramming.html#you-try-it",
    "href": "modules/Module05-FunctionalProgramming.html#you-try-it",
    "title": "Module 5: FunctionalProgramming",
    "section": "You try it!",
    "text": "You try it!\n\nLoad the measles dataset. Filter the dataset so you only have records from 2005, and only have complete cases.\nThen, use split to split the dataframe into a list by region.\nNow, use lapply() to fit a Poisson glm for each region separately that includes effects of MCV1 and MCV2 coverage.\nFind a way, using lapply() or sapply(), coef(), and do.call() to make a nice matrix of the coefficients.\n\n\n\nSolution for data processing\n\n\nmeas &lt;- readr::read_rds(here::here(\"data\", \"measles_final.Rds\"))\nmeas_2005 &lt;- meas |&gt;\n    dplyr::filter(year == 2005) |&gt;\n    tidyr::drop_na()\n\nmeas_regions &lt;- split(meas_2005, meas_2005$region)\n\n\n\n\nHint\n\n\nmodel_list &lt;- lapply(\n    meas_regions,\n    \\(d) glm(...)\n)\nmodel_coefs_list &lt;- lapply(model_list, ...)\nmodel_coefs_mat &lt;- do.call(...)\n\n\n\n\nSolution\n\n\nmodel_list &lt;- lapply(\n    meas_regions,\n    \\(d) glm(\n        measles_cases ~ MCV1_coverage + MCV2_coverage,\n        data = d, family = \"poisson\"\n    )\n)\nmodel_coefs_list &lt;- lapply(model_list, coef)\nmodel_coefs_mat &lt;- do.call(rbind, model_coefs_list)\n\nmodel_coefs_mat\n\n\n\n\n\n(Intercept)\nMCV1_coverage\nMCV2_coverage\n\n\n\n\nAfrica\n17.676171\n-0.2813935\n0.1419766\n\n\nAmericas\n7.278677\n-0.0790399\n0.0169828\n\n\nAsia\n11.417256\n-0.0440387\n0.0143116\n\n\nEurope\n-12.027052\n0.1838390\n0.0008544\n\n\nOceania\n-8.355161\n0.1176879\n-0.0257241",
    "crumbs": [
      "Modules",
      "Functional programming"
    ]
  },
  {
    "objectID": "modules/Module05-FunctionalProgramming.html#summary",
    "href": "modules/Module05-FunctionalProgramming.html#summary",
    "title": "Module 5: FunctionalProgramming",
    "section": "Summary",
    "text": "Summary\n\nFunctional programming tools like *apply() take a function as an input and use the same function multiple times.\nIn R, functions are objects like everything else and manipulating them like objects can help us write readable, fast code.\n\n\n\nSide note: purrr is a modern version of *apply() with a more consistent interface. It’s worth learning and covered in the Advanced R book, but most people still use *apply().\nThis book also covers more standard FP tools like reduction and filtering that we didn’t have time to talk about.",
    "crumbs": [
      "Modules",
      "Functional programming"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#learning-goals",
    "href": "modules/Module03-Iteration.html#learning-goals",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Learning goals",
    "text": "Learning goals\n\nReplace repetitive code with a for loop\nUse vectorization to replace unnecessary loops",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#repeating-code",
    "href": "modules/Module03-Iteration.html#repeating-code",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Repeating code",
    "text": "Repeating code\n\nWe often want to run the same code many times, replacing just one or a few parts.\n\n\ndata(\"penguins\", package = \"palmerpenguins\")\npenguins$bill_depth_mm[penguins$island == \"Biscoe\"] |&gt;\n        mean(na.rm = TRUE) |&gt;\n        round(digits = 2)\n\n[1] 15.87\n\npenguins$bill_depth_mm[penguins$island == \"Dream\"] |&gt;\n        mean(na.rm = TRUE) |&gt;\n        round(digits = 2)\n\n[1] 18.34\n\npenguins$bill_depth_mm[penguins$island == \"Torgersen\"] |&gt;\n        mean(na.rm = TRUE) |&gt;\n        round(digits = 2)\n\n[1] 18.43\n\n\n\nWe can do this ourselves if we only have a small list of changes. But imagine if there were 1000 islands instead!",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#what-is-iteration",
    "href": "modules/Module03-Iteration.html#what-is-iteration",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "What is iteration?",
    "text": "What is iteration?\n\nWhenever you repeat something, that’s iteration.\nIn R, this means running the same code multiple times in a row.\n\n\nfor (this_island in levels(penguins$island)) {\n    island_mean &lt;-\n        penguins$bill_depth_mm[penguins$island == this_island] |&gt;\n        mean(na.rm = TRUE) |&gt;\n        round(digits = 2)\n    \n    cat(paste(\"The mean bill depth on\", this_island, \"Island was\", island_mean,\n                            \"mm.\\n\"))\n}\n\nThe mean bill depth on Biscoe Island was 15.87 mm.\nThe mean bill depth on Dream Island was 18.34 mm.\nThe mean bill depth on Torgersen Island was 18.43 mm.",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#parts-of-a-loop",
    "href": "modules/Module03-Iteration.html#parts-of-a-loop",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Parts of a loop",
    "text": "Parts of a loop\n\nfor (this_island in levels(penguins$island)) {\n    island_mean &lt;-\n        penguins$bill_depth_mm[penguins$island == this_island] |&gt;\n        mean(na.rm = TRUE) |&gt;\n        round(digits = 2)\n    \n    cat(paste(\"The mean bill depth on\", this_island, \"Island was\", island_mean,\n                            \"mm.\\n\"))\n}\n\nThe header declares how many times we will repeat the same code. The header contains a control variable that changes in each repetition and a sequence of values for the control variable to take.",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#parts-of-a-loop-1",
    "href": "modules/Module03-Iteration.html#parts-of-a-loop-1",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Parts of a loop",
    "text": "Parts of a loop\n\nfor (this_island in levels(penguins$island)) {\n    island_mean &lt;-\n        penguins$bill_depth_mm[penguins$island == this_island] |&gt;\n        mean(na.rm = TRUE) |&gt;\n        round(digits = 2)\n    \n    cat(paste(\"The mean bill depth on\", this_island, \"Island was\", island_mean,\n                            \"mm.\\n\"))\n}\n\nThe body of the loop contains code that will be repeated a number of times based on the header instructions. In R, the body has to be surrounded by curly braces.",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#header-parts",
    "href": "modules/Module03-Iteration.html#header-parts",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Header parts",
    "text": "Header parts\n\nfor (this_island in levels(penguins$island)) {...}\n\n\nfor: keyword that declares we are doing a for loop.\n(...): parentheses after for declare the control variable and sequence.\nthis_island: the control variable.\nin: keyword that separates the control varibale and sequence.\nlevels(penguins$island): the sequence.\n{}: curly braces will contain the body code.",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#header-parts-1",
    "href": "modules/Module03-Iteration.html#header-parts-1",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Header parts",
    "text": "Header parts\n\nfor (this_island in levels(penguins$island)) {...}\n\n\nSince levels(penguins$island) evaluates to c(\"Biscoe\", \"Dream\", \"Torgersen\"), our loop will repeat 3 times.\n\n\n\n\nIteration\nthis_island\n\n\n\n\n1\n“Biscoe”\n\n\n2\n“Dream”\n\n\n3\n“Torgersen”\n\n\n\n\nEverything inside of {...} will be repeated three times.",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#loop-iteration-1",
    "href": "modules/Module03-Iteration.html#loop-iteration-1",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Loop iteration 1",
    "text": "Loop iteration 1\n\nisland_mean &lt;-\n    penguins$bill_depth_mm[penguins$island == \"Biscoe\"] |&gt;\n    mean(na.rm = TRUE) |&gt;\n    round(digits = 2)\n\ncat(paste(\"The mean bill depth on\", \"Biscoe\", \"Island was\", island_mean,\n                    \"mm.\\n\"))\n\nThe mean bill depth on Biscoe Island was 15.87 mm.",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#loop-iteration-2",
    "href": "modules/Module03-Iteration.html#loop-iteration-2",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Loop iteration 2",
    "text": "Loop iteration 2\n\nisland_mean &lt;-\n    penguins$bill_depth_mm[penguins$island == \"Dream\"] |&gt;\n    mean(na.rm = TRUE) |&gt;\n    round(digits = 2)\n\ncat(paste(\"The mean bill depth on\", \"Dream\", \"Island was\", island_mean,\n                    \"mm.\\n\"))\n\nThe mean bill depth on Dream Island was 18.34 mm.",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#loop-iteration-3",
    "href": "modules/Module03-Iteration.html#loop-iteration-3",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Loop iteration 3",
    "text": "Loop iteration 3\n\nisland_mean &lt;-\n    penguins$bill_depth_mm[penguins$island == \"Torgersen\"] |&gt;\n    mean(na.rm = TRUE) |&gt;\n    round(digits = 2)\n\ncat(paste(\"The mean bill depth on\", \"Torgersen\", \"Island was\", island_mean,\n                    \"mm.\\n\"))\n\nThe mean bill depth on Torgersen Island was 18.43 mm.",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#the-loop-structure-automates-this-process-for-us-so-we-dont-have-to-copy-and-paste-our-code",
    "href": "modules/Module03-Iteration.html#the-loop-structure-automates-this-process-for-us-so-we-dont-have-to-copy-and-paste-our-code",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "The loop structure automates this process for us so we don’t have to copy and paste our code!",
    "text": "The loop structure automates this process for us so we don’t have to copy and paste our code!\n\nfor (this_island in levels(penguins$island)) {\n    island_mean &lt;-\n        penguins$bill_depth_mm[penguins$island == this_island] |&gt;\n        mean(na.rm = TRUE) |&gt;\n        round(digits = 2)\n    \n    cat(paste(\"The mean bill depth on\", this_island, \"Island was\", island_mean,\n                            \"mm.\\n\"))\n}\n\nThe mean bill depth on Biscoe Island was 15.87 mm.\nThe mean bill depth on Dream Island was 18.34 mm.\nThe mean bill depth on Torgersen Island was 18.43 mm.",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#side-note-the-pipe-operator",
    "href": "modules/Module03-Iteration.html#side-note-the-pipe-operator",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Side note: the pipe operator |>",
    "text": "Side note: the pipe operator |&gt;\n\nThis operator allows us to chain commands together so the output of the previous statement is passed into the next statement.\nE.g. the code\n\n\nisland_mean &lt;-\n    penguins$bill_depth_mm[penguins$island == \"Torgersen\"] |&gt;\n    mean(na.rm = TRUE) |&gt;\n    round(digits = 2)\n\nwill be transformed by R into\n\nisland_mean &lt;-\n    round(\n        mean(\n            penguins$bill_depth_mm[penguins$island == \"Torgersen\"],\n            na.rm = TRUE\n        ),\n        digits = 2\n    )\n\nbefore it gets run. So using the pipe is a way to avoid deeply nested functions.\nNote that another alernative could be like this:\n\nisland_data &lt;- penguins$bill_depth_mm[penguins$island == \"Torgersen\"]\nisland_mean_raw &lt;- mean(island_data, na.rm = TRUE)\nisland_mean &lt;- round(island_mean_raw, digits = 2)\n\nSo using |&gt; can also help us to avoid a lot of assignments.\n\nWhichever style you prefer is fine! Some people like the pipe, some people like nesting, and some people like intermediate assignments. All three are perfectly fine as long as your code is neat and commented.\nIf you go on to the tidyverse class, you will use a lot of piping – it is a very popular coding style in R these days thanks to the inventors of the tidyverse packages.\nAlso note that you need R version 4.1.0 or better to use |&gt;. If you are on an older version of R, it will not be available.\n\nNow, back to loops!",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#remember-write-dry-code",
    "href": "modules/Module03-Iteration.html#remember-write-dry-code",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Remember: write DRY code!",
    "text": "Remember: write DRY code!\n\nDRY = “Don’t Repeat Yourself”\nInstead of copying and pasting, write loops and functions.\nEasier to debug and change in the future!\n\n\n\nOf course, we all copy and paste code sometimes. If you are running on a tight deadline or can’t get a loop or function to work, you might need to. DRY code is good, but working code is best!",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#you-try-it",
    "href": "modules/Module03-Iteration.html#you-try-it",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "You try it!",
    "text": "You try it!\nWrite a loop that goes from 1 to 10, squares each of the numbers, and prints the squared number.\n\n\nfor (i in 1:10) {\n    cat(i ^ 2, \"\\n\")\n}\n\n1 \n4 \n9 \n16 \n25 \n36 \n49 \n64 \n81 \n100",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#wait-did-we-need-to-do-that",
    "href": "modules/Module03-Iteration.html#wait-did-we-need-to-do-that",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Wait, did we need to do that?",
    "text": "Wait, did we need to do that?\n\nWell, yes, because you need to practice loops!\nBut technically no, because we can use vectorization.\nAlmost all basic operations in R are vectorized: they work on a vector of arguments all at the same time.",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#wait-did-we-need-to-do-that-1",
    "href": "modules/Module03-Iteration.html#wait-did-we-need-to-do-that-1",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Wait, did we need to do that?",
    "text": "Wait, did we need to do that?\n\nWell, yes, because you need to practice loops!\nBut technically no, because we can use vectorization.\nAlmost all basic operations in R are vectorized: they work on a vector of arguments all at the same time.\n(Technical note: actually, vectorized operations still do a loop. But they do the loop inside a compiled C program, which is much faster than R loops.)\n\n\n\n# No loop needed!\n(1:10)^2\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\n\n\n\n# Get the first 10 odd numbers, a common CS 101 loop problem on exams\n(1:20)[which((1:20 %% 2) == 1)]\n\n [1]  1  3  5  7  9 11 13 15 17 19\n\n\n\n\n\nSo you should really try vectorization first, then use loops only when you can’t use vectorization.",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#loop-walkthrough",
    "href": "modules/Module03-Iteration.html#loop-walkthrough",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Loop walkthrough",
    "text": "Loop walkthrough\n\nLet’s walk through a complex but useful example where we can’t use vectorization.\nLoad the cleaned measles dataset, and subset it so you only have MCV1 records.\n\n\n\nmeas &lt;- readRDS(here::here(\"data\", \"measles_final.Rds\"))\nstr(meas)\n\nspc_tbl_ [9,416 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ iso3c        : chr [1:9416] \"AFG\" \"AFG\" \"AFG\" \"AFG\" ...\n $ year         : num [1:9416] 2023 2022 2021 2020 2019 ...\n $ measles_cases: num [1:9416] 2792 5166 2900 640 353 ...\n $ population   : num [1:9416] 42239854 41128771 40099462 38972230 37769499 ...\n $ MCV1_coverage: num [1:9416] NA 68 63 66 64 71 67 64 62 60 ...\n $ MCV2_coverage: num [1:9416] NA 49 44 43 41 49 40 40 42 44 ...\n $ country      : chr [1:9416] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" ...\n $ region       : chr [1:9416] \"Asia\" \"Asia\" \"Asia\" \"Asia\" ...\n $ sub_region   : chr [1:9416] \"Southern Asia\" \"Southern Asia\" \"Southern Asia\" \"Southern Asia\" ...\n - attr(*, \"spec\")=List of 3\n  ..$ cols   :List of 6\n  .. ..$ iso3c        : list()\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"collector_character\" \"collector\"\n  .. ..$ year         : list()\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"collector_double\" \"collector\"\n  .. ..$ measles_cases: list()\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"collector_double\" \"collector\"\n  .. ..$ population   : list()\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"collector_double\" \"collector\"\n  .. ..$ MCV1_coverage: list()\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"collector_double\" \"collector\"\n  .. ..$ MCV2_coverage: list()\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"collector_double\" \"collector\"\n  ..$ default: list()\n  .. ..- attr(*, \"class\")= chr [1:2] \"collector_guess\" \"collector\"\n  ..$ delim  : chr \",\"\n  ..- attr(*, \"class\")= chr \"col_spec\"\n - attr(*, \"problems\")=&lt;externalptr&gt;",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#loop-walkthrough-1",
    "href": "modules/Module03-Iteration.html#loop-walkthrough-1",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Loop walkthrough",
    "text": "Loop walkthrough\n\nFirst, make an empty list. This is where we’ll store our results. Make it the same length as the number of countries in the dataset.\n\n\n\nres &lt;- vector(mode = \"list\", length = length(unique(meas$country)))\n\n\nThis is called preallocation and it can make your loops much faster.",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#loop-walkthrough-2",
    "href": "modules/Module03-Iteration.html#loop-walkthrough-2",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Loop walkthrough",
    "text": "Loop walkthrough\n\nLoop through every country in the dataset, and get the median, first and third quartiles, and range for each country. Store those summary statistics in a data frame.\nWhat should the header look like?\n\n\n\ncountries &lt;- unique(meas$country)\nfor (i in 1:length(countries)) {...}\n\n\n\n\nNote that we use the index as the control variable. When you need to do complex operations inside a loop, this is easier than the for-each construction we used earlier.",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#loop-walkthrough-3",
    "href": "modules/Module03-Iteration.html#loop-walkthrough-3",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Loop walkthrough",
    "text": "Loop walkthrough\n\nNow write out the body of the code. First we need to subset the data, to get only the data for the current country.\n\n\n\nfor (i in 1:length(countries)) {\n    # Get the data for the current country only\n    country_data &lt;- subset(meas, country == countries[i])\n}\n\n\n\n\nNext we need to get the summary of the cases for that country.\n\n\n\n\nfor (i in 1:length(countries)) {\n    # Get the data for the current country only\n    country_data &lt;- subset(meas, country == countries[i])\n    \n    # Get the summary statistics for this country\n    country_cases &lt;- country_data$measles_cases\n    country_quart &lt;- quantile(\n        country_cases, na.rm = TRUE, probs = c(0.25, 0.5, 0.75)\n    )\n    country_range &lt;- range(country_cases, na.rm = TRUE)\n}\n\n\n\n\nNext we save the summary statistics into a data frame.\n\n\nfor (i in 1:length(countries)) {\n    # Get the data for the current country only\n    country_data &lt;- subset(meas, country == countries[i])\n    \n    # Get the summary statistics for this country\n    country_cases &lt;- country_data$measles_cases\n    country_quart &lt;- quantile(\n        country_cases, na.rm = TRUE, probs = c(0.25, 0.5, 0.75)\n    )\n    country_range &lt;- range(country_cases, na.rm = TRUE)\n    \n    # Save the summary statistics into a data frame\n    country_summary &lt;- data.frame(\n        country = countries[[i]],\n        min = country_range[[1]],\n        Q1 = country_quart[[1]],\n        median = country_quart[[2]],\n        Q3 = country_quart[[3]],\n        max = country_range[[2]]\n    )\n}\n\n\n\n\nAnd finally, we save the data frame as the next element in our storage list.\n\n\nfor (i in 1:length(countries)) {\n    # Get the data for the current country only\n    country_data &lt;- subset(meas, country == countries[i])\n    \n    # Get the summary statistics for this country\n    country_cases &lt;- country_data$measles_cases\n    country_quart &lt;- quantile(\n        country_cases, na.rm = TRUE, probs = c(0.25, 0.5, 0.75)\n    )\n    country_range &lt;- range(country_cases, na.rm = TRUE)\n    \n    # Save the summary statistics into a data frame\n    country_summary &lt;- data.frame(\n        country = countries[[i]],\n        min = country_range[[1]],\n        Q1 = country_quart[[1]],\n        median = country_quart[[2]],\n        Q3 = country_quart[[3]],\n        max = country_range[[2]]\n    )\n    \n    # Save the results to our container\n    res[[i]] &lt;- country_summary\n}\n\nWarning in min(x): no non-missing arguments to min; returning Inf\n\n\nWarning in max(x): no non-missing arguments to max; returning -Inf\n\n\n\n\n\nLet’s take a look at the results.\n\n\nhead(res)\n\n[[1]]\n      country min   Q1 median     Q3   max\n1 Afghanistan 353 1158 2345.5 6190.5 32455\n\n[[2]]\n  country min Q1 median Q3    max\n1 Albania   0  1     12 29 136034\n\n[[3]]\n  country min  Q1 median   Q3   max\n1 Algeria   0 112   2686 8204 29584\n\n[[4]]\n         country min Q1 median  Q3 max\n1 American Samoa   0  0      0 2.5 498\n\n[[5]]\n  country min Q1 median Q3 max\n1 Andorra   0  0      0  0   5\n\n[[6]]\n  country min    Q1 median    Q3   max\n1  Angola  29 732.5 3389.5 15878 30067\n\n\n\nHow do we deal with this to get it into a nice form?\n\n\n\n\nWe can use a vectorization trick: the function do.call() seems like ancient computer science magic. And it is. But it will actually help us a lot.\n\n\nres_df &lt;- do.call(rbind, res)\nhead(res_df)\n\n\n\n\ncountry\nmin\nQ1\nmedian\nQ3\nmax\n\n\n\n\nAfghanistan\n353\n1158.0\n2345.5\n6190.5\n32455\n\n\nAlbania\n0\n1.0\n12.0\n29.0\n136034\n\n\nAlgeria\n0\n112.0\n2686.0\n8204.0\n29584\n\n\nAmerican Samoa\n0\n0.0\n0.0\n2.5\n498\n\n\nAndorra\n0\n0.0\n0.0\n0.0\n5\n\n\nAngola\n29\n732.5\n3389.5\n15878.0\n30067\n\n\n\n\n\n\nIt combined our data frames together! Let’s take a look at the rbind and do.call() help packages to see what happened.\n\n\n\n\n?rbind\n\nCombine R Objects by Rows or Columns\n\nDescription:\n\n     Take a sequence of vector, matrix or data-frame arguments and\n     combine by _c_olumns or _r_ows, respectively.  These are generic\n     functions with methods for other R classes.\n\nUsage:\n\n     cbind(..., deparse.level = 1)\n     rbind(..., deparse.level = 1)\n     ## S3 method for class 'data.frame'\n     rbind(..., deparse.level = 1, make.row.names = TRUE,\n           stringsAsFactors = FALSE, factor.exclude = TRUE)\n     \nArguments:\n\n     ...: (generalized) vectors or matrices.  These can be given as\n          named arguments.  Other R objects may be coerced as\n          appropriate, or S4 methods may be used: see sections\n          'Details' and 'Value'.  (For the '\"data.frame\"' method of\n          'cbind' these can be further arguments to 'data.frame' such\n          as 'stringsAsFactors'.)\n\ndeparse.level: integer controlling the construction of labels in the\n          case of non-matrix-like arguments (for the default method):\n          'deparse.level = 0' constructs no labels;\n          the default 'deparse.level = 1' typically and 'deparse.level\n          = 2' always construct labels from the argument names, see the\n          'Value' section below.\n\nmake.row.names: (only for data frame method:) logical indicating if\n          unique and valid 'row.names' should be constructed from the\n          arguments.\n\nstringsAsFactors: logical, passed to 'as.data.frame'; only has an\n          effect when the '...' arguments contain a (non-'data.frame')\n          'character'.\n\nfactor.exclude: if the data frames contain factors, the default 'TRUE'\n          ensures that 'NA' levels of factors are kept, see PR#17562\n          and the 'Data frame methods'.  In R versions up to 3.6.x,\n          'factor.exclude = NA' has been implicitly hardcoded (R &lt;=\n          3.6.0) or the default (R = 3.6.x, x &gt;= 1).\n\nDetails:\n\n     The functions 'cbind' and 'rbind' are S3 generic, with methods for\n     data frames.  The data frame method will be used if at least one\n     argument is a data frame and the rest are vectors or matrices.\n     There can be other methods; in particular, there is one for time\n     series objects.  See the section on 'Dispatch' for how the method\n     to be used is selected.  If some of the arguments are of an S4\n     class, i.e., 'isS4(.)' is true, S4 methods are sought also, and\n     the hidden 'cbind' / 'rbind' functions from package 'methods'\n     maybe called, which in turn build on 'cbind2' or 'rbind2',\n     respectively.  In that case, 'deparse.level' is obeyed, similarly\n     to the default method.\n\n     In the default method, all the vectors/matrices must be atomic\n     (see 'vector') or lists.  Expressions are not allowed.  Language\n     objects (such as formulae and calls) and pairlists will be coerced\n     to lists: other objects (such as names and external pointers) will\n     be included as elements in a list result.  Any classes the inputs\n     might have are discarded (in particular, factors are replaced by\n     their internal codes).\n\n     If there are several matrix arguments, they must all have the same\n     number of columns (or rows) and this will be the number of columns\n     (or rows) of the result.  If all the arguments are vectors, the\n     number of columns (rows) in the result is equal to the length of\n     the longest vector.  Values in shorter arguments are recycled to\n     achieve this length (with a 'warning' if they are recycled only\n     _fractionally_).\n\n     When the arguments consist of a mix of matrices and vectors the\n     number of columns (rows) of the result is determined by the number\n     of columns (rows) of the matrix arguments.  Any vectors have their\n     values recycled or subsetted to achieve this length.\n\n     For 'cbind' ('rbind'), vectors of zero length (including 'NULL')\n     are ignored unless the result would have zero rows (columns), for\n     S compatibility.  (Zero-extent matrices do not occur in S3 and are\n     not ignored in R.)\n\n     Matrices are restricted to less than 2^31 rows and columns even on\n     64-bit systems.  So input vectors have the same length\n     restriction: as from R 3.2.0 input matrices with more elements\n     (but meeting the row and column restrictions) are allowed.\n\nValue:\n\n     For the default method, a matrix combining the '...' arguments\n     column-wise or row-wise.  (Exception: if there are no inputs or\n     all the inputs are 'NULL', the value is 'NULL'.)\n\n     The type of a matrix result determined from the highest type of\n     any of the inputs in the hierarchy raw &lt; logical &lt; integer &lt;\n     double &lt; complex &lt; character &lt; list .\n\n     For 'cbind' ('rbind') the column (row) names are taken from the\n     'colnames' ('rownames') of the arguments if these are matrix-like.\n     Otherwise from the names of the arguments or where those are not\n     supplied and 'deparse.level &gt; 0', by deparsing the expressions\n     given, for 'deparse.level = 1' only if that gives a sensible name\n     (a 'symbol', see 'is.symbol').\n\n     For 'cbind' row names are taken from the first argument with\n     appropriate names: rownames for a matrix, or names for a vector of\n     length the number of rows of the result.\n\n     For 'rbind' column names are taken from the first argument with\n     appropriate names: colnames for a matrix, or names for a vector of\n     length the number of columns of the result.\n\nData frame methods:\n\n     The 'cbind' data frame method is just a wrapper for\n     'data.frame(..., check.names = FALSE)'.  This means that it will\n     split matrix columns in data frame arguments.\n\n     The 'rbind' data frame method first drops all zero-column and\n     zero-row arguments.  (If that leaves none, it returns the first\n     argument with columns otherwise a zero-column zero-row data\n     frame.)  It then takes the classes of the columns from the first\n     data frame, and matches columns by name (rather than by position).\n     Factors have their levels expanded as necessary (in the order of\n     the levels of the level sets of the factors encountered) and the\n     result is an ordered factor if and only if all the components were\n     ordered factors.  Old-style categories (integer vectors with\n     levels) are promoted to factors.\n\n     Note that for result column 'j', 'factor(., exclude = X(j))' is\n     applied, where\n\n       X(j) := if(isTRUE(factor.exclude)) {\n                  if(!NA.lev[j]) NA # else NULL\n               } else factor.exclude\n     \n     where 'NA.lev[j]' is true iff any contributing data frame has had\n     a 'factor' in column 'j' with an explicit 'NA' level.\n\nDispatch:\n\n     The method dispatching is _not_ done via 'UseMethod()', but by\n     C-internal dispatching.  Therefore there is no need for, e.g.,\n     'rbind.default'.\n\n     The dispatch algorithm is described in the source file\n     ('.../src/main/bind.c') as\n\n       1. For each argument we get the list of possible class\n          memberships from the class attribute.\n\n       2. We inspect each class in turn to see if there is an\n          applicable method.\n\n       3. If we find a method, we use it.  Otherwise, if there was an\n          S4 object among the arguments, we try S4 dispatch; otherwise,\n          we use the default code.\n\n     If you want to combine other objects with data frames, it may be\n     necessary to coerce them to data frames first.  (Note that this\n     algorithm can result in calling the data frame method if all the\n     arguments are either data frames or vectors, and this will result\n     in the coercion of character vectors to factors.)\n\nReferences:\n\n     Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) _The New S\n     Language_.  Wadsworth & Brooks/Cole.\n\nSee Also:\n\n     'c' to combine vectors (and lists) as vectors, 'data.frame' to\n     combine vectors and matrices as a data frame.\n\nExamples:\n\n     m &lt;- cbind(1, 1:7) # the '1' (= shorter vector) is recycled\n     m\n     m &lt;- cbind(m, 8:14)[, c(1, 3, 2)] # insert a column\n     m\n     cbind(1:7, diag(3)) # vector is subset -&gt; warning\n     \n     cbind(0, rbind(1, 1:3))\n     cbind(I = 0, X = rbind(a = 1, b = 1:3))  # use some names\n     xx &lt;- data.frame(I = rep(0,2))\n     cbind(xx, X = rbind(a = 1, b = 1:3))   # named differently\n     \n     cbind(0, matrix(1, nrow = 0, ncol = 4)) #&gt; Warning (making sense)\n     dim(cbind(0, matrix(1, nrow = 2, ncol = 0))) #-&gt; 2 x 1\n     \n     ## deparse.level\n     dd &lt;- 10\n     rbind(1:4, c = 2, \"a++\" = 10, dd, deparse.level = 0) # middle 2 rownames\n     rbind(1:4, c = 2, \"a++\" = 10, dd, deparse.level = 1) # 3 rownames (default)\n     rbind(1:4, c = 2, \"a++\" = 10, dd, deparse.level = 2) # 4 rownames\n     \n     ## cheap row names:\n     b0 &lt;- gl(3,4, labels=letters[1:3])\n     bf &lt;- setNames(b0, paste0(\"o\", seq_along(b0)))\n     df  &lt;- data.frame(a = 1, B = b0, f = gl(4,3))\n     df. &lt;- data.frame(a = 1, B = bf, f = gl(4,3))\n     new &lt;- data.frame(a = 8, B =\"B\", f = \"1\")\n     (df1  &lt;- rbind(df , new))\n     (df.1 &lt;- rbind(df., new))\n     stopifnot(identical(df1, rbind(df,  new, make.row.names=FALSE)),\n               identical(df1, rbind(df., new, make.row.names=FALSE)))\n\n\n\n\n\n?do.call\n\nExecute a Function Call\n\nDescription:\n\n     'do.call' constructs and executes a function call from a name or a\n     function and a list of arguments to be passed to it.\n\nUsage:\n\n     do.call(what, args, quote = FALSE, envir = parent.frame())\n     \nArguments:\n\n    what: either a function or a non-empty character string naming the\n          function to be called.\n\n    args: a _list_ of arguments to the function call.  The 'names'\n          attribute of 'args' gives the argument names.\n\n   quote: a logical value indicating whether to quote the arguments.\n\n   envir: an environment within which to evaluate the call.  This will\n          be most useful if 'what' is a character string and the\n          arguments are symbols or quoted expressions.\n\nDetails:\n\n     If 'quote' is 'FALSE', the default, then the arguments are\n     evaluated (in the calling environment, not in 'envir').  If\n     'quote' is 'TRUE' then each argument is quoted (see 'quote') so\n     that the effect of argument evaluation is to remove the quotes -\n     leaving the original arguments unevaluated when the call is\n     constructed.\n\n     The behavior of some functions, such as 'substitute', will not be\n     the same for functions evaluated using 'do.call' as if they were\n     evaluated from the interpreter.  The precise semantics are\n     currently undefined and subject to change.\n\nValue:\n\n     The result of the (evaluated) function call.\n\nWarning:\n\n     This should not be used to attempt to evade restrictions on the\n     use of '.Internal' and other non-API calls.\n\nReferences:\n\n     Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) _The New S\n     Language_.  Wadsworth & Brooks/Cole.\n\nSee Also:\n\n     'call' which creates an unevaluated call.\n\nExamples:\n\n     do.call(\"complex\", list(imaginary = 1:3))\n     \n     ## if we already have a list (e.g., a data frame)\n     ## we need c() to add further arguments\n     tmp &lt;- expand.grid(letters[1:2], 1:3, c(\"+\", \"-\"))\n     do.call(\"paste\", c(tmp, sep = \"\"))\n     \n     do.call(paste, list(as.name(\"A\"), as.name(\"B\")), quote = TRUE)\n     \n     ## examples of where objects will be found.\n     A &lt;- 2\n     f &lt;- function(x) print(x^2)\n     env &lt;- new.env()\n     assign(\"A\", 10, envir = env)\n     assign(\"f\", f, envir = env)\n     f &lt;- function(x) print(x)\n     f(A)                                      # 2\n     do.call(\"f\", list(A))                     # 2\n     do.call(\"f\", list(A), envir = env)        # 4\n     do.call( f,  list(A), envir = env)        # 2\n     do.call(\"f\", list(quote(A)), envir = env) # 100\n     do.call( f,  list(quote(A)), envir = env) # 10\n     do.call(\"f\", list(as.name(\"A\")), envir = env) # 100\n     \n     eval(call(\"f\", A))                      # 2\n     eval(call(\"f\", quote(A)))               # 2\n     eval(call(\"f\", A), envir = env)         # 4\n     eval(call(\"f\", quote(A)), envir = env)  # 100\n\n\n\n\n\nOK, so basically what happened is that\n\n\ndo.call(rbind, list)\n\n\nGets transformed into\n\n\nrbind(list[[1]], list[[2]], list[[3]], ..., list[[length(list)]])\n\n\nThat’s vectorization magic!",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#you-try-it-if-we-have-time",
    "href": "modules/Module03-Iteration.html#you-try-it-if-we-have-time",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "You try it! (if we have time)",
    "text": "You try it! (if we have time)\n\nUse the code you wrote before the get the incidence per 1000 people on the entire measles data set (add a column for incidence to the full data).\nUse the code plot(NULL, NULL, ...) to make a blank plot. You will need to set the xlim and ylim arguments to sensible values, and specify the axis titles as “Year” and “Incidence per 1000 people”.\nUsing a for loop and the lines() function, make a plot that shows all of the incidence curves over time, overlapping on the plot.\nHINT: use col = adjustcolor(black, alpha.f = 0.25) to make the curves partially transparent, so you can see the overlap.\nBONUS PROBLEM: using the function cumsum(), make a plot of the cumulative cases (not standardized) over time for all of the countries. (Dealing with the NA’s here is tricky!!)",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#main-problem-hint",
    "href": "modules/Module03-Iteration.html#main-problem-hint",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Main problem hint",
    "text": "Main problem hint\n\nmeas$cases_per_thousand &lt;- meas$measles_cases / meas$population * 1000\ncountries &lt;- unique(meas$country)\n\nplot(\n    NULL, NULL,\n    xlim = c(1980, 2022),\n    ylim = c(0, 50),\n    xlab = \"Year\",\n    ylab = \"Incidence per 1000 people\"\n)\n\nfor (...) {\n    ...\n}",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#main-problem-solution",
    "href": "modules/Module03-Iteration.html#main-problem-solution",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Main problem solution",
    "text": "Main problem solution\n\nmeas$cases_per_thousand &lt;- meas$measles_cases / meas$population * 1000\ncountries &lt;- unique(meas$country)\n\nplot(\n    NULL, NULL,\n    xlim = c(1980, 2022),\n    ylim = c(0, 50),\n    xlab = \"Year\",\n    ylab = \"Incidence per 1000 people\"\n)\n\nfor (i in 1:length(countries)) {\n    country_data &lt;- subset(meas, country == countries[[i]])\n    lines(\n        x = country_data$year,\n        y = country_data$cases_per_thousand,\n        col = adjustcolor(\"black\", alpha.f = 0.25)\n    )\n}",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#main-problem-solution-1",
    "href": "modules/Module03-Iteration.html#main-problem-solution-1",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Main problem solution",
    "text": "Main problem solution",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#bonus-problem-solution",
    "href": "modules/Module03-Iteration.html#bonus-problem-solution",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Bonus problem solution",
    "text": "Bonus problem solution\n\n# First calculate the cumulative cases, treating NA as zeroes\nmeas2 &lt;- dplyr::arrange(meas, iso3c, year)\nmeas2$cumulative_cases &lt;- ave(\n    x = ifelse(is.na(meas2$measles_cases), 0, meas2$measles_cases),\n    meas2$country,\n    FUN = cumsum\n)\n\nplot(\n    NULL, NULL,\n    xlim = c(1980, 2022),\n    ylim = c(1, 7.3e6),\n    xlab = \"Year\",\n    ylab = paste0(\"Cumulative cases since\", min(meas2$year))\n)\n\nfor (i in 1:length(countries)) {\n    country_data &lt;- subset(meas2, country == countries[[i]])\n    lines(\n        x = country_data$year,\n        y = country_data$cumulative_cases,\n        col = adjustcolor(\"black\", alpha.f = 0.25)\n    )\n}",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module03-Iteration.html#bonus-problem-solution-1",
    "href": "modules/Module03-Iteration.html#bonus-problem-solution-1",
    "title": "Module 3: Iteration and vectorization in R",
    "section": "Bonus problem solution",
    "text": "Bonus problem solution",
    "crumbs": [
      "Modules",
      "Iteration"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#learning-objectives",
    "href": "modules/Module01-Quarto.html#learning-objectives",
    "title": "Intro To Quarto",
    "section": "Learning objectives:",
    "text": "Learning objectives:\n\nUnderstand the basic components of a Quarto document.",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#introduction",
    "href": "modules/Module01-Quarto.html#introduction",
    "title": "Intro To Quarto",
    "section": "Introduction",
    "text": "Introduction\n\nQuarto is a command line interface tool, not an R package.\nQuarto unifies the functionality of many packages from the R Markdown ecosystem: rmarkdown, bookdown, distill, xaringan, etc. into a single consistent system.\nQuarto -&gt; native support for multiple programming languages like Python and Julia in addition to R",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#where-to-find-help",
    "href": "modules/Module01-Quarto.html#where-to-find-help",
    "title": "Intro To Quarto",
    "section": "Where to find help",
    "text": "Where to find help\n\nDocumentation",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#why-quarto-documents",
    "href": "modules/Module01-Quarto.html#why-quarto-documents",
    "title": "Intro To Quarto",
    "section": "Why Quarto documents:",
    "text": "Why Quarto documents:\n\nreproducible\nsupport dozens of output formats: PDFs, Word files, presentations, and more.",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#main-uses",
    "href": "modules/Module01-Quarto.html#main-uses",
    "title": "Intro To Quarto",
    "section": "3 main uses:",
    "text": "3 main uses:\n\nCommunication: focus on conclusions not code.\nCollaborating with other scientists (including future you!): conclusions and code.\nEnvironment in which to do data science.",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#quarto-basics",
    "href": "modules/Module01-Quarto.html#quarto-basics",
    "title": "Intro To Quarto",
    "section": "Quarto basics",
    "text": "Quarto basics\n\nQuarto files have .qmd extension.\nContains 3 types of contents:\n\nand optional YAML header surrounded by 3 dashes (—) at the beginning and end\nchunks of R code surrounded by 3 back ticks (```)\ntext mixed with simple formatting like #heading or italics",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#getting-started",
    "href": "modules/Module01-Quarto.html#getting-started",
    "title": "Intro To Quarto",
    "section": "Getting Started",
    "text": "Getting Started\n\nTo get started with your own .qmd file, select File &gt; New File &gt; Quarto Document… in the menu bar.",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#run-code-in-quarto",
    "href": "modules/Module01-Quarto.html#run-code-in-quarto",
    "title": "Intro To Quarto",
    "section": "Run code in quarto",
    "text": "Run code in quarto\n\nRun each code chunk by clicking the Run icon (each chunk will have this green arrow).\nYou can choose to have the plots and output displayed in the document or on RStudio’s console and plot panes. Go to the gear icon next to “Render” and switch to “Chunk Output Console”.\nTo run the complete report, click “Render” and your report will be displayed in the viewer pane as an HTML file (unless the YAML includes .pdf or other extension).",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#visual-editor",
    "href": "modules/Module01-Quarto.html#visual-editor",
    "title": "Intro To Quarto",
    "section": "Visual editor",
    "text": "Visual editor\n\n\n\nVisual editor -&gt; use the buttons on the menu bar to insert images, tables, cross-references, etc. or you can use the catch-all ⌘ + / or Ctrl + / shortcut to insert just about anything.\nThe visual editor displays your content with formatting, but under the hood, it saves your content in plain Markdown and you can switch back and forth between the visual and source editors.",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#source-editor",
    "href": "modules/Module01-Quarto.html#source-editor",
    "title": "Intro To Quarto",
    "section": "Source editor",
    "text": "Source editor\n\n\n\nThe Source editor will feel familiar to those with experience writing R scripts or R Markdown documents.\nCan also be useful for debugging any Quarto syntax errors since it’s often easier to catch these in plain text.\nIf you forget, you can get to a handy reference sheet with Help &gt; Markdown Quick Reference.\n\n\n\n\n\n## Text formatting\n\n*italic* **bold** ~~strikeout~~ `code`\n\nsuperscript^2^ subscript~2~\n\n[underline]{.underline} [small caps]{.smallcaps}\n\n## Headings\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\n## Lists\n\n-   Bulleted list item 1\n\n-   Item 2\n\n    -   Item 2a\n\n    -   Item 2b\n\n1.  Numbered list item 1\n\n2.  Item 2.\n    The numbers are incremented automatically in the output.\n\n## Links and images\n\n&lt;http://example.com&gt;\n\n[linked phrase](http://example.com)\n\n![optional caption text](quarto.png){fig-alt=\"Quarto logo and the word quarto spelled in small case letters\"}\n\n## Tables\n\n| First Header | Second Header |\n|--------------|---------------|\n| Content Cell | Content Cell  |\n| Content Cell | Content Cell  |",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#code-chunks",
    "href": "modules/Module01-Quarto.html#code-chunks",
    "title": "Intro To Quarto",
    "section": "Code chunks",
    "text": "Code chunks\n\nTo run code inside a Quarto document, you need to insert a chunk.\n\nThe keyboard shortcut Cmd + Option + I / Ctrl + Alt + I.\nThe “Insert” button icon in the editor toolbar.",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#chunk-label",
    "href": "modules/Module01-Quarto.html#chunk-label",
    "title": "Intro To Quarto",
    "section": "Chunk label",
    "text": "Chunk label\nChunks can be given an optional label, e.g.",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#this-has-three-advantages",
    "href": "modules/Module01-Quarto.html#this-has-three-advantages",
    "title": "Intro To Quarto",
    "section": "This has three advantages:",
    "text": "This has three advantages:\n\nNavigate to specific chunks using the drop-down code navigator in the bottom-left of the script editor:\n\n\n\nGraphics produced by the chunks will have useful names that make them easier to use elsewhere.\nYou can set up networks of cached chunks to avoid re-performing expensive computations on every run.",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#important",
    "href": "modules/Module01-Quarto.html#important",
    "title": "Intro To Quarto",
    "section": "Important!",
    "text": "Important!\n\nYour chunk labels should be short but evocative and should not contain spaces.\nWe recommend using dashes (-) to separate words (instead of underscores, _) and no other special characters in chunk labels.\nUse whatever name, except: setup, which is used for a specific reason.\nAdditionally, chunk labels cannot be duplicated.\nEach chunk label must be unique.",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#chunk-options",
    "href": "modules/Module01-Quarto.html#chunk-options",
    "title": "Intro To Quarto",
    "section": "Chunk options",
    "text": "Chunk options\n\nChunk output can be customized with options.\nYou can see the full list at https://yihui.org/knitr/options.\nEach of these chunk options get added to the header of the chunk, following #|.",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#the-main-options-are",
    "href": "modules/Module01-Quarto.html#the-main-options-are",
    "title": "Intro To Quarto",
    "section": "The main options are:",
    "text": "The main options are:\n\neval: false prevents code from being evaluated. And obviously if the code is not run, no results will be generated.\necho: false prevents code, but not the results from appearing in the finished file.\nmessage: false or warning: false prevents messages or warnings from appearing in the finished file.\nerror: true causes the render to continue even if code returns an error. This is rarely something you’ll want to include in the final version of your report, but can be very useful to debug. The default, error: false causes rendering to fail if there is a single error in the document.",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#inline-code",
    "href": "modules/Module01-Quarto.html#inline-code",
    "title": "Intro To Quarto",
    "section": "Inline code",
    "text": "Inline code\n\nThere is one other way to embed R code into a Quarto document: directly into the text, with r inside back ticks.\n\nFor example, you can inline code include in between text and that will show a result.\n\n\n\nThe data frame iris has 150 rows.",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#figures",
    "href": "modules/Module01-Quarto.html#figures",
    "title": "Intro To Quarto",
    "section": "Figures",
    "text": "Figures\n\nThe figures in a Quarto document can be embedded (e.g., a PNG or JPEG file) or generated as a result of a code chunk.\nIt’s best if plots have consistent width. To enforce this, set fig-width: 6 (6”) and fig-asp: 0.618 (the golden ratio) in the defaults. Then in individual chunks, only adjust fig-asp.\nControl the output size with out-width and set it to a percentage of the body width of the output document. We suggest to out-width: “70%” and fig-align: center.",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#figures-1",
    "href": "modules/Module01-Quarto.html#figures-1",
    "title": "Intro To Quarto",
    "section": "Figures",
    "text": "Figures\n\nTo put multiple plots in a single row, set the layout-ncol to 2 for two plots, 3 for three plots, etc. This effectively sets out-width to “50%” for each of your plots if layout-ncol is 2,\nGreat blog post by Thomas Lin Pedersen about controling plot scaling.",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#tables",
    "href": "modules/Module01-Quarto.html#tables",
    "title": "Intro To Quarto",
    "section": "Tables",
    "text": "Tables\n\nYou can include two types of tables in a Quarto document:\n\nmarkdown tables that you create directly in your Quarto document, or\ntables generated as a result of a code chunk.\n\n\nMore on Tables Soon :)",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Module01-Quarto.html#yaml-header",
    "href": "modules/Module01-Quarto.html#yaml-header",
    "title": "Intro To Quarto",
    "section": "YAML header",
    "text": "YAML header\n\nYou can control many other “whole document” settings by tweaking the parameters of the YAML header. You might wonder what YAML stands for: it’s “YAML Ain’t Markup Language”\nBe careful with the YAML!",
    "crumbs": [
      "Modules",
      "Quarto"
    ]
  },
  {
    "objectID": "modules/Arrow.html#learning-objectives",
    "href": "modules/Arrow.html#learning-objectives",
    "title": "Introduction To Arrow",
    "section": "Learning objectives:",
    "text": "Learning objectives:\n\nUse {arrow} to load large data files into R efficiently\nPartition large data files into parquet files for quicker access, less memory usage, and quicker wrangling\nWrangle {arrow} data using existing {dplyr} operations",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#why-learn-arrow",
    "href": "modules/Arrow.html#why-learn-arrow",
    "title": "Introduction To Arrow",
    "section": "Why learn {arrow}?",
    "text": "Why learn {arrow}?\n\nCSV files = very common for ease of access and use\nBig/messy CSVs = slow\n{arrow} 📦 reads large datasets quickly & uses {dplyr} syntax",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#packages-used",
    "href": "modules/Arrow.html#packages-used",
    "title": "Introduction To Arrow",
    "section": "Packages used",
    "text": "Packages used\n\nlibrary(arrow)\nlibrary(curl)\nlibrary(tidyverse)",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#download-data",
    "href": "modules/Arrow.html#download-data",
    "title": "Introduction To Arrow",
    "section": "Download data",
    "text": "Download data\n\nCase study: item checkouts dataset from Seattle libraries\nDON’T download.file()!!! (41,389,465 rows of data)\n\n\n\"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv\",}\n  \"data/seattle-library-checkouts.csv\",\n  resume = TRUE\n)",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#open-the-data",
    "href": "modules/Arrow.html#open-the-data",
    "title": "Introduction To Arrow",
    "section": "Open the data",
    "text": "Open the data\n\nSize in memory ≈ 2 × size on disk\nread_csv() ➡️ arrow::open_dataset()\nScans a few thousand rows to determine dataset structure\n\nISBN is empty for 80k rows, so we specify\n\nDoes NOT load entire dataset into memory\n\n\nseattle_csv &lt;- open_dataset(\n  sources = \"data/seattle-library-checkouts.csv\", \n  col_types = schema(ISBN = string()),\n  format = \"csv\"\n)",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#glimpse-the-data",
    "href": "modules/Arrow.html#glimpse-the-data",
    "title": "Introduction To Arrow",
    "section": "Glimpse the data",
    "text": "Glimpse the data\n\nseattle_csv |&gt; glimpse()\n#&gt; FileSystemDataset with 1 csv file\n#&gt; 41,389,465 rows x 12 columns\n#&gt; $ UsageClass      &lt;string&gt; \"Physical\", \"Physical\", \"Digital\", \"Physical\", \"Ph…\n#&gt; $ CheckoutType    &lt;string&gt; \"Horizon\", \"Horizon\", \"OverDrive\", \"Horizon\", \"Hor…\n#&gt; $ MaterialType    &lt;string&gt; \"BOOK\", \"BOOK\", \"EBOOK\", \"BOOK\", \"SOUNDDISC\", \"BOO…\n#&gt; $ CheckoutYear     &lt;int64&gt; 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 20…\n#&gt; $ CheckoutMonth    &lt;int64&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n#&gt; $ Checkouts        &lt;int64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 3, 2, 1, 3, 2,…\n#&gt; $ Title           &lt;string&gt; \"Super rich : a guide to having it all / Russell S…\n#&gt; $ ISBN            &lt;string&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#&gt; $ Creator         &lt;string&gt; \"Simmons, Russell\", \"Barclay, James, 1965-\", \"Tim …\n#&gt; $ Subjects        &lt;string&gt; \"Self realization, Conduct of life, Attitude Psych…\n#&gt; $ Publisher       &lt;string&gt; \"Gotham Books,\", \"Pyr,\", \"Random House, Inc.\", \"Di…\n#&gt; $ PublicationYear &lt;string&gt; \"c2011.\", \"2010.\", \"2015\", \"2005.\", \"c2004.\", \"c20…",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#manipulate-the-data",
    "href": "modules/Arrow.html#manipulate-the-data",
    "title": "Introduction To Arrow",
    "section": "Manipulate the data",
    "text": "Manipulate the data\n\nCan use {dplyr} functions on data\n\n\nseattle_csv |&gt; \n  group_by(CheckoutYear) |&gt; \n  summarise(Checkouts = sum(Checkouts)) |&gt; \n  arrange(CheckoutYear) |&gt; \n  collect()\n#&gt; # A tibble: 18 × 2\n#&gt;   CheckoutYear Checkouts\n#&gt;          &lt;int&gt;     &lt;int&gt;\n#&gt; 1         2005   3798685\n#&gt; 2         2006   6599318\n#&gt; 3         2007   7126627\n#&gt; 4         2008   8438486\n#&gt; 5         2009   9135167\n#&gt; 6         2010   8608966\n#&gt; # ℹ 12 more rows",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#parquet-csv",
    "href": "modules/Arrow.html#parquet-csv",
    "title": "Introduction To Arrow",
    "section": "Parquet > CSV",
    "text": "Parquet &gt; CSV\n\nSlow: Manipulating large CSV datasets with {readr}\nFaster: Manipulating large CSV datasets with {arrow}\nMuch faster: Manipulating large parquet datasets with {arrow}\n\nData subdivided into multiple files",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#benefits-of-parquet",
    "href": "modules/Arrow.html#benefits-of-parquet",
    "title": "Introduction To Arrow",
    "section": "Benefits of parquet",
    "text": "Benefits of parquet\n\nSmaller files than CSV (efficient encodings + compression)\nStores datatypes (vs CSV storing all as character & guessing)\n“Column-oriented” (“thinks” like a dataframe)\nSplits data into chunks you can (often) skip (faster)\n\nBut:\n\nNot human-readable",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#partitioning",
    "href": "modules/Arrow.html#partitioning",
    "title": "Introduction To Arrow",
    "section": "Partitioning",
    "text": "Partitioning\n\nSplit data across files so analyses can skip unused data\nExperiment to find best partition for your data\nRecommendations:\n\n20 MB &lt; Filesize &lt; 2 GB\n&lt;= 10,000 files",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#seattle-library-csv-to-parquet",
    "href": "modules/Arrow.html#seattle-library-csv-to-parquet",
    "title": "Introduction To Arrow",
    "section": "Seattle library CSV to parquet",
    "text": "Seattle library CSV to parquet\n\ndplyr::group_by() to define partitions\narrow::write_dataset() to save as parquet\n\n\npq_path &lt;- \"data/seattle-library-checkouts\"\nseattle_csv |&gt;\n  group_by(CheckoutYear) |&gt;\n  write_dataset(path = pq_path, format = \"parquet\")",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#seattle-library-parquet-files",
    "href": "modules/Arrow.html#seattle-library-parquet-files",
    "title": "Introduction To Arrow",
    "section": "Seattle library parquet files",
    "text": "Seattle library parquet files\n\nApache Hive “self-describing” directory/file names\n\n\ntibble(\n  files = list.files(pq_path, recursive = TRUE),\n  size_MB = file.size(file.path(pq_path, files)) / 1024^2\n)\n#&gt; # A tibble: 18 × 2\n#&gt;   files                            size_MB\n#&gt;   &lt;chr&gt;                              &lt;dbl&gt;\n#&gt; 1 CheckoutYear=2005/part-0.parquet    109.\n#&gt; 2 CheckoutYear=2006/part-0.parquet    164.\n#&gt; 3 CheckoutYear=2007/part-0.parquet    178.\n#&gt; 4 CheckoutYear=2008/part-0.parquet    195.\n#&gt; 5 CheckoutYear=2009/part-0.parquet    214.\n#&gt; 6 CheckoutYear=2010/part-0.parquet    222.\n#&gt; # ℹ 12 more rows",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#parquet-arrow-dplyr",
    "href": "modules/Arrow.html#parquet-arrow-dplyr",
    "title": "Introduction To Arrow",
    "section": "parquet + {arrow} + {dplyr}",
    "text": "parquet + {arrow} + {dplyr}\n\nseattle_pq &lt;- open_dataset(pq_path)\nquery &lt;- seattle_pq |&gt; \n  filter(CheckoutYear &gt;= 2018, MaterialType == \"BOOK\") |&gt;\n  group_by(CheckoutYear, CheckoutMonth) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(CheckoutYear, CheckoutMonth)",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#results-uncollected",
    "href": "modules/Arrow.html#results-uncollected",
    "title": "Introduction To Arrow",
    "section": "Results (uncollected)",
    "text": "Results (uncollected)\n\nquery\n#&gt; FileSystemDataset (query)\n#&gt; CheckoutYear: int32\n#&gt; CheckoutMonth: int64\n#&gt; TotalCheckouts: int64\n#&gt; \n#&gt; * Grouped by CheckoutYear\n#&gt; * Sorted by CheckoutYear [asc], CheckoutMonth [asc]\n#&gt; See $.data for the source Arrow object",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#results-collected",
    "href": "modules/Arrow.html#results-collected",
    "title": "Introduction To Arrow",
    "section": "Results (collected)",
    "text": "Results (collected)\n\nquery |&gt; collect()\n#&gt; # A tibble: 58 × 3\n#&gt; # Groups:   CheckoutYear [5]\n#&gt;   CheckoutYear CheckoutMonth TotalCheckouts\n#&gt;          &lt;int&gt;         &lt;int&gt;          &lt;int&gt;\n#&gt; 1         2018             1         355101\n#&gt; 2         2018             2         309813\n#&gt; 3         2018             3         344487\n#&gt; 4         2018             4         330988\n#&gt; 5         2018             5         318049\n#&gt; 6         2018             6         341825\n#&gt; # ℹ 52 more rows",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#available-verbs",
    "href": "modules/Arrow.html#available-verbs",
    "title": "Introduction To Arrow",
    "section": "Available verbs",
    "text": "Available verbs\n\n?arrow-dplyr for supported functions\n\n(book uses ?acero but that’s way harder to remember)",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "modules/Arrow.html#performance",
    "href": "modules/Arrow.html#performance",
    "title": "Introduction To Arrow",
    "section": "Performance",
    "text": "Performance\n\nx |&gt; \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |&gt;\n  group_by(CheckoutMonth) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(desc(CheckoutMonth)) |&gt;\n  collect() |&gt; \n  system.time()\n\n\nCSV: 11.951s\nParquet: 0.263s",
    "crumbs": [
      "Modules",
      "Arrow"
    ]
  },
  {
    "objectID": "exercises/Exercise2.html",
    "href": "exercises/Exercise2.html",
    "title": "Exercise 2: Exercises for Tuesday",
    "section": "",
    "text": "In this exercise, you’ll build a neat report about a dataset you’ve seen before. This will be a practical example for you to put together all the skills you’ve learned so far. In particular, we want you to practice: Tableone and other useful packages, linear models, bootstrapping, and power, all in the context of the programming skills you’ve been developing. And you’ll do it all in Quarto to get a nice report!\n\nExercise guidelines\n\nWrite a report about an epidemiological question you think you can answer from the QCRC dataset. It’s ok if you only use the “Main_Dataset” sheet but if you’re comfortable with joins, feel free to combine information from other sheets. Your report should be a neatly formatted Quarto document with standard introduction, methods, results, and discussion sections (but each section can be short, and you don’t need references). We recommend that you use the “pdf”, “typst”, or “docx” output formats, because these are more useful than HTML in real life and can be a bit more challenging to work with.\nSelect an outcome variable, and then choose an exposure of interest. Select other covariates that you think could be effect modifiers or confounders, and justify your choices. State a research question and your hypothesis.\nInclude a “table one” of descriptive statistics.\nFit an appropriate GLM for your outcome. Report appropriate crude and adjusted measures of effect for each of the covariates you chose to analyze.\nPerform any other analyses you think are necessary to get your point across – this could be correlations, plots, quantiles, other methods you like to use, anything.\nReport and interpret your findings in the context of your research question.\n\nBonus questions\nIf you finish the main questions, you can try any of these bonus problems that interest you to make your report better. You’ll need to use the course resources and/or google and/or AI (RESPONSIBLY!!!) to work on these.\n\nTry to compare multiple models using AIC or analysis of deviance (you can google how to do these). Interpret the model comparisons, but remember that model selection is not causal.\nCalculate bootstrap confidence intervals for your model using the BCa method, and compare them to the profile confidence intervals. How different are they? If you really want to get your hands dirty, also calculate approximate Wald-type confidence intervals (±1.96 times the estimated standard error for a coefficient).\nPretend that the sample size you observed was a true constraint based on budget, and do a power analysis for the effect of your main exposure in your model. Since this is post-hoc, it doesn’t give you a lot of information, but how would you interpret this if you ran it before collecting the data?\nGet meaningful predictions/estimates from your model and interpret those – for example, if you chose a binary outcome, you could calculate risk differences for specific strata, or for a continuous/count outcome you could get predicted outcomes in specific strata.\nAdd a bibliography to your document and cite some references.\n\n\n\n\n\n\n\nReuseCC BY-NC 4.0",
    "crumbs": [
      "Exercises",
      "Exercise 2"
    ]
  },
  {
    "objectID": "exercises/Exercise1.html",
    "href": "exercises/Exercise1.html",
    "title": "Intermediate R Lab using QCRC Data",
    "section": "",
    "text": "In this lab, we’ll analyze the QCRC_FINAL_Deidentified.xlsx dataset to practice:\n\nTwo-sample t-test\nChi-square test\nLinear regression\nLogistic regression\nWriting if statements\nIteration using for loops and apply\n\n\nLet’s get started!",
    "crumbs": [
      "Exercises",
      "Exercise 1"
    ]
  },
  {
    "objectID": "exercises/Exercise1.html#exercise-1",
    "href": "exercises/Exercise1.html#exercise-1",
    "title": "Intermediate R Lab using QCRC Data",
    "section": "Exercise",
    "text": "Exercise\n\nRun the test above.\nNow insert a new code chunk below and write the code to repeat the chi-square test for Intubated vs. Died.",
    "crumbs": [
      "Exercises",
      "Exercise 1"
    ]
  },
  {
    "objectID": "exercises/Exercise1.html#exercise-2",
    "href": "exercises/Exercise1.html#exercise-2",
    "title": "Intermediate R Lab using QCRC Data",
    "section": "Exercise",
    "text": "Exercise\n\nRun the code above.\nNow, add a chuck and write the code to fit a model predicting Vent LOS using BMI adjust the analysis to account for Age by adding in an additional covariate.",
    "crumbs": [
      "Exercises",
      "Exercise 1"
    ]
  },
  {
    "objectID": "exercises/Exercise1.html#exercise-3",
    "href": "exercises/Exercise1.html#exercise-3",
    "title": "Intermediate R Lab using QCRC Data",
    "section": "Exercise",
    "text": "Exercise\n\nRun the model above.\nNow, insert a new chunk ande try predicting Intubated using BMI and Age.",
    "crumbs": [
      "Exercises",
      "Exercise 1"
    ]
  },
  {
    "objectID": "exercises/Exercise1.html#exercise-4",
    "href": "exercises/Exercise1.html#exercise-4",
    "title": "Intermediate R Lab using QCRC Data",
    "section": "Exercise",
    "text": "Exercise\n\n\nWrite an if-else statement that tests if a patient’s age is greater than 45 and prints:\n\n“Patient is older than 45.” if true\nOtherwise prints “Patient is 45 or younger.”\n\n\n\n\nCodeage_value &lt;- 72\n\nif (age_value &gt; 65) {\n  print(\"Patient is elderly\")\n} else {\n  print(\"Patient is not elderly\")\n}\n\n[1] \"Patient is elderly\"",
    "crumbs": [
      "Exercises",
      "Exercise 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to “R Programming for Infectious Disease Modeling”!\nThis website contains all of the material for the 2025 Summer Institute in Modeling for Infectious Diseases (SISMID) Module “R Programming for Infectious Disease Modeling”.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#prerequisities",
    "href": "index.html#prerequisities",
    "title": "Welcome",
    "section": "Prerequisities",
    "text": "Prerequisities\nParticipants are expected to have a basic understanding of R, including how to open and write R scripts, read in data, and use basic functions and syntax. Participants are also expected to have prior knowledge of basic descriptive statistics and regression modeling consistent with an introductory statistical course.\nBefore the course begins, you should install R and RStudio on your laptop. If you are using an older version of R, you should update it before the course begins. You will need at least R version 4.4.1 for this course, but using the most recent version (4.5.1 at the time of writing) is always preferable. You’ll also need Quarto.\n\nYou can install R from the CRAN website by clicking on the correct download link for your OS.\nYou can install RStudio from the Posit website.\nYou can install the latest version of Quarto from the Quarto website, but if you have a relatively up-to-date version of RStudio, it will also come with Quarto.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#about-the-instructors",
    "href": "index.html#about-the-instructors",
    "title": "Welcome",
    "section": "About the instructors",
    "text": "About the instructors\n\n\n\nCo-Instructor: Alex Edwards\n\n\nAlex Edwards is an Instructor in the Department of Biostatistics and Bioinformatics at Emory University’s Rollins School of Public Health. With a robust background in healthcare and public health research, Alex brings a wealth of knowledge and experience to his teaching. He is an alumnus of the Rollins School of Public Health, where he earned his Master of Science in Public Health (MSPH) in 2017. Currently, Alex is pursuing his PhD at the Stellenbosch University, ZA, focusing on the spatial modeling of HIV multi-morbidity disorder in sub-Saharan Africa. Alex has published extensively on the intersection of HIV and non-communicable diseases, contributing valuable insights to the field. His dedication to advancing public health is evident in his teaching and research endeavors.\n\n\n\nCo-Instructor: Zane Billings\n\n\nZane Billings is a PhD student in Epidemiology and Biostatistics at the University of Georgia, working with Andreas Handel. He has been using R since 2017, and uses R for nearly all of his statistics and data science practice. Zane’s research focuses on the immune response to influenza vaccination, and uses machine learning and multilevel regression modeling (in R!) to improve our understanding of influenza immunology.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "modules/Module00-Welcome.html#welcome-to-sismid-workshop-r-programming-for-infectious-disease-modeling",
    "href": "modules/Module00-Welcome.html#welcome-to-sismid-workshop-r-programming-for-infectious-disease-modeling",
    "title": "Welcome!",
    "section": "Welcome to SISMID Workshop: R Programming for Infectious Disease Modeling!",
    "text": "Welcome to SISMID Workshop: R Programming for Infectious Disease Modeling!\nAlex Edwards (Any)\nPhD Candidate, Division of Epidemiology and Health Sciences, Stellenbosch University\nEmail: Alex.edwards@emory.edu\n\nZane Billings (he/him)\nPhD Candidate, Department of Epidemiology and Biostatistics\nEmail: Wesley.Billings@uga.edu",
    "crumbs": [
      "Modules",
      "Intro slides"
    ]
  },
  {
    "objectID": "modules/Module00-Welcome.html#introductions",
    "href": "modules/Module00-Welcome.html#introductions",
    "title": "Welcome!",
    "section": "Introductions",
    "text": "Introductions\n\nName?\nCurrent position / institution?\nMost interesting pathogen/condition/exposure to you?",
    "crumbs": [
      "Modules",
      "Intro slides"
    ]
  },
  {
    "objectID": "modules/Module00-Welcome.html#course-website",
    "href": "modules/Module00-Welcome.html#course-website",
    "title": "Welcome!",
    "section": "Course website",
    "text": "Course website\n\nAll of the materials for this course can be found online here: https://wzbillings.github.io/SISMID-2025/.\nThis contains the schedule, course resources, and online versions of all of our slide decks.\nThe Course Resources page contains download links for all of the data, exercises, and slides for this class.\nPlease feel free to download these resources and share them – all of the course content is under the Creative Commons BY-NC 4.0 license.",
    "crumbs": [
      "Modules",
      "Intro slides"
    ]
  },
  {
    "objectID": "modules/Module00-Welcome.html#overall-workshop-objectives",
    "href": "modules/Module00-Welcome.html#overall-workshop-objectives",
    "title": "Welcome!",
    "section": "Overall Workshop Objectives",
    "text": "Overall Workshop Objectives\nBy the end of this workshop, you should be able to\n\nWrite code that uses advanced R programming tools like control flow and custom functions.\nUse advanced R programming tools to answer interesting epidemiology questions.\nBe (more) prepared to use R in other SISMID modules",
    "crumbs": [
      "Modules",
      "Intro slides"
    ]
  },
  {
    "objectID": "modules/Module00-Welcome.html#workshop-overview",
    "href": "modules/Module00-Welcome.html#workshop-overview",
    "title": "Welcome!",
    "section": "Workshop Overview",
    "text": "Workshop Overview\nMultiple modules that will each:\n\nStart with learning objectives\nEnd with summary slides\nInclude mini-exercises or practice problems\n\nThemes that will show up throughout the workshop:\n\nReproducibility\nGood coding techniques\nThinking algorithmically\n\nNote that we listed a lot of topics on the course description, we won’t have time to get in-depth with most of them. What are you most interested in learning?",
    "crumbs": [
      "Modules",
      "Intro slides"
    ]
  },
  {
    "objectID": "modules/Module00-Welcome.html#course-structure",
    "href": "modules/Module00-Welcome.html#course-structure",
    "title": "Welcome!",
    "section": "Course structure",
    "text": "Course structure\n\nWe’ll start the course with some fundamentals.\nThese include advanced programming techniques, applications of those programming techniques, and just some neat things we think will help prepare you for infectious disease modeling.\nSome examples are: using R packages to improve your life, simulating power, bootstrap confidence intervals.\nWe’ll have time for some other advanced topics too, but maybe not all of them depending. These could be: disease mapping, maximum likelihood, differential equations, and advanced linear models.",
    "crumbs": [
      "Modules",
      "Intro slides"
    ]
  },
  {
    "objectID": "modules/Module00-Welcome.html#useful-free-resources",
    "href": "modules/Module00-Welcome.html#useful-free-resources",
    "title": "Welcome!",
    "section": "Useful (+ Free) Resources",
    "text": "Useful (+ Free) Resources\nWant more?\n\nR for Data Science: http://r4ds.had.co.nz/\n(great general information)\nFundamentals of Data Visualization: https://clauswilke.com/dataviz/\nR for Epidemiology: https://www.r4epi.com/\nThe Epidemiologist R Handbook: https://epirhandbook.com/en/\nR basics by Rafael A. Irizarry: https://rafalab.github.io/dsbook/r-basics.html (great general information)\nOpen Case Studies: https://www.opencasestudies.org/\n(resource for specific public health cases with statistical implementation and interpretation)",
    "crumbs": [
      "Modules",
      "Intro slides"
    ]
  },
  {
    "objectID": "modules/Module00-Welcome.html#useful-free-resources-1",
    "href": "modules/Module00-Welcome.html#useful-free-resources-1",
    "title": "Welcome!",
    "section": "Useful (+Free) Resources",
    "text": "Useful (+Free) Resources\nNeed help?\n\nVarious “Cheat Sheets”: https://github.com/rstudio/cheatsheets/\nR reference card: http://cran.r-project.org/doc/contrib/Short-refcard.pdf\nR jargon: https://link.springer.com/content/pdf/bbm%3A978-1-4419-1318-0%2F1.pdf\nR vs Stata: https://link.springer.com/content/pdf/bbm%3A978-1-4419-1318-0%2F1.pdf\nR terminology: https://cran.r-project.org/doc/manuals/r-release/R-lang.pdf",
    "crumbs": [
      "Modules",
      "Intro slides"
    ]
  },
  {
    "objectID": "modules/Module00-Welcome.html#installing-r",
    "href": "modules/Module00-Welcome.html#installing-r",
    "title": "Welcome!",
    "section": "Installing R",
    "text": "Installing R\nHopefully everyone has pre-installed R and RStudio (and Quarto if needed). Let’s take a minute to make sure everyone is set up and then jump in!",
    "crumbs": [
      "Modules",
      "Intro slides"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#learning-objectives",
    "href": "modules/Module02-StatReview.html#learning-objectives",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Learning objectives:",
    "text": "Learning objectives:\n\nDevelop a statistical hypothesis for a given research question\nIdentify appropriate statistical test for a given hypothesis\nImplement meaningful code to model data and answer research questions",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#data",
    "href": "modules/Module02-StatReview.html#data",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Data",
    "text": "Data\n\nqcrc_main &lt;- import(here(\"data\", \"QCRC_FINAL_Deidentified.xlsx\")) %&gt;% \n             mutate(BMI = as.numeric(BMI),\n                    ICU_LOS = as.numeric(ICU_LOS))\nneo &lt;- import(\"https://www.kaggle.com/api/v1/datasets/download/zahrazolghadr/neonatal-hypothermia\")\noptions(scipen = 999) #This sets the numer of digits for p-values so that we can avoid scientific notation",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#helpful-package",
    "href": "modules/Module02-StatReview.html#helpful-package",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Helpful Package",
    "text": "Helpful Package\nThe janitor package can be helpful in expediting the initial data cleaning that comes along with analysis. One particular function I want to draw your attention to is clean_names as it will:\n\nreplace spaces with underscores\nmake all letters lowercase\nremove non-standard characters",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#janitor-example",
    "href": "modules/Module02-StatReview.html#janitor-example",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Janitor Example",
    "text": "Janitor Example\nCurrently the names are a bit of a mess:\n\n\n[1] \"Patient_DEID\"     \"Decatur_Transfer\" \"Age\"              \"Female\"          \n[5] \"Race\"             \"Died\"             \"30D_Mortality\"   \n\n\nLet’s clean them:\n\nqcrc_main &lt;- clean_names(qcrc_main)\n\nNow, they are much more manageable:\n\n\n[1] \"patient_deid\"     \"decatur_transfer\" \"age\"              \"female\"          \n[5] \"race\"             \"died\"             \"x30d_mortality\"",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#t-test-one-sample",
    "href": "modules/Module02-StatReview.html#t-test-one-sample",
    "title": "Inferential Statistics and Modeling Review",
    "section": "T-test One Sample",
    "text": "T-test One Sample\nHypothesis test used to determine whether the mean calculated from sample data collected from a single group is different from a designated value specified by the researcher",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#t-test-one-sample-question",
    "href": "modules/Module02-StatReview.html#t-test-one-sample-question",
    "title": "Inferential Statistics and Modeling Review",
    "section": "T-test One Sample Question",
    "text": "T-test One Sample Question\nIn the QCRC sample, is the average BMI of patients significantly different from the average BMI of the patients examined in the paper, Associations between body-mass index and COVID-19 severity in 6·9 million people in England: a prospective, community-based, cohort study by G.Min Et. Al., of \\(28.76 kg/m^2\\)\nHypothesis\n\\(H_0: \\mu{BMI}_{qcrc} = \\mu{BMI}_{covid}\\)\n\\(H_A: \\mu{BMI}_{qcrc} \\neq \\mu{BMI}_{covid}\\)",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#t-test-one-sample-function",
    "href": "modules/Module02-StatReview.html#t-test-one-sample-function",
    "title": "Inferential Statistics and Modeling Review",
    "section": "T-test One Sample Function",
    "text": "T-test One Sample Function\nFor all t-test we will use the base R function t.test, the t.test function will calculate the test and give us basic output\nFor a one-sample we must provide two arguments:\n\nx, the continuous data column in the dataset we wish to test against the given mean\nmu, the given mean we are comparing against",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#t-test-one-sample-output",
    "href": "modules/Module02-StatReview.html#t-test-one-sample-output",
    "title": "Inferential Statistics and Modeling Review",
    "section": "T-test One Sample Output",
    "text": "T-test One Sample Output\n\nt.test(x=qcrc_main$bmi, mu=28.76)\n\n\n    One Sample t-test\n\ndata:  qcrc_main$bmi\nt = 5.7635, df = 284, p-value = 0.00000002144\nalternative hypothesis: true mean is not equal to 28.76\n95 percent confidence interval:\n 30.63377 32.57746\nsample estimates:\nmean of x \n 31.60561",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#t-test-two-sample",
    "href": "modules/Module02-StatReview.html#t-test-two-sample",
    "title": "Inferential Statistics and Modeling Review",
    "section": "T-Test Two Sample",
    "text": "T-Test Two Sample\nHypothesis test used to test whether the unknown population means of two groups are equal or not.",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#t-test-two-sample-question",
    "href": "modules/Module02-StatReview.html#t-test-two-sample-question",
    "title": "Inferential Statistics and Modeling Review",
    "section": "T-Test Two Sample Question",
    "text": "T-Test Two Sample Question\nIs there a statistically significant difference in the BMI of male and female COVID patients in the provided sample of Emory Patients?\nHypothesis\n\\(H_0: \\mu{BMI}_{males} = \\mu{BMI}_{females}\\)\n\\(H_A: \\mu{BMI}_{males} \\neq \\mu{BMI}_{females}\\)",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#t-test-two-sample-function",
    "href": "modules/Module02-StatReview.html#t-test-two-sample-function",
    "title": "Inferential Statistics and Modeling Review",
    "section": "T-Test Two Sample Function",
    "text": "T-Test Two Sample Function\nFor all t-test we will use the base R function t.test, the t.test function will calculate the test and give us basic output\nFor a two-sample we must provide a single argument:\n\ncontinuous ~ categorical or outcome ~ exposure, formula that models the relationship with your continuous outcome on the left side and two-level categorical variable on the right side separated by a ~",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#t-test-two-sample-output",
    "href": "modules/Module02-StatReview.html#t-test-two-sample-output",
    "title": "Inferential Statistics and Modeling Review",
    "section": "T-Test Two Sample Output",
    "text": "T-Test Two Sample Output\n\nt.test(qcrc_main$bmi~qcrc_main$female)\n\n\n    Welch Two Sample t-test\n\ndata:  qcrc_main$bmi by qcrc_main$female\nt = 1.6845, df = 249.91, p-value = 0.09333\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -0.2864912  3.6734024\nsample estimates:\nmean in group Female   mean in group Male \n            32.53256             30.83910",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#t-test-paired",
    "href": "modules/Module02-StatReview.html#t-test-paired",
    "title": "Inferential Statistics and Modeling Review",
    "section": "T-test Paired",
    "text": "T-test Paired\nHypothesis test used to determine whether the mean difference between two sets of observations is zero",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#t-test-paired-question",
    "href": "modules/Module02-StatReview.html#t-test-paired-question",
    "title": "Inferential Statistics and Modeling Review",
    "section": "T-test Paired Question",
    "text": "T-test Paired Question\nIn the neonatal hypothermia dataset there exist a statistically significant difference in body temperature between time point one (t.1) and time point two (t.2).\nHypothesis\n\\(H_0: \\mu_{t.2 - t.1} = 0\\) or \\(H_0: \\mu_{t.1 - t.2} = 0\\) \\(H_A: \\mu_{t.2 - t.1} \\ne 0\\) or \\(H_A: \\mu_{t.1 - t.2} \\ne 0\\)",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#t-test-paired-function",
    "href": "modules/Module02-StatReview.html#t-test-paired-function",
    "title": "Inferential Statistics and Modeling Review",
    "section": "T-test Paired Function",
    "text": "T-test Paired Function\nFor all t-test we will use the base R function t.test, the t.test function will calculate the test and give us basic output\nFor a paired t-test we must provide three arguments:\n\nx, the first measurement continuous variable\ny, the second measurement continuous variable\npaired = T, indicator variable to let the function know you want to perform a paired t-test",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#t-test-paired-output",
    "href": "modules/Module02-StatReview.html#t-test-paired-output",
    "title": "Inferential Statistics and Modeling Review",
    "section": "T-test Paired Output",
    "text": "T-test Paired Output\n\nt.test(x=neo$t.1, y=neo$t.2, paired = T)\n\n\n    Paired t-test\n\ndata:  neo$t.1 and neo$t.2\nt = -22.284, df = 199, p-value &lt; 0.00000000000000022\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -1.1668622 -0.9771378\nsample estimates:\nmean difference \n         -1.072",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#t-test-paired-output-reversed",
    "href": "modules/Module02-StatReview.html#t-test-paired-output-reversed",
    "title": "Inferential Statistics and Modeling Review",
    "section": "T-test Paired Output reversed",
    "text": "T-test Paired Output reversed\nNote, a paired t-test looks at magnitude of difference so you can normally ignore the negative signs in the output or reverse the x and y variables in the arguments\n\nt.test(x=neo$t.2, y=neo$t.1, paired = T)\n\n\n    Paired t-test\n\ndata:  neo$t.2 and neo$t.1\nt = 22.284, df = 199, p-value &lt; 0.00000000000000022\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.9771378 1.1668622\nsample estimates:\nmean difference \n          1.072",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#anova-one-way",
    "href": "modules/Module02-StatReview.html#anova-one-way",
    "title": "Inferential Statistics and Modeling Review",
    "section": "ANOVA One Way",
    "text": "ANOVA One Way\nAnalysis of Variance\nHypothesis test used to analyze the difference between the means of more than two groups",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#anova-question",
    "href": "modules/Module02-StatReview.html#anova-question",
    "title": "Inferential Statistics and Modeling Review",
    "section": "ANOVA Question",
    "text": "ANOVA Question\nIn the Emory dataset, do we find a statistically significant difference in BMI between racial groups?\nHypothesis\n\\(H_0: \\mu_{AA} = \\mu_{A} = \\mu_{W} = \\mu_{M} = \\mu_{U}\\)\n\\(H_A: At\\; least\\; one\\; \\mu_i\\; differs\\; between\\; the\\; groups\\)\nPlain English: At least one group mean is different from at least one other mean",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#anova-functions",
    "href": "modules/Module02-StatReview.html#anova-functions",
    "title": "Inferential Statistics and Modeling Review",
    "section": "ANOVA Function(s)",
    "text": "ANOVA Function(s)\nFor an ANOVA we will use the base R function aov, the aovfunction will calculate the test and give us basic output but we need to complement it with the summary function to get the full amount of information\nNote, ANOVA only tells you a group is different. Not which group!\nSo, we will also perform a Tukey pairwise comparisons corection to find our different group(s).",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#anova-function-arguments",
    "href": "modules/Module02-StatReview.html#anova-function-arguments",
    "title": "Inferential Statistics and Modeling Review",
    "section": "ANOVA Function Arguments",
    "text": "ANOVA Function Arguments\nFor an ANOVA using aov we need the following argument:\n\ncontinuous ~ categorical or outcome ~ exposure, formula that models the relationship with your continuous outcome on the left side and 3+ level categorical variable on the right side separated by a ~\n\nFor the summary function we require one argument:\n\nobject, an object created by storing the output of the aov function",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#anova-output-without-summary",
    "href": "modules/Module02-StatReview.html#anova-output-without-summary",
    "title": "Inferential Statistics and Modeling Review",
    "section": "ANOVA Output Without summary",
    "text": "ANOVA Output Without summary\nNotice, no confidence intervals or P-values.\n\naov(qcrc_main$bmi~qcrc_main$race)\n\nCall:\n   aov(formula = qcrc_main$bmi ~ qcrc_main$race)\n\nTerms:\n                qcrc_main$race Residuals\nSum of Squares        1678.259 18052.692\nDeg. of Freedom              4       280\n\nResidual standard error: 8.029564\nEstimated effects may be unbalanced\n3 observations deleted due to missingness",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#anova-output-with-summary",
    "href": "modules/Module02-StatReview.html#anova-output-with-summary",
    "title": "Inferential Statistics and Modeling Review",
    "section": "ANOVA Output With summary",
    "text": "ANOVA Output With summary\n\nanova &lt;- aov(qcrc_main$bmi~qcrc_main$race)\nsummary(anova)\n\n                Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nqcrc_main$race   4   1678   419.6   6.508 0.0000508 ***\nResiduals      280  18053    64.5                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n3 observations deleted due to missingness",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#anova-multiple-comparisons-tukeyhsd-function",
    "href": "modules/Module02-StatReview.html#anova-multiple-comparisons-tukeyhsd-function",
    "title": "Inferential Statistics and Modeling Review",
    "section": "ANOVA Multiple Comparisons TukeyHSD function",
    "text": "ANOVA Multiple Comparisons TukeyHSD function\nFor TukeyHSD we require one argument:\n\nobject, an object created by storing the output of the aov function",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#anova-output-with-tukeyhsd",
    "href": "modules/Module02-StatReview.html#anova-output-with-tukeyhsd",
    "title": "Inferential Statistics and Modeling Review",
    "section": "ANOVA Output with TukeyHSD",
    "text": "ANOVA Output with TukeyHSD\n\nTukeyHSD(anova)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = qcrc_main$bmi ~ qcrc_main$race)\n\n$`qcrc_main$race`\n                                                                    diff\nAsian-African American  or Black                              -9.1196581\nCaucasian or White-African American  or Black                 -5.1159544\nMultiple-African American  or Black                           -3.0641026\nUnknown, Unavailable or Unreported-African American  or Black -2.0871795\nCaucasian or White-Asian                                       4.0037037\nMultiple-Asian                                                 6.0555556\nUnknown, Unavailable or Unreported-Asian                       7.0324786\nMultiple-Caucasian or White                                    2.0518519\nUnknown, Unavailable or Unreported-Caucasian or White          3.0287749\nUnknown, Unavailable or Unreported-Multiple                    0.9769231\n                                                                     lwr\nAsian-African American  or Black                              -16.636088\nCaucasian or White-African American  or Black                  -8.506117\nMultiple-African American  or Black                           -25.166827\nUnknown, Unavailable or Unreported-African American  or Black  -6.690034\nCaucasian or White-Asian                                       -3.933860\nMultiple-Asian                                                -17.183251\nUnknown, Unavailable or Unreported-Asian                       -1.493833\nMultiple-Caucasian or White                                   -20.197612\nUnknown, Unavailable or Unreported-Caucasian or White          -2.233779\nUnknown, Unavailable or Unreported-Multiple                   -21.489312\n                                                                    upr\nAsian-African American  or Black                              -1.603228\nCaucasian or White-African American  or Black                 -1.725791\nMultiple-African American  or Black                           19.038622\nUnknown, Unavailable or Unreported-African American  or Black  2.515675\nCaucasian or White-Asian                                      11.941267\nMultiple-Asian                                                29.294363\nUnknown, Unavailable or Unreported-Asian                      15.558790\nMultiple-Caucasian or White                                   24.301316\nUnknown, Unavailable or Unreported-Caucasian or White          8.291328\nUnknown, Unavailable or Unreported-Multiple                   23.443158\n                                                                  p adj\nAsian-African American  or Black                              0.0086325\nCaucasian or White-African American  or Black                 0.0004339\nMultiple-African American  or Black                           0.9955263\nUnknown, Unavailable or Unreported-African American  or Black 0.7249240\nCaucasian or White-Asian                                      0.6378766\nMultiple-Asian                                                0.9528496\nUnknown, Unavailable or Unreported-Asian                      0.1596273\nMultiple-Caucasian or White                                   0.9990901\nUnknown, Unavailable or Unreported-Caucasian or White         0.5113490\nUnknown, Unavailable or Unreported-Multiple                   0.9999539",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#correlation",
    "href": "modules/Module02-StatReview.html#correlation",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Correlation",
    "text": "Correlation\nThis analysis serves to characterize the relationship between two continuous variable. For instance, if we see an increase in calories do we expect to see an increase in weight. This analysis returns a value \\(\\rho\\) that tells you the strength and direction of the relationship and takes the values [-1,1] inclusive. A -1 means a strong negative relationship and a 1 means a strong positive relationship. A zero means no relationship. The closer to zero the weaker the relationship\nCorrelation is NOT Causation!",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#correlation-question",
    "href": "modules/Module02-StatReview.html#correlation-question",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Correlation Question",
    "text": "Correlation Question\nIs there a relationship between BMI and length of hospital stay in the Emory COVID dataset?\nHypothesis\n\\(H_0: \\rho = 0\\)\n\\(H_A: \\rho \\ne 0\\)",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#correlation-function",
    "href": "modules/Module02-StatReview.html#correlation-function",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Correlation Function",
    "text": "Correlation Function\nFor correlation we will use the base R function cor.test, the cor.test function will calculate \\(\\rho\\) and give us basic output\nFor correlation we must provide three arguments:\n\nx, the first measurement continuous variable\ny, the second measurement continuous variable\nuse = \"complete.obs\", ignores missing values for the calculation and uses only complete observations (not sure why it doesn’t use na.rm and yes, it’s obnoxious)",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#correlation-output",
    "href": "modules/Module02-StatReview.html#correlation-output",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Correlation Output",
    "text": "Correlation Output\n\ncor.test(qcrc_main$icu_los, qcrc_main$bmi, use = \"complete.obs\")\n\n\n    Pearson's product-moment correlation\n\ndata:  qcrc_main$icu_los and qcrc_main$bmi\nt = 3.234, df = 283, p-value = 0.001365\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.07422377 0.29842419\nsample estimates:\n      cor \n0.1887828",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#correlation-output-graph-check",
    "href": "modules/Module02-StatReview.html#correlation-output-graph-check",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Correlation Output Graph Check",
    "text": "Correlation Output Graph Check\nAlways check your correlation with a graph to make sure the data is linear\n\nggplot(qcrc_main, aes(x=bmi, y= icu_los))+\n  geom_point()+\n  geom_smooth(method=\"lm\")+\n  stat_cor()",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#simple-linear-regression",
    "href": "modules/Module02-StatReview.html#simple-linear-regression",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nThis analysis is used to characterize the relationship between two continuous variables, where one is treated as the predictor (independent variable) and the other as the outcome (dependent variable). For example, we can analyze whether an increase in calorie intake is associated with an increase in weight. Simple linear regression provides a model to estimate this relationship using a straight-line equation.",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#slr-model-equation",
    "href": "modules/Module02-StatReview.html#slr-model-equation",
    "title": "Inferential Statistics and Modeling Review",
    "section": "SLR Model Equation",
    "text": "SLR Model Equation\n\\(y = \\beta_0 + \\beta_1x + \\epsilon\\)\n\n\\(\\beta_1\\)​: The slope of the line, indicating the strength and direction of the relationship.\n​\\(\\beta_0\\): The y-intercept, representing the value of \\(y\\) when \\(x=0\\).\n\\(\\epsilon\\): Random error.\n\nLike correlation, regression does not imply causation! Always interpret results within the study’s context and limitations.",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#slr-question",
    "href": "modules/Module02-StatReview.html#slr-question",
    "title": "Inferential Statistics and Modeling Review",
    "section": "SLR Question",
    "text": "SLR Question\nIs BMI (predictor) associated with the length of hospital stay (outcome) in the Emory COVID dataset? Meaning, if BMI changes how much (\\(\\beta_1\\) ) does it change length of stay?\nHypothesis\n\\(H_0: \\beta_1 = 0\\)\n\\(H_0: \\beta_1 \\ne 0\\)",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#slr-function",
    "href": "modules/Module02-StatReview.html#slr-function",
    "title": "Inferential Statistics and Modeling Review",
    "section": "SLR Function",
    "text": "SLR Function\nWe can perform simple linear regression in R using the lm() function. Here’s how the key components fit:\n\nformula = y ~ x specifies the dependent and independent variables.\ndata specifies the dataset to use.",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#slr-output",
    "href": "modules/Module02-StatReview.html#slr-output",
    "title": "Inferential Statistics and Modeling Review",
    "section": "SLR Output",
    "text": "SLR Output\n\n# Fit a simple linear regression model\nmodel &lt;- lm(icu_los ~ bmi, data = qcrc_main)\n\n# Summarize the results\nsummary(model)\n\n\nCall:\nlm(formula = icu_los ~ bmi, data = qcrc_main)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.180  -6.659  -1.791   4.850  47.637 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   4.3119     2.0068   2.149  0.03251 * \nbmi           0.1986     0.0614   3.234  0.00137 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.625 on 283 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.03564,   Adjusted R-squared:  0.03223 \nF-statistic: 10.46 on 1 and 283 DF,  p-value: 0.001365",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#output-interpretation",
    "href": "modules/Module02-StatReview.html#output-interpretation",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Output Interpretation",
    "text": "Output Interpretation\n\nEstimate for \\(\\beta_1\\)​: Indicates the expected change in the icu_los for a one-unit increase in bmi.\nP-value: Tests if \\(\\beta_1\\)​ is significantly different from 0.\n\\(\\mathbf{R^2}\\)​: Proportion of variance in the outcome explained by the predictor.",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#multiple-linear-regression",
    "href": "modules/Module02-StatReview.html#multiple-linear-regression",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nMultiple Linear Regression (MLR) is used to explore and model the relationship between one dependent (outcome) variable and two or more independent (predictor) variables. The goal is to understand how each predictor contributes to the outcome, while controlling for the others.",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#mlr-model-equation",
    "href": "modules/Module02-StatReview.html#mlr-model-equation",
    "title": "Inferential Statistics and Modeling Review",
    "section": "MLR Model Equation",
    "text": "MLR Model Equation\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_kx_k + \\epsilon\\)\nWhere:\n\n\\(y\\): Dependent variable (outcome).\n\\(x_1, x_2, \\dots, x_k\\)​: Independent variables (predictors).\n\\(\\beta_0\\)​: Intercept (value of \\(y\\) when all \\(x\\)’s are 0).\n\\(\\beta_1, \\beta_2, \\dots, \\beta_k​\\): Regression coefficients, showing the change in \\(y\\) for a one-unit increase in \\(x\\), holding other variables constant.\n\\(\\epsilon\\): Random error.",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#mlr-question",
    "href": "modules/Module02-StatReview.html#mlr-question",
    "title": "Inferential Statistics and Modeling Review",
    "section": "MLR Question",
    "text": "MLR Question\nDoes BMI, age, and gender predict the length of hospital stay in the Emory COVID dataset? Or, in another way, how does length of stay change when BMI changes when we control for age and gender?\nHypothesis\n\\(H_0:\\beta_{BMI}​\\;=\\;\\beta_{age}\\;​=\\;\\beta_{gender}\\;​=\\;0\\)\n\\(H_A: At\\; least\\; one\\; \\beta \\neq 0\\)",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#mlr-function",
    "href": "modules/Module02-StatReview.html#mlr-function",
    "title": "Inferential Statistics and Modeling Review",
    "section": "MLR Function",
    "text": "MLR Function\nWe can perform multiple linear regression in R using the lm() function. Here’s how the key components fit:\n\nformula =\\(x_1\\;+\\;x_2\\;+\\dots\\;+x_k\\) specifies the dependent and independent variables.\ndata specifies the dataset to use.",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#mlr-output",
    "href": "modules/Module02-StatReview.html#mlr-output",
    "title": "Inferential Statistics and Modeling Review",
    "section": "MLR Output",
    "text": "MLR Output\n\n# Fit the MLR model\nmodel &lt;- lm(icu_los ~ bmi + female + age, data = qcrc_main)\n\n# Summarize the results\nsummary(model)\n\n\nCall:\nlm(formula = icu_los ~ bmi + female + age, data = qcrc_main)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.698  -6.211  -1.879   4.690  48.262 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.43801    3.69327  -0.389 0.697304    \nbmi          0.24431    0.06588   3.708 0.000251 ***\nfemaleMale   1.62934    1.02860   1.584 0.114310    \nage          0.05374    0.03511   1.530 0.127022    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.586 on 281 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.05116,   Adjusted R-squared:  0.04103 \nF-statistic: 5.051 on 3 and 281 DF,  p-value: 0.002011",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#mlr-output-interpretation",
    "href": "modules/Module02-StatReview.html#mlr-output-interpretation",
    "title": "Inferential Statistics and Modeling Review",
    "section": "MLR Output Interpretation",
    "text": "MLR Output Interpretation\n\nCoefficients (\\(\\beta\\)): The expected change in \\(y\\) for a one-unit increase in the predictor, holding all other predictors constant..\nP-value: Small p-values (e.g., &lt;0.05&lt; 0.05&lt;0.05) suggest a significant relationship between the predictor and the outcome.\nAdjusted \\(R^2\\): Higher values indicate a better model fit, accounting for the number of predictors.",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#logistic-regression",
    "href": "modules/Module02-StatReview.html#logistic-regression",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is used to model the relationship between a binary outcome variable (e.g., “died” vs. “survived”) and one or more predictor variables. It estimates the probability of the outcome occurring based on the predictors.",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#logistic-model-equation",
    "href": "modules/Module02-StatReview.html#logistic-model-equation",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Logistic Model Equation",
    "text": "Logistic Model Equation\nThe logistic regression model predicts the log-odds of the outcome: \\(\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 \\cdot \\text{female} + \\beta_2 \\cdot \\text{intubated}\\)\nWhere:\n\n\\(p\\): Probability of the outcome occurring (e.g., \\(P(died = 1)\\)).\n\\(\\beta_0\\) ​: Intercept (log-odds of the outcome when all predictors are 0).\n\\(\\beta_1\\)​: Effect of being female on the log-odds of death, holding intubation status constant.\n\\(\\beta_2\\)​: Effect of being intubated on the log-odds of death, holding gender constant.",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#logistic-question",
    "href": "modules/Module02-StatReview.html#logistic-question",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Logistic Question",
    "text": "Logistic Question\nDoes gender (female) and intubation status predict the likelihood of death in the dataset?\nHypothesis\n\n\\(H_0:\\beta_{female}\\;​=\\;\\beta_{intubated}\\;​=\\;0\\) (Neither gender nor intubation affect the odds of death.)\n\\(H_a​: At\\; least\\; one\\; \\beta \\neq 0\\).",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#logistic-function",
    "href": "modules/Module02-StatReview.html#logistic-function",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Logistic Function",
    "text": "Logistic Function\nTo fit a logistic regression model in R, use the glm() function with family = binomial.\n\nformula: Specifies the model to fit using the syntax: \\(outcome \\sim x_1 + x_2 +\\dots+x_k\\)\ndata specifies the dataset to use.\nfamily Specifies the type of regression to perform by defining the error distribution and link function. For logistic regression we use binomial but there are also options for gaussian and poisson",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#logistic-output",
    "href": "modules/Module02-StatReview.html#logistic-output",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Logistic Output",
    "text": "Logistic Output\n\n# Fit the logistic regression model\nmodel &lt;- glm(died ~ female + intubated, data = qcrc_main, family = binomial)\n\n# Summarize the results\nsummary(model)\n\n\nCall:\nglm(formula = died ~ female + intubated, family = binomial, data = qcrc_main)\n\nCoefficients:\n            Estimate Std. Error z value  Pr(&gt;|z|)    \n(Intercept) -1.27187    0.30248  -4.205 0.0000261 ***\nfemaleMale   0.05613    0.25344   0.221    0.8247    \nintubated    0.75926    0.31004   2.449    0.0143 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 369.34  on 287  degrees of freedom\nResidual deviance: 362.73  on 285  degrees of freedom\nAIC: 368.73\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#logistic-important",
    "href": "modules/Module02-StatReview.html#logistic-important",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Logistic IMPORTANT!!",
    "text": "Logistic IMPORTANT!!\nThe \\(\\beta\\)s that we get from a logistic regression are useless, you MUST exponeniate them for them to be meaningful.\n\nexp(coef(model))\n\n(Intercept)  femaleMale   intubated \n  0.2803069   1.0577398   2.1366863",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module02-StatReview.html#logistic-output-interpretation",
    "href": "modules/Module02-StatReview.html#logistic-output-interpretation",
    "title": "Inferential Statistics and Modeling Review",
    "section": "Logistic Output Interpretation",
    "text": "Logistic Output Interpretation\nExample:\n\n\\(\\text{Odds Ratio for Female} = \\exp(0.056) \\approx 1.06\\): The odds of death for females are 1.06 times higher than for males.\n\\(\\text{Odds Ratio for Intubated} = \\exp(0.759) \\approx 2.13\\) : The odds of death for intubated patients are 2.13 times higher than for non-intubated patients.",
    "crumbs": [
      "Modules",
      "Stats in R"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#learning-objectives",
    "href": "modules/Module04-Functions.html#learning-objectives",
    "title": "Module 4: Functions",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter module XX, you should be able to:\n\nName the parts of a function\nWrite a function\nUse the R/RStudio debugging tools for functions",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#writing-your-own-functions",
    "href": "modules/Module04-Functions.html#writing-your-own-functions",
    "title": "Module 4: Functions",
    "section": "Writing your own functions",
    "text": "Writing your own functions\nwhy create your own function?\n\nto cut down on repetitive coding\nto organize code into manageable chunks\nto avoid running code unintentionally\nto use names that make sense to you",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#writing-your-own-functions-1",
    "href": "modules/Module04-Functions.html#writing-your-own-functions-1",
    "title": "Module 4: Functions",
    "section": "Writing your own functions",
    "text": "Writing your own functions\nHere we will write a function that multiplies some number (x) by 2:\n\ntimes_2 &lt;- function(x) x*2\n\nWhen you run the line of code above, you make it ready to use (no output yet!) Let’s test it!\n\ntimes_2(x=10)\n\n[1] 20",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#writing-your-own-functions-2",
    "href": "modules/Module04-Functions.html#writing-your-own-functions-2",
    "title": "Module 4: Functions",
    "section": "Writing your own functions:",
    "text": "Writing your own functions:\nAdding the curly brackets - { } - allows you to use functions spanning multiple lines:\n\ntimes_3 &lt;- function(x) {\n  x*3\n}\ntimes_3(x=10)\n\n[1] 30",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#writing-your-own-functions-return",
    "href": "modules/Module04-Functions.html#writing-your-own-functions-return",
    "title": "Module 4: Functions",
    "section": "Writing your own functions: return",
    "text": "Writing your own functions: return\nIf we want something specific for the function’s output, we use return(). Note, if you want to return more than one object, you need to put it into a list using the list() function.\n\ntimes_4 &lt;- function(x) {\n  output &lt;- x * 4\n  return(list(output, x))\n}\ntimes_4(x = 10)\n\n[[1]]\n[1] 40\n\n[[2]]\n[1] 10\n\n\n\nR will “implicitly” return the last evaluated expression by default.\nYou should get in the habit of using “explicit” return statements so that you don’t get unexpected output.",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#function-syntax",
    "href": "modules/Module04-Functions.html#function-syntax",
    "title": "Module 4: Functions",
    "section": "Function Syntax",
    "text": "Function Syntax\nThis is a brief introduction. The syntax is:\nfunctionName = function(inputs) {\n&lt; function body &gt;\nreturn(list(value1, value2))\n}\nNote to create the function for use you need to\n\nCode/type the function\nExecute/run the lines of code\n\nOnly then will the function be available in the Environment pane and ready to use.",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#writing-your-own-functions-multiple-arguments",
    "href": "modules/Module04-Functions.html#writing-your-own-functions-multiple-arguments",
    "title": "Module 4: Functions",
    "section": "Writing your own functions: multiple arguments",
    "text": "Writing your own functions: multiple arguments\nFunctions can take multiple arguments / inputs. Here the function has two arguments x and y\n\ntimes_2_plus_y &lt;- function(x, y) {\n  out &lt;- x * 2 + y\n  return(out)\n}\ntimes_2_plus_y(x = 10, y = 3)\n\n[1] 23",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#writing-your-own-functions-arugment-defaults",
    "href": "modules/Module04-Functions.html#writing-your-own-functions-arugment-defaults",
    "title": "Module 4: Functions",
    "section": "Writing your own functions: arugment defaults",
    "text": "Writing your own functions: arugment defaults\nFunctions can have default arguments. This lets us use the function without specifying the arguments\n\ntimes_2_plus_y &lt;- function(x = 10, y = 3) {\n  out &lt;- x * 2 + y\n  return(out)\n}\ntimes_2_plus_y()\n\n[1] 23\n\n\nWe got an answer b/c we put defaults into the function arguments.",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#writing-a-simple-function",
    "href": "modules/Module04-Functions.html#writing-a-simple-function",
    "title": "Module 4: Functions",
    "section": "Writing a simple function",
    "text": "Writing a simple function\nLet’s write a function, sqdif, that:\n\ntakes two numbers x and y with default values of 2 and 3.\ntakes the difference\nsquares this difference\nthen returns the final value\n\nfunctionName = function(inputs) {\n&lt; function body &gt;\nreturn(list(value1, value2))\n}\n\nsqdif &lt;- function(x=2,y=3){\n     output &lt;- (x-y)^2\n     return(output)\n}\n\nsqdif()\n\n[1] 1\n\nsqdif(x=10,y=5)\n\n[1] 25\n\nsqdif(10,5)\n\n[1] 25\n\n\n\n\nNote that this function is implicitly vectorized even if we didn’t mean for it to be.\n\n\nsqdif(c(2, 3, 4), c(7, 3.14, 98))\n\n[1]   25.0000    0.0196 8836.0000",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#writing-your-own-functions-characters",
    "href": "modules/Module04-Functions.html#writing-your-own-functions-characters",
    "title": "Module 4: Functions",
    "section": "Writing your own functions: characters",
    "text": "Writing your own functions: characters\nFunctions can have any kind of data type input. For example, here is a function with characters:\n\nloud &lt;- function(word) {\n  output &lt;- rep(toupper(word), 5)\n  return(output)\n}\nloud(word = \"hooray!\")\n\n[1] \"HOORAY!\" \"HOORAY!\" \"HOORAY!\" \"HOORAY!\" \"HOORAY!\"",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#the-...-elipsis-argument",
    "href": "modules/Module04-Functions.html#the-...-elipsis-argument",
    "title": "Module 4: Functions",
    "section": "The ... (elipsis) argument",
    "text": "The ... (elipsis) argument\n\nWhat if we want our function to take in an arbitrary amount of numbers?\nAs an example, let’s consider a function that calculates the geometric mean.\n\n\\[\n\\text{geometric mean}(\\mathbf{x}) = \\sqrt[n]{x_1 \\cdot x_2 \\cdot \\ldots \\cdot x_n} = \\exp\\left( \\frac{1}{n}\\left(\\log x_1 + \\ldots + \\log x_n \\right) \\right)\n\\]\n\nWe can do this on a vector, of course.\n\n\ngeo_mean &lt;- function(x) {\n    output &lt;- exp(mean(log(x)))\n    return(output)\n}\n\ngeo_mean(c(2, 3, 4, 5))\n\n[1] 3.309751",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#the-...-elipsis-argument-1",
    "href": "modules/Module04-Functions.html#the-...-elipsis-argument-1",
    "title": "Module 4: Functions",
    "section": "The ... (elipsis) argument",
    "text": "The ... (elipsis) argument\n\nBut sometimes it’s easier to write this a different way.\n\n\ngeo_mean &lt;- function(...) {\n    x &lt;- c(...)\n    output &lt;- exp(mean(log(x)))\n    return(output)\n}\n\ngeo_mean(2, 3, 4, 5)\n\n[1] 3.309751\n\n\n\n... is a placeholder for as many arguments as you want.",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#for-pass-through",
    "href": "modules/Module04-Functions.html#for-pass-through",
    "title": "Module 4: Functions",
    "section": "... for pass-through",
    "text": "... for pass-through\n\nThe most common use of ... is to “pass-through” arguments to underlying functions.\nmean() has additional arguments, what if we want to use those in our geometric mean calculation?\n\n\ngeo_mean &lt;- function(x, trim = 0, na.rm = FALSE) {\n        output &lt;- exp(mean(log(x), trim = trim, na.rm = na.rm))\n    return(output)\n}\n\n\nOr we can do this an easier way.",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#function-debugging-tools-if-you-get-stuck",
    "href": "modules/Module04-Functions.html#function-debugging-tools-if-you-get-stuck",
    "title": "Module 4: Functions",
    "section": "Function debugging tools if you get stuck",
    "text": "Function debugging tools if you get stuck\n\ntraceback()\nbrowser()\ndebug() and debugonce()\nRStudio breakpoints\nLast resort: printing and dumping\nMore info: https://adv-r.hadley.nz/debugging.html",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#function-instead-of-loop",
    "href": "modules/Module04-Functions.html#function-instead-of-loop",
    "title": "Module 4: Functions",
    "section": "Function instead of Loop",
    "text": "Function instead of Loop\nNow, let’s see how we can repeat our loop example from the measles dataset with a function instead.\nHere we are going to set up a function that takes our data frame and outputs the median, first and third quartiles, and range of measles cases for a specified country.\n\nget_country_stats &lt;- function(df, iso3_code){\n    \n    country_data &lt;- subset(df, iso3c == iso3_code)\n    \n    # Get the summary statistics for this country\n    country_cases &lt;- country_data$measles_cases\n    country_quart &lt;- quantile(\n        country_cases, na.rm = TRUE, probs = c(0.25, 0.5, 0.75)\n    )\n    country_range &lt;- range(country_cases, na.rm = TRUE)\n    \n    country_name &lt;- unique(country_data$country)\n    \n    country_summary &lt;- data.frame(\n        country = country_name,\n        min = country_range[[1]],\n        Q1 = country_quart[[1]],\n        median = country_quart[[2]],\n        Q3 = country_quart[[3]],\n        max = country_range[[2]]\n    )\n    \n    return(country_summary)\n}\n\nNow we can use the function.\n\nmeas &lt;- readRDS(here::here(\"data\", \"measles_final.Rds\"))\nget_country_stats(df=meas, iso3_code=\"IND\")\n\n  country  min      Q1  median       Q3    max\n1   India 3305 31135.5 47109.5 80797.25 252940\n\nget_country_stats(df=meas, iso3_code=\"PAK\")\n\n   country min     Q1 median    Q3   max\n1 Pakistan 386 2065.5 4075.5 17422 55543",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#summary",
    "href": "modules/Module04-Functions.html#summary",
    "title": "Module 4: Functions",
    "section": "Summary",
    "text": "Summary\n\nSimple functions take the form:\n\nfunctionName = function(arguments) {\n    &lt; function body &gt;\n    return(list(outputs))\n}\n\nWe can specify arguments defaults when you create the function",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#you-try-it",
    "href": "modules/Module04-Functions.html#you-try-it",
    "title": "Module 4: Functions",
    "section": "You try it!",
    "text": "You try it!\nCreate your own function that calculates the incidence rate per \\(K\\) cases in a given country for all years where that country appeared (e.g. incidence per 10,000 people or another number \\(K\\)).\nStep 1. Determine your arguments.\nStep 2. Begin your function by subsetting the data to include only the country specified in the arguments (i.e, country_data).\nStep 3. Get the years of interest (what errors might you run into here?).\nStep 4. Calculate the incidence rate per \\(K\\) people in each year.\nStep 5. Return the output neatly.\nBonus exercise: allow the country code argument to be optional, or add a new argument that allows the user to only calculate the incidence for certain years.",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#mini-exercise-answer",
    "href": "modules/Module04-Functions.html#mini-exercise-answer",
    "title": "Module 4: Functions",
    "section": "Mini Exercise Answer",
    "text": "Mini Exercise Answer\n\nget_incidence &lt;- function(df, iso3_code, K = 10000){\n    \n    country_data &lt;- subset(df, iso3c == iso3_code)\n    \n    incidence &lt;- country_data$measles_cases / country_data$population * K\n    names(incidence) &lt;- country_data$year\n    \n    return(incidence)\n}\n\nget_incidence(df=meas, iso3_code=\"IND\")\n\n      2023       2022       2021       2020       2019       2018       2017 \n0.45603205 0.28907547 0.04049550 0.04013214 0.07540965 0.14224947 0.08884979 \n      2016       2015       2014       2013       2012       2011       2010 \n0.12886248 0.22805022 0.20294566 0.06416849 0.02593200 0.26744142 0.25356807 \n      2009       2008       2007       2006       2005       2004       2003 \n0.45918728 0.36675830 0.34583747 0.54747898 0.31794361 0.48794093 0.42192914 \n      2002       2001       2000       1999       1998       1997       1996 \n0.36459551 0.47990173 0.36649458 0.20195097 0.33276727 0.60861874 0.47872368 \n      1995       1994       1993       1992       1991       1990       1989 \n0.38882932 0.53443386 0.54397290 0.79109798 0.87586166 1.02948793 1.94564007 \n      1988       1987       1986       1985       1984       1983       1982 \n1.93562738 3.10083364 1.94360299 2.06623051 2.50206072 1.73819235 2.00496602 \n      1981       1980 \n2.83437652 1.63650050 \n\nget_incidence(df=meas, iso3_code=\"PAK\")\n\n      2023       2022       2021       2020       2019       2018       2017 \n0.73692544 0.35526364 0.44939088 0.12090842 0.09252406 1.50215163 0.42402323 \n      2016       2015       2014       2013       2012       2011       2010 \n0.12658949 0.01829650 0.06578580 0.42607889 0.39791131 0.22084288 0.22221137 \n      2009       2008       2007       2006       2005       2004       2003 \n0.04539161 0.06072114 0.15396495 0.42910095 0.17095625 0.24893257 0.28404208 \n      2002       2001       2000       1999       1998       1997       1996 \n0.23906241 0.24174444 0.13370480 0.19640005 0.16036998 0.13075755 0.07942591 \n      1995       1994       1993       1992       1991       1990       1989 \n0.12920918 0.10994611 0.15667487 0.24245113 0.05176020 1.88755151 0.21035120 \n      1988       1987       1986       1985       1984       1983       1982 \n5.14440235 4.41204007 4.20439485 2.74769085 1.84269015 2.30609510 2.26464854 \n      1981       1980 \n3.44249798 3.54397944",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module04-Functions.html#sidenote-combining-functions-and-loops",
    "href": "modules/Module04-Functions.html#sidenote-combining-functions-and-loops",
    "title": "Module 4: Functions",
    "section": "Sidenote: combining functions and loops",
    "text": "Sidenote: combining functions and loops\n\nWriting functions is a way to make complex code a lot easier to understand.\nThese two loops do exactly the same thing, but the one with the function is much easier to read for most people.\n\n\nmy_countries &lt;- c(\"IND\", \"PAK\", \"BGD\", \"NPL\", \"BTN\")\nn &lt;- length(my_countries)\nres &lt;- vector(mode = \"list\", length = n)\n\n\nfor (i in 1:n) {\n    this_country &lt;- my_countries[[i]]\n    country_data &lt;- subset(meas, iso3c == this_country)\n    incidence &lt;- country_data$measles_cases / country_data$population * 10000\n    names(incidence) &lt;- country_data$year\n    res[[i]] &lt;- incidence\n}\nstr(res)\n\nList of 5\n $ : Named num [1:44] 0.456 0.2891 0.0405 0.0401 0.0754 ...\n  ..- attr(*, \"names\")= chr [1:44] \"2023\" \"2022\" \"2021\" \"2020\" ...\n $ : Named num [1:44] 0.7369 0.3553 0.4494 0.1209 0.0925 ...\n  ..- attr(*, \"names\")= chr [1:44] \"2023\" \"2022\" \"2021\" \"2020\" ...\n $ : Named num [1:44] 0.0162 0.0182 0.012 0.1439 0.3521 ...\n  ..- attr(*, \"names\")= chr [1:44] \"2023\" \"2022\" \"2021\" \"2020\" ...\n $ : Named num [1:44] 0.3117 0.0426 0.0476 0.1322 0.1491 ...\n  ..- attr(*, \"names\")= chr [1:44] \"2023\" \"2022\" \"2021\" \"2020\" ...\n $ : Named num [1:44] 0.127 0.0895 NA NA 0.0261 ...\n  ..- attr(*, \"names\")= chr [1:44] \"2023\" \"2022\" \"2021\" \"2020\" ...\n\n\n\nfor (i in 1:n) {\n    res[[i]] &lt;- get_incidence(meas, my_countries[[i]], 10000)\n}\nstr(res)\n\nList of 5\n $ : Named num [1:44] 0.456 0.2891 0.0405 0.0401 0.0754 ...\n  ..- attr(*, \"names\")= chr [1:44] \"2023\" \"2022\" \"2021\" \"2020\" ...\n $ : Named num [1:44] 0.7369 0.3553 0.4494 0.1209 0.0925 ...\n  ..- attr(*, \"names\")= chr [1:44] \"2023\" \"2022\" \"2021\" \"2020\" ...\n $ : Named num [1:44] 0.0162 0.0182 0.012 0.1439 0.3521 ...\n  ..- attr(*, \"names\")= chr [1:44] \"2023\" \"2022\" \"2021\" \"2020\" ...\n $ : Named num [1:44] 0.3117 0.0426 0.0476 0.1322 0.1491 ...\n  ..- attr(*, \"names\")= chr [1:44] \"2023\" \"2022\" \"2021\" \"2020\" ...\n $ : Named num [1:44] 0.127 0.0895 NA NA 0.0261 ...\n  ..- attr(*, \"names\")= chr [1:44] \"2023\" \"2022\" \"2021\" \"2020\" ...",
    "crumbs": [
      "Modules",
      "Writing functions"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#learning-goals",
    "href": "modules/Module06-S3-lm-formulas.html#learning-goals",
    "title": "Module 6: S3 and R Formulas",
    "section": "Learning goals",
    "text": "Learning goals\n\nExplain what S3 is and give an example of an S3 method\nUnderstand the R help pages for S3 methods\nUse vectorization to replace unnecessary loops\nUnderstand when and how to use the R formula syntax\nWrite basic linear models with R formulas",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#objects-and-classes-and-methods",
    "href": "modules/Module06-S3-lm-formulas.html#objects-and-classes-and-methods",
    "title": "Module 6: S3 and R Formulas",
    "section": "Objects and classes and methods",
    "text": "Objects and classes and methods\n\nWe’ve talked about imperative programming and functional programming. Now we’ll talk about object-oriented programming.\nS3 is a “functional OOP” system, which is different from OOP you might have dealt with in Python or C++.\nThe main propery of S3 is polymorphism: one generic function can act differently on different classes of objects, but symbolically represents the same thing and has a consistent interface.",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#an-s3-example",
    "href": "modules/Module06-S3-lm-formulas.html#an-s3-example",
    "title": "Module 6: S3 and R Formulas",
    "section": "An S3 example",
    "text": "An S3 example\n\nsummary() is an example of an S3 generic. And the documentation tells us this.\n\n\n?summary\n\nObject Summaries\n\nDescription:\n\n     'summary' is a generic function used to produce result summaries\n     of the results of various model fitting functions.  The function\n     invokes particular 'methods' which depend on the 'class' of the\n     first argument.\n\nUsage:\n\n     summary(object, ...)\n     \n     ## Default S3 method:\n     summary(object, ..., digits, quantile.type = 7)\n     ## S3 method for class 'data.frame'\n     summary(object, maxsum = 7,\n            digits = max(3, getOption(\"digits\")-3), ...)\n     \n     ## S3 method for class 'factor'\n     summary(object, maxsum = 100, ...)\n     \n     ## S3 method for class 'matrix'\n     summary(object, ...)\n     \n     ## S3 method for class 'summaryDefault'\n     format(x, digits = max(3L, getOption(\"digits\") - 3L), zdigits = 4L, ...)\n      ## S3 method for class 'summaryDefault'\n     print(x, digits = max(3L, getOption(\"digits\") - 3L), zdigits = 4L, ...)\n     \nArguments:\n\n  object: an object for which a summary is desired.\n\n       x: a result of the _default_ method of 'summary()'.\n\n  maxsum: integer, indicating how many levels should be shown for\n          'factor's.\n\n  digits: integer (or 'NULL', see 'Details'), used for number\n          formatting with 'signif()' (for 'summary.default') or\n          'format()' (for 'summary.data.frame').  In 'summary.default',\n          if not specified (i.e., 'missing(.)'), 'signif()' will _not_\n          be called anymore (since R &gt;= 3.4.0, where the default has\n          been changed to only round in the 'print' and 'format'\n          methods).\n\n zdigits: integer, typically positive to be used in the internal\n          'zapsmall(*, digits = digits + zdigits)' call.\n\nquantile.type: integer code used in 'quantile(*, type=quantile.type)'\n          for the default method.\n\n     ...: additional arguments affecting the summary produced.\n\nDetails:\n\n     For 'factor's, the frequency of the first 'maxsum - 1' most\n     frequent levels is shown, and the less frequent levels are\n     summarized in '\"(Others)\"' (resulting in at most 'maxsum'\n     frequencies).\n\n     The 'digits' argument may be 'NULL' for some methods specifying to\n     use the default value, e.g., for the '\"summaryDefault\"' 'format()'\n     method.\n\n     The functions 'summary.lm' and 'summary.glm' are examples of\n     particular methods which summarize the results produced by 'lm'\n     and 'glm'.\n\nValue:\n\n     The form of the value returned by 'summary' depends on the class\n     of its argument.  See the documentation of the particular methods\n     for details of what is produced by that method.\n\n     The default method returns an object of class 'c(\"summaryDefault\",\n     \"table\")' which has specialized 'format' and 'print' methods.  The\n     'factor' method returns an integer vector.\n\n     The matrix and data frame methods return a matrix of class\n     '\"table\"', obtained by applying 'summary' to each column and\n     collating the results.\n\nReferences:\n\n     Chambers, J. M. and Hastie, T. J. (1992) _Statistical Models in\n     S_.  Wadsworth & Brooks/Cole.\n\nSee Also:\n\n     'anova', 'summary.glm', 'summary.lm'.\n\nExamples:\n\n     summary(attenu, digits = 4) #-&gt; summary.data.frame(...), default precision\n     summary(attenu $ station, maxsum = 20) #-&gt; summary.factor(...)\n     \n     lst &lt;- unclass(attenu$station) &gt; 20 # logical with NAs\n     ## summary.default() for logicals -- different from *.factor:\n     summary(lst)\n     summary(as.factor(lst))\n     \n     ## show the effect of zdigits for skewed data\n     set.seed(17); x &lt;- rlnorm(100, sdlog=2)\n     dput(sx &lt;- summary(x))\n     sx # exponential format for this data\n     print(sx, zdigits = 3) # fixed point: \"more readable\"",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#polymorphism-of-summary",
    "href": "modules/Module06-S3-lm-formulas.html#polymorphism-of-summary",
    "title": "Module 6: S3 and R Formulas",
    "section": "Polymorphism of summary",
    "text": "Polymorphism of summary\n\nWhen we call summary(an_object) it determines what it should do based on the class of an_object.\nEach of the following summaries shows us different statistics.\n\n\nclass(an_lm)\n\n[1] \"lm\"\n\nsummary(an_lm)\n\n\nCall:\nlm(formula = bwt ~ age, data = birthwt)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2294.78  -517.63    10.51   530.80  1774.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2655.74     238.86   11.12   &lt;2e-16 ***\nage            12.43      10.02    1.24    0.216    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 728.2 on 187 degrees of freedom\nMultiple R-squared:  0.008157,  Adjusted R-squared:  0.002853 \nF-statistic: 1.538 on 1 and 187 DF,  p-value: 0.2165\n\n\n\n\nclass(a_glm)\n\n[1] \"glm\" \"lm\" \n\nsummary(a_glm)\n\n\nCall:\nglm(formula = low ~ age, family = \"binomial\", data = birthwt)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  0.38458    0.73212   0.525    0.599\nage         -0.05115    0.03151  -1.623    0.105\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 234.67  on 188  degrees of freedom\nResidual deviance: 231.91  on 187  degrees of freedom\nAIC: 235.91\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nclass(a_glmm)\n\n[1] \"glmerMod\"\nattr(,\"package\")\n[1] \"lme4\"\n\nsummary(a_glmm)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: low ~ age + (1 | id)\n   Data: birthwt\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n    237.9     247.6    -116.0     231.9       186 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-0.8451 -0.7066 -0.5908  1.3042  1.9639 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id     (Intercept) 0.009912 0.09956 \nNumber of obs: 189, groups:  id, 189\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  0.38540    0.73534   0.524    0.600\nage         -0.05127    0.03234  -1.585    0.113\n\nCorrelation of Fixed Effects:\n    (Intr)\nage -0.966",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#some-oop-definitions",
    "href": "modules/Module06-S3-lm-formulas.html#some-oop-definitions",
    "title": "Module 6: S3 and R Formulas",
    "section": "Some OOP definitions",
    "text": "Some OOP definitions\n\nA class is a category of object. Objects of the same class have the same fields or attributes.\nA generic is a function that has different behavior for multiple classes of inputs.\nThe methods for a function are the function implementations for specific classes.\nMethod dispatch is the process where you call a generic function, the generic function checks the class of the input, and the generic function calls the method for that class.\nA default method is dispatched by a generic function when there is no specific method for the class of the input. Not every generic has a default method.",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#s3-classes",
    "href": "modules/Module06-S3-lm-formulas.html#s3-classes",
    "title": "Module 6: S3 and R Formulas",
    "section": "S3 classes",
    "text": "S3 classes\n\nEverything that exists in R is an object and has a type.\nBut not everything is an S3 object, and only S3 objects have a class.\nThis is confusing, because the function class() will tell you something for every object, no matter what. If an object does not have a class, class() will return the type.\nTo check if an object has an S3 class, you have to check the extremely confusing attr(an_object, \"class\") instead.",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#s3-classes-and-types",
    "href": "modules/Module06-S3-lm-formulas.html#s3-classes-and-types",
    "title": "Module 6: S3 and R Formulas",
    "section": "S3 classes (and types)",
    "text": "S3 classes (and types)\n\nThese objects have types but not classes: NULL, vectors, lists, functions, and some other special ones that you won’t use.\nYou can pretty much pretend that types are classes.",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#s3-classes-and-types-1",
    "href": "modules/Module06-S3-lm-formulas.html#s3-classes-and-types-1",
    "title": "Module 6: S3 and R Formulas",
    "section": "S3 classes (and types)",
    "text": "S3 classes (and types)\n\nan_object &lt;- 1:10\ntypeof(an_object)\n\n[1] \"integer\"\n\nclass(an_object)\n\n[1] \"integer\"\n\nattr(an_object, \"class\")\n\nNULL",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#s3-classes-and-types-2",
    "href": "modules/Module06-S3-lm-formulas.html#s3-classes-and-types-2",
    "title": "Module 6: S3 and R Formulas",
    "section": "S3 classes (and types)",
    "text": "S3 classes (and types)\n\ntypeof(an_lm)\n\n[1] \"list\"\n\nclass(an_lm)\n\n[1] \"lm\"\n\nattr(an_lm, \"class\")\n\n[1] \"lm\"",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#s3-classes-and-types-3",
    "href": "modules/Module06-S3-lm-formulas.html#s3-classes-and-types-3",
    "title": "Module 6: S3 and R Formulas",
    "section": "S3 classes (and types)",
    "text": "S3 classes (and types)\n\nIf an object has an S3 class, then it is a list.\nThe only things that aren’t lists are the other special objects with types.\nComplicated S3 classes are just lists where the elements have specific names.\n\n\nstr(an_lm)\n\nList of 12\n $ coefficients : Named num [1:2] 2655.7 12.4\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"age\"\n $ residuals    : Named num [1:189] -369 -515 -347 -323 -279 ...\n  ..- attr(*, \"names\")= chr [1:189] \"85\" \"86\" \"87\" \"88\" ...\n $ effects      : Named num [1:189] -40481 -903 -340 -309 -284 ...\n  ..- attr(*, \"names\")= chr [1:189] \"(Intercept)\" \"age\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:189] 2892 3066 2904 2917 2879 ...\n  ..- attr(*, \"names\")= chr [1:189] \"85\" \"86\" \"87\" \"88\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:189, 1:2] -13.7477 0.0727 0.0727 0.0727 0.0727 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:189] \"85\" \"86\" \"87\" \"88\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"age\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.07 1.14\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 187\n $ xlevels      : Named list()\n $ call         : language lm(formula = bwt ~ age, data = birthwt)\n $ terms        :Classes 'terms', 'formula'  language bwt ~ age\n  .. ..- attr(*, \"variables\")= language list(bwt, age)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"bwt\" \"age\"\n  .. .. .. ..$ : chr \"age\"\n  .. ..- attr(*, \"term.labels\")= chr \"age\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(bwt, age)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"bwt\" \"age\"\n $ model        :'data.frame':  189 obs. of  2 variables:\n  ..$ bwt: int [1:189] 2523 2551 2557 2594 2600 2622 2637 2637 2663 2665 ...\n  ..$ age: int [1:189] 19 33 20 21 18 21 22 17 29 26 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language bwt ~ age\n  .. .. ..- attr(*, \"variables\")= language list(bwt, age)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"bwt\" \"age\"\n  .. .. .. .. ..$ : chr \"age\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"age\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(bwt, age)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"bwt\" \"age\"\n - attr(*, \"class\")= chr \"lm\"\n\n\n\n\n?lm\n\nFitting Linear Models\n\nDescription:\n\n     'lm' is used to fit linear models, including multivariate ones.\n     It can be used to carry out regression, single stratum analysis of\n     variance and analysis of covariance (although 'aov' may provide a\n     more convenient interface for these).\n\nUsage:\n\n     lm(formula, data, subset, weights, na.action,\n        method = \"qr\", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,\n        singular.ok = TRUE, contrasts = NULL, offset, ...)\n     \n     ## S3 method for class 'lm'\n     print(x, digits = max(3L, getOption(\"digits\") - 3L), ...)\n     \nArguments:\n\n formula: an object of class '\"formula\"' (or one that can be coerced to\n          that class): a symbolic description of the model to be\n          fitted.  The details of model specification are given under\n          'Details'.\n\n    data: an optional data frame, list or environment (or object\n          coercible by 'as.data.frame' to a data frame) containing the\n          variables in the model.  If not found in 'data', the\n          variables are taken from 'environment(formula)', typically\n          the environment from which 'lm' is called.\n\n  subset: an optional vector specifying a subset of observations to be\n          used in the fitting process.  (See additional details about\n          how this argument interacts with data-dependent bases in the\n          'Details' section of the 'model.frame' documentation.)\n\n weights: an optional vector of weights to be used in the fitting\n          process.  Should be 'NULL' or a numeric vector.  If non-NULL,\n          weighted least squares is used with weights 'weights' (that\n          is, minimizing 'sum(w*e^2)'); otherwise ordinary least\n          squares is used.  See also 'Details',\n\nna.action: a function which indicates what should happen when the data\n          contain 'NA's.  The default is set by the 'na.action' setting\n          of 'options', and is 'na.fail' if that is unset.  The\n          'factory-fresh' default is 'na.omit'.  Another possible value\n          is 'NULL', no action.  Value 'na.exclude' can be useful.\n\n  method: the method to be used; for fitting, currently only 'method =\n          \"qr\"' is supported; 'method = \"model.frame\"' returns the\n          model frame (the same as with 'model = TRUE', see below).\n\nmodel, x, y, qr: logicals.  If 'TRUE' the corresponding components of\n          the fit (the model frame, the model matrix, the response, the\n          QR decomposition) are returned.\n\nsingular.ok: logical. If 'FALSE' (the default in S but not in R) a\n          singular fit is an error.\n\ncontrasts: an optional list. See the 'contrasts.arg' of\n          'model.matrix.default'.\n\n  offset: this can be used to specify an _a priori_ known component to\n          be included in the linear predictor during fitting.  This\n          should be 'NULL' or a numeric vector or matrix of extents\n          matching those of the response.  One or more 'offset' terms\n          can be included in the formula instead or as well, and if\n          more than one are specified their sum is used.  See\n          'model.offset'.\n\n     ...: For 'lm()': additional arguments to be passed to the low\n          level regression fitting functions (see below).\n\n  digits: the number of _significant_ digits to be passed to\n          'format(coef(x), .)' when 'print()'ing.\n\nDetails:\n\n     Models for 'lm' are specified symbolically.  A typical model has\n     the form 'response ~ terms' where 'response' is the (numeric)\n     response vector and 'terms' is a series of terms which specifies a\n     linear predictor for 'response'.  A terms specification of the\n     form 'first + second' indicates all the terms in 'first' together\n     with all the terms in 'second' with duplicates removed.  A\n     specification of the form 'first:second' indicates the set of\n     terms obtained by taking the interactions of all terms in 'first'\n     with all terms in 'second'.  The specification 'first*second'\n     indicates the _cross_ of 'first' and 'second'.  This is the same\n     as 'first + second + first:second'.\n\n     If the formula includes an 'offset', this is evaluated and\n     subtracted from the response.\n\n     If 'response' is a matrix a linear model is fitted separately by\n     least-squares to each column of the matrix and the result inherits\n     from '\"mlm\"' (\"multivariate linear model\").\n\n     See 'model.matrix' for some further details.  The terms in the\n     formula will be re-ordered so that main effects come first,\n     followed by the interactions, all second-order, all third-order\n     and so on: to avoid this pass a 'terms' object as the formula (see\n     'aov' and 'demo(glm.vr)' for an example).\n\n     A formula has an implied intercept term.  To remove this use\n     either 'y ~ x - 1' or 'y ~ 0 + x'.  See 'formula' for more details\n     of allowed formulae.\n\n     Non-'NULL' 'weights' can be used to indicate that different\n     observations have different variances (with the values in\n     'weights' being inversely proportional to the variances); or\n     equivalently, when the elements of 'weights' are positive integers\n     w_i, that each response y_i is the mean of w_i unit-weight\n     observations (including the case that there are w_i observations\n     equal to y_i and the data have been summarized). However, in the\n     latter case, notice that within-group variation is not used.\n     Therefore, the sigma estimate and residual degrees of freedom may\n     be suboptimal; in the case of replication weights, even wrong.\n     Hence, standard errors and analysis of variance tables should be\n     treated with care.\n\n     'lm' calls the lower level functions 'lm.fit', etc, see below, for\n     the actual numerical computations.  For programming only, you may\n     consider doing likewise.\n\n     All of 'weights', 'subset' and 'offset' are evaluated in the same\n     way as variables in 'formula', that is first in 'data' and then in\n     the environment of 'formula'.\n\nValue:\n\n     'lm' returns an object of 'class' '\"lm\"' or for multivariate\n     ('multiple') responses of class 'c(\"mlm\", \"lm\")'.\n\n     The functions 'summary' and 'anova' are used to obtain and print a\n     summary and analysis of variance table of the results.  The\n     generic accessor functions 'coefficients', 'effects',\n     'fitted.values' and 'residuals' extract various useful features of\n     the value returned by 'lm'.\n\n     An object of class '\"lm\"' is a list containing at least the\n     following components:\n\ncoefficients: a named vector of coefficients\n\nresiduals: the residuals, that is response minus fitted values.\n\nfitted.values: the fitted mean values.\n\n    rank: the numeric rank of the fitted linear model.\n\n weights: (only for weighted fits) the specified weights.\n\ndf.residual: the residual degrees of freedom.\n\n    call: the matched call.\n\n   terms: the 'terms' object used.\n\ncontrasts: (only where relevant) the contrasts used.\n\n xlevels: (only where relevant) a record of the levels of the factors\n          used in fitting.\n\n  offset: the offset used (missing if none were used).\n\n       y: if requested, the response used.\n\n       x: if requested, the model matrix used.\n\n   model: if requested (the default), the model frame used.\n\nna.action: (where relevant) information returned by 'model.frame' on\n          the special handling of 'NA's.\n\n     In addition, non-null fits will have components 'assign',\n     'effects' and (unless not requested) 'qr' relating to the linear\n     fit, for use by extractor functions such as 'summary' and\n     'effects'.\n\nUsing time series:\n\n     Considerable care is needed when using 'lm' with time series.\n\n     Unless 'na.action = NULL', the time series attributes are stripped\n     from the variables before the regression is done.  (This is\n     necessary as omitting 'NA's would invalidate the time series\n     attributes, and if 'NA's are omitted in the middle of the series\n     the result would no longer be a regular time series.)\n\n     Even if the time series attributes are retained, they are not used\n     to line up series, so that the time shift of a lagged or\n     differenced regressor would be ignored.  It is good practice to\n     prepare a 'data' argument by 'ts.intersect(..., dframe = TRUE)',\n     then apply a suitable 'na.action' to that data frame and call 'lm'\n     with 'na.action = NULL' so that residuals and fitted values are\n     time series.\n\nAuthor(s):\n\n     The design was inspired by the S function of the same name\n     described in Chambers (1992).  The implementation of model formula\n     by Ross Ihaka was based on Wilkinson & Rogers (1973).\n\nReferences:\n\n     Chambers, J. M. (1992) _Linear models._ Chapter 4 of _Statistical\n     Models in S_ eds J. M. Chambers and T. J. Hastie, Wadsworth &\n     Brooks/Cole.\n\n     Wilkinson, G. N. and Rogers, C. E. (1973).  Symbolic descriptions\n     of factorial models for analysis of variance.  _Applied\n     Statistics_, *22*, 392-399.  doi:10.2307/2346786\n     &lt;https://doi.org/10.2307/2346786&gt;.\n\nSee Also:\n\n     'summary.lm' for more detailed summaries and 'anova.lm' for the\n     ANOVA table; 'aov' for a different interface.\n\n     The generic functions 'coef', 'effects', 'residuals', 'fitted',\n     'vcov'.\n\n     'predict.lm' (via 'predict') for prediction, including confidence\n     and prediction intervals; 'confint' for confidence intervals of\n     _parameters_.\n\n     'lm.influence' for regression diagnostics, and 'glm' for\n     *generalized* linear models.\n\n     The underlying low level functions, 'lm.fit' for plain, and\n     'lm.wfit' for weighted regression fitting.\n\n     More 'lm()' examples are available e.g., in 'anscombe',\n     'attitude', 'freeny', 'LifeCycleSavings', 'longley', 'stackloss',\n     'swiss'.\n\n     'biglm' in package 'biglm' for an alternative way to fit linear\n     models to large datasets (especially those with many cases).\n\nExamples:\n\n     require(graphics)\n     \n     ## Annette Dobson (1990) \"An Introduction to Generalized Linear Models\".\n     ## Page 9: Plant Weight Data.\n     ctl &lt;- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)\n     trt &lt;- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)\n     group &lt;- gl(2, 10, 20, labels = c(\"Ctl\",\"Trt\"))\n     weight &lt;- c(ctl, trt)\n     lm.D9 &lt;- lm(weight ~ group)\n     lm.D90 &lt;- lm(weight ~ group - 1) # omitting intercept\n     \n     anova(lm.D9)\n     summary(lm.D90)\n     \n     opar &lt;- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))\n     plot(lm.D9, las = 1)      # Residuals, Fitted, ...\n     par(opar)\n     \n     ### less simple examples in \"See Also\" above",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#s3-help-pages",
    "href": "modules/Module06-S3-lm-formulas.html#s3-help-pages",
    "title": "Module 6: S3 and R Formulas",
    "section": "S3 help pages",
    "text": "S3 help pages\n\nNow you know enough to understand the help pages for S3 objects.\n\n\n?summary\n\nObject Summaries\n\nDescription:\n\n     'summary' is a generic function used to produce result summaries\n     of the results of various model fitting functions.  The function\n     invokes particular 'methods' which depend on the 'class' of the\n     first argument.\n\nUsage:\n\n     summary(object, ...)\n     \n     ## Default S3 method:\n     summary(object, ..., digits, quantile.type = 7)\n     ## S3 method for class 'data.frame'\n     summary(object, maxsum = 7,\n            digits = max(3, getOption(\"digits\")-3), ...)\n     \n     ## S3 method for class 'factor'\n     summary(object, maxsum = 100, ...)\n     \n     ## S3 method for class 'matrix'\n     summary(object, ...)\n     \n     ## S3 method for class 'summaryDefault'\n     format(x, digits = max(3L, getOption(\"digits\") - 3L), zdigits = 4L, ...)\n      ## S3 method for class 'summaryDefault'\n     print(x, digits = max(3L, getOption(\"digits\") - 3L), zdigits = 4L, ...)\n     \nArguments:\n\n  object: an object for which a summary is desired.\n\n       x: a result of the _default_ method of 'summary()'.\n\n  maxsum: integer, indicating how many levels should be shown for\n          'factor's.\n\n  digits: integer (or 'NULL', see 'Details'), used for number\n          formatting with 'signif()' (for 'summary.default') or\n          'format()' (for 'summary.data.frame').  In 'summary.default',\n          if not specified (i.e., 'missing(.)'), 'signif()' will _not_\n          be called anymore (since R &gt;= 3.4.0, where the default has\n          been changed to only round in the 'print' and 'format'\n          methods).\n\n zdigits: integer, typically positive to be used in the internal\n          'zapsmall(*, digits = digits + zdigits)' call.\n\nquantile.type: integer code used in 'quantile(*, type=quantile.type)'\n          for the default method.\n\n     ...: additional arguments affecting the summary produced.\n\nDetails:\n\n     For 'factor's, the frequency of the first 'maxsum - 1' most\n     frequent levels is shown, and the less frequent levels are\n     summarized in '\"(Others)\"' (resulting in at most 'maxsum'\n     frequencies).\n\n     The 'digits' argument may be 'NULL' for some methods specifying to\n     use the default value, e.g., for the '\"summaryDefault\"' 'format()'\n     method.\n\n     The functions 'summary.lm' and 'summary.glm' are examples of\n     particular methods which summarize the results produced by 'lm'\n     and 'glm'.\n\nValue:\n\n     The form of the value returned by 'summary' depends on the class\n     of its argument.  See the documentation of the particular methods\n     for details of what is produced by that method.\n\n     The default method returns an object of class 'c(\"summaryDefault\",\n     \"table\")' which has specialized 'format' and 'print' methods.  The\n     'factor' method returns an integer vector.\n\n     The matrix and data frame methods return a matrix of class\n     '\"table\"', obtained by applying 'summary' to each column and\n     collating the results.\n\nReferences:\n\n     Chambers, J. M. and Hastie, T. J. (1992) _Statistical Models in\n     S_.  Wadsworth & Brooks/Cole.\n\nSee Also:\n\n     'anova', 'summary.glm', 'summary.lm'.\n\nExamples:\n\n     summary(attenu, digits = 4) #-&gt; summary.data.frame(...), default precision\n     summary(attenu $ station, maxsum = 20) #-&gt; summary.factor(...)\n     \n     lst &lt;- unclass(attenu$station) &gt; 20 # logical with NAs\n     ## summary.default() for logicals -- different from *.factor:\n     summary(lst)\n     summary(as.factor(lst))\n     \n     ## show the effect of zdigits for skewed data\n     set.seed(17); x &lt;- rlnorm(100, sdlog=2)\n     dput(sx &lt;- summary(x))\n     sx # exponential format for this data\n     print(sx, zdigits = 3) # fixed point: \"more readable\"",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#finding-classes-and-finding-methods",
    "href": "modules/Module06-S3-lm-formulas.html#finding-classes-and-finding-methods",
    "title": "Module 6: S3 and R Formulas",
    "section": "Finding classes and finding methods",
    "text": "Finding classes and finding methods\n\nUse the methods() function to find all methods that are available for a specific generic function.\n\n\nmethods(summary)\n\n [1] summary,ANY-method                  summary,diagonalMatrix-method      \n [3] summary,sparseMatrix-method         summary.allFit*                    \n [5] summary.aov                         summary.aovlist*                   \n [7] summary.aspell*                     summary.check_packages_in_dir*     \n [9] summary.connection                  summary.corAR1*                    \n[11] summary.corARMA*                    summary.corCAR1*                   \n[13] summary.corCompSymm*                summary.corExp*                    \n[15] summary.corGaus*                    summary.corIdent*                  \n[17] summary.corLin*                     summary.corNatural*                \n[19] summary.corRatio*                   summary.corSpher*                  \n[21] summary.corStruct*                  summary.corSymm*                   \n[23] summary.data.frame                  summary.Date                       \n[25] summary.default                     summary.difftime                   \n[27] summary.ecdf*                       summary.factor                     \n[29] summary.glm                         summary.gls*                       \n[31] summary.infl*                       summary.lm                         \n[33] summary.lme*                        summary.lmList*                    \n[35] summary.lmList4*                    summary.loess*                     \n[37] summary.loglm*                      summary.manova                     \n[39] summary.matrix                      summary.merMod*                    \n[41] summary.mlm*                        summary.modelStruct*               \n[43] summary.negbin*                     summary.nls*                       \n[45] summary.nlsList*                    summary.packageStatus*             \n[47] summary.pdBlocked*                  summary.pdCompSymm*                \n[49] summary.pdDiag*                     summary.pdIdent*                   \n[51] summary.pdLogChol*                  summary.pdMat*                     \n[53] summary.pdNatural*                  summary.pdSymm*                    \n[55] summary.polr*                       summary.POSIXct                    \n[57] summary.POSIXlt                     summary.ppr*                       \n[59] summary.prcomp*                     summary.prcomplist*                \n[61] summary.princomp*                   summary.proc_time                  \n[63] summary.reStruct*                   summary.rlang:::list_of_conditions*\n[65] summary.rlang_error*                summary.rlang_message*             \n[67] summary.rlang_trace*                summary.rlang_warning*             \n[69] summary.rlm*                        summary.shingle*                   \n[71] summary.srcfile                     summary.srcref                     \n[73] summary.stepfun                     summary.stl*                       \n[75] summary.summary.merMod*             summary.table                      \n[77] summary.trellis*                    summary.tukeysmooth*               \n[79] summary.varComb*                    summary.varConstPower*             \n[81] summary.varConstProp*               summary.varExp*                    \n[83] summary.varFixed*                   summary.varFunc*                   \n[85] summary.varIdent*                   summary.varPower*                  \n[87] summary.warnings                   \nsee '?methods' for accessing help and source code\n\n\n\n\nAlso use the methods(class = ...) function to find all methods that are available for a specific class.\n\n\nmethods(class = \"lm\")\n\n [1] add1           alias          anova          case.names     coerce        \n [6] confint        cooks.distance deviance       dfbeta         dfbetas       \n[11] drop1          dummy.coef     effects        extractAIC     family        \n[16] formula        hatvalues      influence      initialize     kappa         \n[21] labels         logLik         model.frame    model.matrix   nobs          \n[26] plot           predict        print          proj           qqnorm        \n[31] qr             residuals      rstandard      rstudent       show          \n[36] simulate       slotsFromS3    summary        variable.names vcov          \nsee '?methods' for accessing help and source code\n\n\n\n\n\nYou can use a dot (.) to reference specific methods, not just generics.\n\n\n?summary.lm\n\nSummarizing Linear Model Fits\n\nDescription:\n\n     'summary' method for class '\"lm\"'.\n\nUsage:\n\n     ## S3 method for class 'lm'\n     summary(object, correlation = FALSE, symbolic.cor = FALSE, ...)\n     \n     ## S3 method for class 'summary.lm'\n     print(x, digits = max(3, getOption(\"digits\") - 3),\n           symbolic.cor = x$symbolic.cor,\n           signif.stars = getOption(\"show.signif.stars\"), ...)\n     \nArguments:\n\n  object: an object of class '\"lm\"', usually, a result of a call to\n          'lm'.\n\n       x: an object of class '\"summary.lm\"', usually, a result of a\n          call to 'summary.lm'.\n\ncorrelation: logical; if 'TRUE', the correlation matrix of the\n          estimated parameters is returned and printed.\n\n  digits: the number of significant digits to use when printing.\n\nsymbolic.cor: logical. If 'TRUE', print the correlations in a symbolic\n          form (see 'symnum') rather than as numbers.\n\nsignif.stars: logical. If 'TRUE', 'significance stars' are printed for\n          each coefficient.\n\n     ...: further arguments passed to or from other methods.\n\nDetails:\n\n     'print.summary.lm' tries to be smart about formatting the\n     coefficients, standard errors, etc. and additionally gives\n     'significance stars' if 'signif.stars' is 'TRUE'.\n\n     Aliased coefficients are omitted in the returned object but\n     restored by the 'print' method.\n\n     Correlations are printed to two decimal places (or symbolically):\n     to see the actual correlations print 'summary(object)$correlation'\n     directly.\n\nValue:\n\n     The function 'summary.lm' computes and returns a list of summary\n     statistics of the fitted linear model given in 'object', using the\n     components (list elements) '\"call\"' and '\"terms\"' from its\n     argument, plus\n\nresiduals: the _weighted_ residuals, the usual residuals rescaled by\n          the square root of the weights specified in the call to 'lm'.\n\ncoefficients: a p x 4 matrix with columns for the estimated\n          coefficient, its standard error, t-statistic and\n          corresponding (two-sided) p-value.  Aliased coefficients are\n          omitted.\n\n aliased: named logical vector showing if the original coefficients are\n          aliased.\n\n   sigma: the square root of the estimated variance of the random error\n\n                       sigma^2 = 1/(n-p) Sum(w[i] R[i]^2),              \n          \n          where R[i] is the i-th residual, 'residuals[i]'.\n\n      df: degrees of freedom, a 3-vector (p, n-p, p*), the first being\n          the number of non-aliased coefficients, the last being the\n          total number of coefficients.\n\nfstatistic: (for models including non-intercept terms) a 3-vector with\n          the value of the F-statistic with its numerator and\n          denominator degrees of freedom.\n\nr.squared: R^2, the 'fraction of variance explained by the model',\n\n                    R^2 = 1 - Sum(R[i]^2) / Sum((y[i]- y*)^2),          \n          \n          where y* is the mean of y[i] if there is an intercept and\n          zero otherwise.\n\nadj.r.squared: the above R^2 statistic '_adjusted_', penalizing for\n          higher p.\n\ncov.unscaled: a p x p matrix of (unscaled) covariances of the coef[j],\n          j=1, ..., p.\n\ncorrelation: the correlation matrix corresponding to the above\n          'cov.unscaled', if 'correlation = TRUE' is specified.\n\nsymbolic.cor: (only if 'correlation' is true.)  The value of the\n          argument 'symbolic.cor'.\n\nna.action: from 'object', if present there.\n\nSee Also:\n\n     The model fitting function 'lm', 'summary'.\n\n     Function 'coef' will extract the matrix of coefficients with\n     standard errors, t-statistics and p-values.\n\nExamples:\n\n     ##-- Continuing the  lm(.) example:\n     coef(lm.D90)  # the bare coefficients\n     sld90 &lt;- summary(lm.D90 &lt;- lm(weight ~ group -1))  # omitting intercept\n     sld90\n     coef(sld90)  # much more\n     \n     ## model with *aliased* coefficient:\n     lm.D9. &lt;- lm(weight ~ group + I(group != \"Ctl\"))\n     Sm.D9. &lt;- summary(lm.D9.)\n     Sm.D9. #  shows the NA NA NA NA  line\n     stopifnot(length(cc &lt;- coef(lm.D9.)) == 3, is.na(cc[3]),\n               dim(coef(Sm.D9.)) == c(2,4), Sm.D9.$df == c(2, 18, 3))",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#utility-of-s3-methods",
    "href": "modules/Module06-S3-lm-formulas.html#utility-of-s3-methods",
    "title": "Module 6: S3 and R Formulas",
    "section": "Utility of S3 methods",
    "text": "Utility of S3 methods\n\nHopefully you can now see why understanding S3 methods is important.\nBut if you aren’t convinced, maybe formula objects can convince you. Look how many methods there are!\n\n\nmethods(class = \"formula\")\n\n [1] [             aggregate     alias         all.equal     ansari.test  \n [6] barplot       bartlett.test boxplot       cdplot        coerce       \n[11] cor.test      deriv         deriv3        fligner.test  formula      \n[16] friedman.test ftable        getInitial    head          initialize   \n[21] kruskal.test  ks.test       lines         mood.test     mosaicplot   \n[26] pairs         plot          points        ppr           prcomp       \n[31] princomp      print         quade.test    selfStart     show         \n[36] simulate      slotsFromS3   spineplot     stripchart    sunflowerplot\n[41] t.test        terms         text          update        var.test     \n[46] wilcox.test  \nsee '?methods' for accessing help and source code\n\n\n\n\nAnd that doesn’t include functions like lm() which ONLY have methods for formulas, not generics.",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#formula-example",
    "href": "modules/Module06-S3-lm-formulas.html#formula-example",
    "title": "Module 6: S3 and R Formulas",
    "section": "Formula example",
    "text": "Formula example\n\nSay you want to get the correlation between MCV1 coverage rate and measles cases in Pakistan. You can call the cor.test() function by passing two vectors.\n\n\nmeas &lt;- readr::read_rds(here::here(\"data\", \"measles_final.Rds\"))\n\ncor.test(\n    x = meas$MCV1_coverage[meas$iso3c == \"PAK\"],\n    y = meas$measles_cases[meas$iso3c == \"PAK\"]\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  meas$MCV1_coverage[meas$iso3c == \"PAK\"] and meas$measles_cases[meas$iso3c == \"PAK\"]\nt = -3.2036, df = 41, p-value = 0.002627\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.6591955 -0.1699300\nsample estimates:\n       cor \n-0.4474377 \n\n\n\n\nBut if we inspect the methods for cor.test, we see it is an S3 generic with a method for formula class objects.\n\n\nmethods(cor.test)\n\n[1] cor.test.default* cor.test.formula*\nsee '?methods' for accessing help and source code\n\n\n\n\n\nTyping in the code is easier, and reading the output is easier.\n\n\ncor.test(\n    ~ measles_cases + MCV1_coverage,\n    data = subset(meas, iso3c == \"PAK\")\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  measles_cases and MCV1_coverage\nt = -3.2036, df = 41, p-value = 0.002627\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.6591955 -0.1699300\nsample estimates:\n       cor \n-0.4474377 \n\n\n\n\n\nBut it pays to understand S3, because in certain circumstances, using the default method is easier!",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#writing-formulas",
    "href": "modules/Module06-S3-lm-formulas.html#writing-formulas",
    "title": "Module 6: S3 and R Formulas",
    "section": "Writing formulas",
    "text": "Writing formulas\n\nThe formula language is a smaller language inside of R used for writing statistical models, and the full details are in the ?formula documentation.\nThis little formula language is so useful it is now used in many textbooks and other languages (Python and Julia).\nThe basic syntax is dv ~ iv1 + iv2 + ....\nRecall that in cor.test() our formula didn’t have a LHS – there is no dependent variable in correlation.",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#you-try-it",
    "href": "modules/Module06-S3-lm-formulas.html#you-try-it",
    "title": "Module 6: S3 and R Formulas",
    "section": "You try it!",
    "text": "You try it!\n\nBefore, we saw that boxplot had a formula method. Use boxplot() with a formula to make boxplots of measles cases in 2002 for each region in the dataset.\n\n\n\nHint: your code should look like this for the correct DV and IV.\n\n\nboxplot(dv ~ iv, data = subset(meas, year == 2002))\n\n\n\n\nSolution:\n\n\nboxplot(measles_cases ~ region, data = subset(meas, year == 2002))",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#you-try-it-more-advanced",
    "href": "modules/Module06-S3-lm-formulas.html#you-try-it-more-advanced",
    "title": "Module 6: S3 and R Formulas",
    "section": "You try it (more advanced)",
    "text": "You try it (more advanced)\n\nLoad the QCRC dataset, you only need the sheet “Main_Dataset” for this.\nFit a logistic regression model with 30 day mortality (30D_Mortality in the spreadsheet) as the outcome. Include any continuous or categorical predictors you want, we recommend avoiding variables that are (supposed to be) date/times.\nUse appropriate S3 generic functions to get statistics about your model that you would want to report.\n\n\n\nHint: to read in the data, do this.\nHint: you might need to do some data cleaning, because missing data are written as a period in the spreadsheet.\n\n\nqcrc &lt;- readxl::read_xlsx(\n    here::here(\"data\", \"QCRC_FINAL_Deidentified.xlsx\"),\n    sheet = \"Main_Dataset\"\n)\n\n\n\n\nHint: your logistic regression code should look like this.\nHint: you need to put backticks (this thing: `, NOT an apostrophe. It’s made by the ~ key on american keyboards.) around the name of the variable `30D_Mortality`.\n\n\nmodel_30dm &lt;- glm(\n    something ~ something + something + ...,\n    data = qcrc,\n    family = \"binomial\"\n)\n\n\n\n\nHint: some useful functions with methods for glm-class objects are summary, coef, and confint. (You probably also need the function exp!)\n\n\n\n\nOne solution, you could have used any predictors you want, though you should be able to explain why they might affect 30 Day mortality.\n\n\n# A little cleaning for D Dimer\n# First turn the \".\" into actual NA's and make it a real number\nqcrc$d_dimer &lt;- as.numeric(dplyr::na_if(qcrc$`D dimer`, \".\"))\n# Take the log cause it's skewed and positive\nqcrc$d_dimer &lt;- log(qcrc$d_dimer)\n# Impute missing values with the median -- there are much better ways to do this\nmed_d_dimer &lt;- median(qcrc$d_dimer, na.rm = FALSE)\nqcrc$d_dimer &lt;- ifelse(is.na(qcrc$d_dimer), med_d_dimer, qcrc$d_dimer)\n\n# A little cleaning for Race\nqcrc$Race &lt;- factor(qcrc$Race) |&gt;\n    # Make White the reference level\n    relevel(ref = \"Caucasian or White\") |&gt;\n    # Groups other than \"Causian or White\" and \"Black or African American\" are\n    # really small and heterogeneous so put them together\n    # Not ideal but sometimes the best we can do\n    forcats::fct_lump_n(n = 2)\n\n# Fit a model that controls for treatment, d_dimer, and race/ethnicity\nmodel_30dm &lt;- glm(\n    `30D_Mortality` ~ Remdesivir_or_placebo + d_dimer + Race,\n    data = qcrc,\n    family = \"binomial\"\n)\n\n\n\n\nLook at a lot of useful information with summary().\n\n\nsummary(model_30dm)\n\n\nCall:\nglm(formula = `30D_Mortality` ~ Remdesivir_or_placebo + d_dimer + \n    Race, family = \"binomial\", data = qcrc)\n\nCoefficients:\n                               Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)                     -2.4352     0.8190  -2.973  0.00295 **\nRemdesivir_or_placebo           -0.7004     0.4105  -1.706  0.08793 . \nd_dimer                          0.1848     0.1015   1.820  0.06871 . \nRaceAfrican American  or Black   0.2738     0.3899   0.702  0.48260   \nRaceOther                        0.7733     0.4990   1.550  0.12119   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 317.14  on 256  degrees of freedom\nResidual deviance: 308.52  on 252  degrees of freedom\n  (31 observations deleted due to missingness)\nAIC: 318.52\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nGet the OR’s with coef().\n\n\ncoef(model_30dm) |&gt;\n    exp() |&gt;\n    round(2)\n\n                   (Intercept)          Remdesivir_or_placebo \n                          0.09                           0.50 \n                       d_dimer RaceAfrican American  or Black \n                          1.20                           1.31 \n                     RaceOther \n                          2.17 \n\n\n\n\n\nGet the profile CI’s with confint().\n\n\nconfint(model_30dm) |&gt;\n    exp() |&gt;\n    round(2)\n\nWaiting for profiling to be done...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n0.02\n0.43\n\n\nRemdesivir_or_placebo\n0.21\n1.07\n\n\nd_dimer\n0.99\n1.47\n\n\nRaceAfrican American or Black\n0.63\n2.93\n\n\nRaceOther\n0.82\n5.86\n\n\n\n\n\n\n\n\nA basic interpretation of the coefficient for Remdesivir_or_placebo (coded as 1 for remdesivir and 0 for placebo) might be “Odds of mortality after 30 days in the remdesivisir group were 0.48 times the odds in the placebo group, for patients with the same D dimer level and race/ethnicity. The 95% CI for the odds ratio was 0.20 to 1.04. The effect was not significant at the \\(\\alpha = 0.05\\) significance group, but is suggestive that at least some patients benefitted from remdesivir treatment.”",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Module06-S3-lm-formulas.html#summary",
    "href": "modules/Module06-S3-lm-formulas.html#summary",
    "title": "Module 6: S3 and R Formulas",
    "section": "Summary",
    "text": "Summary\n\nWe learned the basics of “functional OOP” and specifically R’s S3 system, including how to check classes and find S3 methods for a class or classes for an S3 method.\nWe learned the basics of R formulas, a consistent and powerful sub-language in R for writing statistical models.\nThere’s a lot more to formulas, but we’ll discuss them more in a separate module based on interest/time.\nIf you want to read more about S3 or OOP in R, check out the OOP section in Wickham’s Advanced R.",
    "crumbs": [
      "Modules",
      "S3 and Formulas"
    ]
  },
  {
    "objectID": "modules/Walkthrough2.html#a-whole-paper-using-r-and-quarto",
    "href": "modules/Walkthrough2.html#a-whole-paper-using-r-and-quarto",
    "title": "Data science walkthrough 2",
    "section": "A whole paper using R and quarto",
    "text": "A whole paper using R and quarto\n\nFor this data science walkthough, I wanted to briefly walkthrough a paper I wrote a published that is done almost 100% in R and Quarto.\nThe analyses use a lot of tools that we’ve talked about, so we’ll discuss how these can be useful.\nPlease ask any questions you have!\nYou can see the public code for the paper here: https://github.com/ahgroup/billings-high-dose-heterologous-public/."
  },
  {
    "objectID": "modules/linear-models.html",
    "href": "modules/linear-models.html",
    "title": "Case study: Linear Models and Linear Monsters",
    "section": "",
    "text": "Fit basic linear and generalized linear models including models for counts.\nFit GLMs with interaction terms.\nFit GLMs with random effects using lme4.\n\nOptional topics: 1. Fit GLMs with spline terms using mgcv. 1. Fit binomial and overdispersed binomial models, and compare them to bernoulli models on the same data. 1. Understand the limitations of frequentist linear models and know that brms exists.",
    "crumbs": [
      "Case studies / advanced topics",
      "Advanced linear models"
    ]
  },
  {
    "objectID": "modules/linear-models.html#learning-goals",
    "href": "modules/linear-models.html#learning-goals",
    "title": "Case study: Linear Models and Linear Monsters",
    "section": "",
    "text": "Fit basic linear and generalized linear models including models for counts.\nFit GLMs with interaction terms.\nFit GLMs with random effects using lme4.\n\nOptional topics: 1. Fit GLMs with spline terms using mgcv. 1. Fit binomial and overdispersed binomial models, and compare them to bernoulli models on the same data. 1. Understand the limitations of frequentist linear models and know that brms exists.",
    "crumbs": [
      "Case studies / advanced topics",
      "Advanced linear models"
    ]
  },
  {
    "objectID": "modules/linear-models.html#some-notes",
    "href": "modules/linear-models.html#some-notes",
    "title": "Case study: Linear Models and Linear Monsters",
    "section": "Some notes",
    "text": "Some notes\n\nThroughout this course, we’re attempted to use relatively few datasets in order to minimize confusion.\nWhen trying to show off many different features of data, this is not so easy. Most regression models in real life do not need all of these features at one time, and they are easier to discuss separately.\nSo we have a choice ahead of us: we can either introduce many different datasets, or use a completely fabricated dataset where we know the truth and can ensure that these statistical problems are present.\nFor this workshop, we’ve simulated a dataset that can help you explore some advanced concepts.",
    "crumbs": [
      "Case studies / advanced topics",
      "Advanced linear models"
    ]
  },
  {
    "objectID": "modules/linear-models.html#simulated-city-cases-dataset",
    "href": "modules/linear-models.html#simulated-city-cases-dataset",
    "title": "Case study: Linear Models and Linear Monsters",
    "section": "(Simulated) city cases dataset",
    "text": "(Simulated) city cases dataset\n\nSuppose the Queen of Caldwych is holding court in her capital city of Harcrest when her Royal Chief Epidemiologist (you can pretend that’s you, if you want) tells her, “Your Grace! I fear the grimbleth in our nation’s water supply is a cause of excess cases of Klaron’s gortensia beyond what can be explained by other factors!”\nBeing a kind and benevolent monarch, the Queen immediately commissions a 3 year prospective study to identify all incident cases of Klaron’s gortensia in a spatially representative sample of 23 cities in Caldwych, including Harcrest. She also provides funding for a representative sampling of the concentration of grimbleth in the municipal drinking water supply using the latest and greatest assay that can detect parts per million to the nearest hundreth accurately.\nYou can also assume the temperature and population measurements are completely accurate.\nYou know that Klaron’s gortensia is a chronic multifactorial condition with no single deterministic cause. Several environmental, (epi)genetic, and time-varying exposures can contribute to the development of Klaron’s gortensia. You can assume that we have a 100% accurate diagnostic method for Klaron’s gortensia.",
    "crumbs": [
      "Case studies / advanced topics",
      "Advanced linear models"
    ]
  },
  {
    "objectID": "modules/linear-models.html#poisson-regression",
    "href": "modules/linear-models.html#poisson-regression",
    "title": "Case study: Linear Models and Linear Monsters",
    "section": "Poisson regression",
    "text": "Poisson regression\n\nIncident case counts are a discrete variable that can only be 0, 1, 2, and so on, potentially up to the total population. These variables are often skewed and cannot be accurately modeled by a normal distribution.\nFit a glm that uses the \"poisson\" family where the outcome is case counts and the predictor is log grimbleth concentration in drinking water.\nYou also need to add an offset term to your model – if we want to interpret any case count predictors fairly, we HAVE to do this. You can add the term offset(log(population)) to the RHS of your model in order to account for this. Once the offset is specified correctly, you can interpret the exponentiated coefficients as incidence rate ratios.\n\n\nCode# stuff here\n\n\n\nIn many cases, Poisson models are too restrictive for most counts – this is called overdispersion.\nIf there is no overdispersion, the ratio of the explained deviance to the deviance degrees of freedom will be approximately 1. To get a formal test, we can use the fact that the explained deviance follows a chi-square distribution with degrees of freedom equal to the deviance DF if there is no overdispersion.\nIs there a statistically significant amount of overdispersion in the case counts?\n\n\nCode# Get deviance\n# Get deviance DF\n# pchisq(deviance, df, lower.tail = FALSE)\n\n\n\nUsually there is overdispersion in real data, and a common and easy way to handle this is with negative binomial (also called gamma-Poisson) regression.\n\n\nCode# Do nb glm from MASS here",
    "crumbs": [
      "Case studies / advanced topics",
      "Advanced linear models"
    ]
  },
  {
    "objectID": "modules/linear-models.html#interactions",
    "href": "modules/linear-models.html#interactions",
    "title": "Case study: Linear Models and Linear Monsters",
    "section": "Interactions",
    "text": "Interactions\n\nUsing interaction syntax in a formula (outcome ~ variable 1 * variable2 or outcome ~ variable1 + variable2 + variable1:variable2), check whether region and industry are related to case counts.\nDoes the effect of temperature vary by region?\nCan you think of any other potential causally important interactions that you think we should test?",
    "crumbs": [
      "Case studies / advanced topics",
      "Advanced linear models"
    ]
  },
  {
    "objectID": "modules/linear-models.html#random-effects",
    "href": "modules/linear-models.html#random-effects",
    "title": "Case study: Linear Models and Linear Monsters",
    "section": "Random effects",
    "text": "Random effects\n\nIn real life, there will ALWAYS be variance in experimental units that cannot be explained by all of the covariates we measure.\nIn this dataset, our experimental units are cities, and there are factors that vary across cities that confound inferences on case counts, and that we haven’t observed. We can control for baseline differences across cities with a random intercept.\nUse lme4::glmer() and (1 | city) and check out the random intercept.\nAre there any other grouping factors we should consider? What makes this tricky?\nAre there any effects that you think should vary across cities other than the baseline?",
    "crumbs": [
      "Case studies / advanced topics",
      "Advanced linear models"
    ]
  },
  {
    "objectID": "modules/linear-models.html#splines",
    "href": "modules/linear-models.html#splines",
    "title": "Case study: Linear Models and Linear Monsters",
    "section": "Splines",
    "text": "Splines\n\nSplines are a way to fit nonlinear effects without having to explicitly say what shape we think the effect is. They are easy to implement with mgcv.\nIs there evidence for nonlinear effects of log grimbleth concentration? What about nonlinear effects of temperature?\nIf there is a nonlinear effect of either of these variables, is it the same across all regions? Use the by argument of the smoothing spline function s() to check this.",
    "crumbs": [
      "Case studies / advanced topics",
      "Advanced linear models"
    ]
  },
  {
    "objectID": "modules/linear-models.html#bayesian-models",
    "href": "modules/linear-models.html#bayesian-models",
    "title": "Case study: Linear Models and Linear Monsters",
    "section": "Bayesian models",
    "text": "Bayesian models\n\nIt’s hard to fit models with both random effects and splines at the same time in frequentist world.\nHowever, many complicated models that have no MLE or implementation as frequentist models can be fit using Bayesian methods.\nMost models can be dropped into the package brms and work exactly the same, and the interpretation is much easier. Although they can take much longer to run.",
    "crumbs": [
      "Case studies / advanced topics",
      "Advanced linear models"
    ]
  },
  {
    "objectID": "modules/quarto/markdown.html",
    "href": "modules/quarto/markdown.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "modules/quarto/markdown.html#text-formatting",
    "href": "modules/quarto/markdown.html#text-formatting",
    "title": "",
    "section": "Text formatting",
    "text": "Text formatting\nitalic bold strikeout code\nsuperscript2 subscript2\nunderline small caps"
  },
  {
    "objectID": "modules/quarto/markdown.html#headings",
    "href": "modules/quarto/markdown.html#headings",
    "title": "",
    "section": "Headings",
    "text": "Headings"
  },
  {
    "objectID": "modules/quarto/markdown.html#nd-level-header",
    "href": "modules/quarto/markdown.html#nd-level-header",
    "title": "",
    "section": "2nd Level Header",
    "text": "2nd Level Header\n\n3rd Level Header"
  },
  {
    "objectID": "modules/quarto/markdown.html#lists",
    "href": "modules/quarto/markdown.html#lists",
    "title": "",
    "section": "Lists",
    "text": "Lists\n\nBulleted list item 1\nItem 2\n\nItem 2a\nItem 2b\n\n\n\nNumbered list item 1\nItem 2. The numbers are incremented automatically in the output."
  },
  {
    "objectID": "modules/quarto/markdown.html#links-and-images",
    "href": "modules/quarto/markdown.html#links-and-images",
    "title": "",
    "section": "Links and images",
    "text": "Links and images\nhttp://example.com\nlinked phrase\n\n\n\noptional caption text"
  },
  {
    "objectID": "modules/quarto/markdown.html#tables",
    "href": "modules/quarto/markdown.html#tables",
    "title": "",
    "section": "Tables",
    "text": "Tables\n\n\n\nFirst Header\nSecond Header\n\n\n\n\nContent Cell\nContent Cell\n\n\nContent Cell\nContent Cell"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Course Resources",
    "section": "",
    "text": "Data and Exercise downloads\n\nDownload the zip file from the GitHub page.\n\n\n\nUseful (+ Free) Resources\n\nR for Data Science: http://r4ds.had.co.nz/\n(great general information)\nFundamentals of Data Visualization: https://clauswilke.com/dataviz/\nR for Epidemiology: https://www.r4epi.com/\nThe Epidemiologist R Handbook: https://epirhandbook.com/en/\nR basics by Rafael A. Irizarry: https://rafalab.github.io/dsbook/r-basics.html (great general information)\nOpen Case Studies: https://www.opencasestudies.org/\n(resource for specific public health cases with statistical implementation and interpretation)\n\n\n\nNeed help?\n\nVarious “Cheat Sheets”: https://github.com/rstudio/cheatsheets/\nR reference card: http://cran.r-project.org/doc/contrib/Short-refcard.pdf\n\nR jargon: https://link.springer.com/content/pdf/bbm%3A978-1-4419-1318-0%2F1.pdf\nR vs Stata: https://link.springer.com/content/pdf/bbm%3A978-1-4419-1318-0%2F1.pdf\nR terminology: https://cran.r-project.org/doc/manuals/r-release/R-lang.pdf\n\n\n\nOther references\n\n\nBatra, Neale, Alex Spina, Paula Blomquist, Finlay Campbell, Henry Laurenson-Schafer, Florence Isaac, Natalie Fischer, et al. 2021. epiR Handbook. Edited by Neale Batra. https://epirhandbook.com/; Applied Epi Incorporated.\n\n\nCarchedi, Nick, and Sean Kross. 2024. “Learn r, in r.” Swirl. https://swirlstats.com/.\n\n\nKeyes, David. 2024. R for the Rest of Us: A Statistics-Free Introduction. San Francisco, CA: No Starch Press.\n\n\nMatloff, Norman. 2011. The Art of R Programming. San Francisco, CA: No Starch Press.\n\n\nR Core team. 2024. An Introduction to R. https://cran.r-project.org/doc/manuals/r-release/R-intro.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. 2nd ed. Sebastopol, CA: https://r4ds.hadley.nz/; O’Reilly Media.\n\n\n\n\n\n\n\n\nReuseCC BY-NC 4.0",
    "crumbs": [
      "Course Resources"
    ]
  }
]